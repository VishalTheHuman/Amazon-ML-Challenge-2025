{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b2f23ce-841b-4e80-9f31-f5ac1334df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_train_then_test.py\n",
    "# DistilBERT + contrastive + Huber regression\n",
    "# Trains on 100% of TRAIN_CSV (no validation), then predicts on TEST_CSV and writes predictions.csv\n",
    "\n",
    "import os, math, random, json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "# paths/columns (override via env if needed)\n",
    "TRAIN_CSV       = os.environ.get(\"TRAIN_CSV\", \"jl_fs/train.csv\")\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"jl_fs/test.csv\")  # file with sample_id, catalog_content\n",
    "ID_COL          = os.environ.get(\"ID_COL\", \"sample_id\")\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "PRICE_COL       = os.environ.get(\"PRICE_COL\", \"price\")\n",
    "\n",
    "MODEL_ID        = os.environ.get(\"MODEL_ID\", \"distilbert-base-uncased\")\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_distilbert_contrastive_full\")\n",
    "\n",
    "EPOCHS          = int(os.environ.get(\"EPOCHS\", \"3\"))\n",
    "MAX_LEN         = int(os.environ.get(\"MAX_LEN\", \"192\"))\n",
    "BATCH_SIZE      = int(os.environ.get(\"BATCH_SIZE\", \"16\"))\n",
    "LR              = float(os.environ.get(\"LR\", \"3e-5\"))\n",
    "WEIGHT_DECAY    = float(os.environ.get(\"WEIGHT_DECAY\", \"0.01\"))\n",
    "WARMUP_RATIO    = float(os.environ.get(\"WARMUP_RATIO\", \"0.06\"))\n",
    "GRAD_ACCUM      = int(os.environ.get(\"GRAD_ACCUM\", \"1\"))\n",
    "MAX_GRAD_NORM   = float(os.environ.get(\"MAX_GRAD_NORM\", \"1.0\"))\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() != \"false\"\n",
    "\n",
    "# hybrid loss mixing\n",
    "ALPHA_CONTRAST  = float(os.environ.get(\"ALPHA_CONTRAST\", \"0.25\"))  # 0..1\n",
    "TAU             = float(os.environ.get(\"TAU\", \"0.05\"))\n",
    "\n",
    "# light augmentation for 2nd view\n",
    "WORD_MASK_P     = float(os.environ.get(\"WORD_MASK_P\", \"0.08\"))\n",
    "DROPOUT_PROB    = float(os.environ.get(\"DROPOUT_PROB\", \"0.1\"))\n",
    "\n",
    "SEED            = int(os.environ.get(\"SEED\", \"42\"))\n",
    "MIN_PRICE       = float(os.environ.get(\"MIN_PRICE\", \"1e-6\"))\n",
    "\n",
    "FINAL_CKPT_NAME = \"final_full.pt\"\n",
    "PRED_CSV_NAME   = \"predictions.csv\"\n",
    "# --------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f355dabe-0af0-459d-96f2-f2cbf32ced2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def log2_price(p: np.ndarray) -> np.ndarray:\n",
    "    return np.log2(np.clip(p, MIN_PRICE, None))\n",
    "\n",
    "def delog2(x: np.ndarray) -> np.ndarray:\n",
    "    return np.power(2.0, x)\n",
    "\n",
    "class PriceTextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], prices_log2: Optional[np.ndarray], tokenizer, max_len: int):\n",
    "        self.texts = texts\n",
    "        self.prices_log2 = prices_log2\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "        return self.tok(\n",
    "            text if isinstance(text, str) else \"\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def _random_word_mask(self, input_ids: torch.Tensor, mask_token_id: int, prob: float) -> torch.Tensor:\n",
    "        if prob <= 0.0:\n",
    "            return input_ids\n",
    "        ids = input_ids.clone()\n",
    "        special = set(self.tok.all_special_ids)\n",
    "        for i in range(ids.size(0)):\n",
    "            for j in range(ids.size(1)):\n",
    "                if ids[i, j].item() in special:\n",
    "                    continue\n",
    "                if random.random() < prob:\n",
    "                    ids[i, j] = mask_token_id\n",
    "        return ids\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc1 = self._tokenize(self.texts[idx])\n",
    "        enc2 = {k: v.clone() for k, v in enc1.items()}\n",
    "        enc2[\"input_ids\"] = self._random_word_mask(enc2[\"input_ids\"], self.tok.mask_token_id, WORD_MASK_P)\n",
    "        item = {\n",
    "            \"input_ids_1\": enc1[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_1\": enc1[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_2\": enc2[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_2\": enc2[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        if self.prices_log2 is not None:\n",
    "            item[\"target\"] = torch.tensor(self.prices_log2[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "@dataclass\n",
    "class Collate:\n",
    "    pad_id: int\n",
    "    def __call__(self, batch):\n",
    "        keys1 = [\"input_ids_1\", \"attention_mask_1\"]\n",
    "        keys2 = [\"input_ids_2\", \"attention_mask_2\"]\n",
    "\n",
    "        def pad_stack(keylist):\n",
    "            maxlen = max(x[keylist[0]].size(0) for x in batch)\n",
    "            out = {}\n",
    "            for k in keylist:\n",
    "                pad_val = self.pad_id if \"input_ids\" in k else 0\n",
    "                tensors = []\n",
    "                for x in batch:\n",
    "                    v = x[k]\n",
    "                    if v.size(0) < maxlen:\n",
    "                        pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\n",
    "                        v = torch.cat([v, pad], dim=0)\n",
    "                    tensors.append(v.unsqueeze(0))\n",
    "                out[k] = torch.cat(tensors, dim=0)\n",
    "            return out\n",
    "\n",
    "        out1 = pad_stack(keys1)\n",
    "        out2 = pad_stack(keys2)\n",
    "        res = {**out1, **out2}\n",
    "        if \"target\" in batch[0]:\n",
    "            res[\"target\"] = torch.stack([x[\"target\"] for x in batch], dim=0)\n",
    "        return res\n",
    "\n",
    "class DistilBertPriceModel(nn.Module):\n",
    "    def __init__(self, model_id: str, proj_dim: int = 256, dropout: float = DROPOUT_PROB):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_id)\n",
    "        hidden = self.backbone.config.dim\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        yhat = self.regressor(cls).squeeze(-1)\n",
    "        z = self.proj(cls)\n",
    "        return yhat, z\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        y1, z1 = self.forward_once(input_ids_1, attention_mask_1)\n",
    "        y2, z2 = self.forward_once(input_ids_2, attention_mask_2)\n",
    "        return (y1 + y2) / 2.0, z1, z2\n",
    "\n",
    "def info_nce(z1: torch.Tensor, z2: torch.Tensor, tau: float) -> torch.Tensor:\n",
    "    z1 = F.normalize(z1, dim=-1)\n",
    "    z2 = F.normalize(z2, dim=-1)\n",
    "    logits = torch.matmul(z1, z2.t()) / tau\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    loss1 = F.cross_entropy(logits, labels)\n",
    "    loss2 = F.cross_entropy(logits.t(), labels)\n",
    "    return 0.5 * (loss1 + loss2)\n",
    "\n",
    "def huber_loss(pred, target, delta=1.0):\n",
    "    return F.huber_loss(pred, target, delta=delta)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def make_loader(texts, y_log2, tokenizer, batch_size, shuffle):\n",
    "    ds = PriceTextDataset(texts=texts, prices_log2=y_log2, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "    collate = Collate(pad_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa387e0-fb33-455c-b3a5-eae117a96f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ Device: cuda\n",
      "ðŸ”§ Loading train CSV: jl_fs/train.csv\n",
      "ðŸ§® Trainable params: 67,741,697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1127/2696675027.py:43: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "/tmp/ipykernel_1127/2696675027.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 300/4688 loss=1.2891 reg=1.5871 con=0.3949\n",
      "epoch 1 step 600/4688 loss=0.9002 reg=1.1328 con=0.2026\n",
      "epoch 1 step 900/4688 loss=0.7557 reg=0.9616 con=0.1382\n",
      "epoch 1 step 1200/4688 loss=0.6735 reg=0.8626 con=0.1061\n",
      "epoch 1 step 1500/4688 loss=0.6203 reg=0.7981 con=0.0867\n",
      "epoch 1 step 1800/4688 loss=0.5816 reg=0.7510 con=0.0733\n",
      "epoch 1 step 2100/4688 loss=0.5533 reg=0.7165 con=0.0635\n",
      "epoch 1 step 2400/4688 loss=0.5307 reg=0.6889 con=0.0561\n",
      "epoch 1 step 2700/4688 loss=0.5133 reg=0.6675 con=0.0506\n",
      "epoch 1 step 3000/4688 loss=0.4974 reg=0.6479 con=0.0459\n",
      "epoch 1 step 3300/4688 loss=0.4842 reg=0.6315 con=0.0423\n",
      "epoch 1 step 3600/4688 loss=0.4732 reg=0.6179 con=0.0390\n",
      "epoch 1 step 3900/4688 loss=0.4638 reg=0.6062 con=0.0364\n",
      "epoch 1 step 4200/4688 loss=0.4549 reg=0.5952 con=0.0340\n",
      "epoch 1 step 4500/4688 loss=0.4474 reg=0.5858 con=0.0321\n",
      "âœ… Epoch 1: TRAIN SMAPE = 45.780%\n",
      "epoch 2 step 300/4688 loss=0.2885 reg=0.3834 con=0.0038\n",
      "epoch 2 step 600/4688 loss=0.2934 reg=0.3900 con=0.0037\n",
      "epoch 2 step 900/4688 loss=0.2894 reg=0.3847 con=0.0036\n",
      "epoch 2 step 1200/4688 loss=0.2911 reg=0.3869 con=0.0036\n",
      "epoch 2 step 1500/4688 loss=0.2881 reg=0.3830 con=0.0035\n",
      "epoch 2 step 1800/4688 loss=0.2866 reg=0.3810 con=0.0034\n",
      "epoch 2 step 2100/4688 loss=0.2871 reg=0.3818 con=0.0032\n",
      "epoch 2 step 2400/4688 loss=0.2864 reg=0.3808 con=0.0031\n",
      "epoch 2 step 2700/4688 loss=0.2868 reg=0.3814 con=0.0030\n",
      "epoch 2 step 3000/4688 loss=0.2871 reg=0.3819 con=0.0028\n",
      "epoch 2 step 3300/4688 loss=0.2866 reg=0.3812 con=0.0028\n",
      "epoch 2 step 3600/4688 loss=0.2860 reg=0.3804 con=0.0028\n",
      "epoch 2 step 3900/4688 loss=0.2857 reg=0.3800 con=0.0028\n",
      "epoch 2 step 4200/4688 loss=0.2855 reg=0.3798 con=0.0027\n",
      "epoch 2 step 4500/4688 loss=0.2844 reg=0.3783 con=0.0027\n",
      "âœ… Epoch 2: TRAIN SMAPE = 39.334%\n",
      "epoch 3 step 300/4688 loss=0.2220 reg=0.2953 con=0.0020\n",
      "epoch 3 step 600/4688 loss=0.2252 reg=0.2995 con=0.0022\n",
      "epoch 3 step 900/4688 loss=0.2226 reg=0.2960 con=0.0022\n",
      "epoch 3 step 1200/4688 loss=0.2222 reg=0.2955 con=0.0023\n",
      "epoch 3 step 1500/4688 loss=0.2221 reg=0.2953 con=0.0024\n",
      "epoch 3 step 1800/4688 loss=0.2225 reg=0.2960 con=0.0023\n",
      "epoch 3 step 2100/4688 loss=0.2233 reg=0.2969 con=0.0023\n",
      "epoch 3 step 2400/4688 loss=0.2236 reg=0.2973 con=0.0023\n",
      "epoch 3 step 2700/4688 loss=0.2238 reg=0.2976 con=0.0024\n",
      "epoch 3 step 3000/4688 loss=0.2231 reg=0.2967 con=0.0024\n",
      "epoch 3 step 3300/4688 loss=0.2218 reg=0.2950 con=0.0023\n",
      "epoch 3 step 3600/4688 loss=0.2219 reg=0.2951 con=0.0023\n",
      "epoch 3 step 3900/4688 loss=0.2215 reg=0.2945 con=0.0023\n",
      "epoch 3 step 4200/4688 loss=0.2201 reg=0.2926 con=0.0023\n",
      "epoch 3 step 4500/4688 loss=0.2200 reg=0.2925 con=0.0023\n",
      "âœ… Epoch 3: TRAIN SMAPE = 33.793%\n",
      "ðŸ’¾ Saved final model to price_distilbert_contrastive_full/final_full.pt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ðŸ–¥ï¸ Device: {device}\")\n",
    "print(f\"ðŸ”§ Loading train CSV: {TRAIN_CSV}\")\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# checks & cleaning\n",
    "for col in [ID_COL, TEXT_COL, PRICE_COL]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' missing in {TRAIN_CSV}. Found: {df.columns.tolist()}\")\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "df = df.loc[pd.to_numeric(df[PRICE_COL], errors=\"coerce\").notnull()].copy()\n",
    "df[PRICE_COL] = df[PRICE_COL].astype(float)\n",
    "df = df.loc[df[PRICE_COL] >= 0.0].reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.mask_token is None:\n",
    "    tokenizer.add_special_tokens({\"mask_token\": \"[MASK]\"})\n",
    "\n",
    "model = DistilBertPriceModel(MODEL_ID).to(device)\n",
    "if tokenizer.vocab_size != model.backbone.get_input_embeddings().weight.size(0):\n",
    "    model.backbone.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"ðŸ§® Trainable params: {count_parameters(model):,}\")\n",
    "\n",
    "# train loader on 100% data\n",
    "y_all_log = log2_price(df[PRICE_COL].values)\n",
    "loader_all = make_loader(df[TEXT_COL].tolist(), y_all_log, tokenizer, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# optimizer/scheduler\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "grouped = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(grouped, lr=LR)\n",
    "steps_per_epoch = max(1, math.ceil(len(loader_all) / GRAD_ACCUM))\n",
    "num_training_steps = EPOCHS * steps_per_epoch\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(num_training_steps * WARMUP_RATIO),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "history = []\n",
    "\n",
    "# -------------------- TRAIN (full data) --------------------\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    run_loss = run_reg = run_con = 0.0\n",
    "    for step, batch in enumerate(loader_all, 1):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            yhat, z1, z2 = model(\n",
    "                input_ids_1=batch[\"input_ids_1\"],\n",
    "                attention_mask_1=batch[\"attention_mask_1\"],\n",
    "                input_ids_2=batch[\"input_ids_2\"],\n",
    "                attention_mask_2=batch[\"attention_mask_2\"],\n",
    "            )\n",
    "            loss_reg = huber_loss(yhat, batch[\"target\"])\n",
    "            loss_con = info_nce(z1, z2, tau=TAU)\n",
    "            loss = (1.0 - ALPHA_CONTRAST) * loss_reg + ALPHA_CONTRAST * loss_con\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % GRAD_ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        run_loss += loss.item(); run_reg += loss_reg.item(); run_con += loss_con.item()\n",
    "        if step % 300 == 0:\n",
    "            print(f\"epoch {epoch} step {step}/{len(loader_all)} \"\n",
    "                  f\"loss={run_loss/step:.4f} reg={run_reg/step:.4f} con={run_con/step:.4f}\")\n",
    "\n",
    "    # TRAIN SMAPE (on full data)\n",
    "    model.eval()\n",
    "    preds_log, tgts_log = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_all:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            yhat, _, _ = model(\n",
    "                input_ids_1=batch[\"input_ids_1\"],\n",
    "                attention_mask_1=batch[\"attention_mask_1\"],\n",
    "                input_ids_2=batch[\"input_ids_2\"],\n",
    "                attention_mask_2=batch[\"attention_mask_2\"],\n",
    "            )\n",
    "            preds_log.append(yhat.detach().float().cpu().numpy())\n",
    "            tgts_log.append(batch[\"target\"].detach().float().cpu().numpy())\n",
    "    preds_log = np.concatenate(preds_log, axis=0)\n",
    "    tgts_log  = np.concatenate(tgts_log, axis=0)\n",
    "    train_smape = smape_np(delog2(tgts_log), delog2(preds_log))\n",
    "    print(f\"âœ… Epoch {epoch}: TRAIN SMAPE = {train_smape:.3f}%\")\n",
    "\n",
    "    history.append({\"epoch\": int(epoch), \"train_smape\": float(train_smape)})\n",
    "    with open(os.path.join(OUTPUT_DIR, \"metrics_history.json\"), \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "# save final checkpoint + summary\n",
    "torch.save({\"model_state\": model.state_dict(), \"tokenizer\": MODEL_ID},\n",
    "           os.path.join(OUTPUT_DIR, FINAL_CKPT_NAME))\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump({\"history\": history, \"last_train_smape\": float(history[-1][\"train_smape\"])}, f, indent=2)\n",
    "print(f\"ðŸ’¾ Saved final model to {os.path.join(OUTPUT_DIR, FINAL_CKPT_NAME)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49e40a61-467a-4e75-ace0-78bd13f26ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”® Loading test CSV: jl_fs/test.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_1127/3910470799.py\", line 32, in __call__\n    \"input_ids\": pad(\"input_ids\", self.pad_id),\n  File \"/tmp/ipykernel_1127/3910470799.py\", line 27, in pad\n    pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\nTypeError: full() received an invalid combination of arguments - got (tuple, NoneType, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, Number fill_value, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m infer_loader:\n\u001b[1;32m     46\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1515\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1550\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1550\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_utils.py:750\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_1127/3910470799.py\", line 32, in __call__\n    \"input_ids\": pad(\"input_ids\", self.pad_id),\n  File \"/tmp/ipykernel_1127/3910470799.py\", line 27, in pad\n    pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\nTypeError: full() received an invalid combination of arguments - got (tuple, NoneType, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, Number fill_value, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -------------------- INFERENCE on TEST_CSV --------------------\n",
    "if TEST_CSV and os.path.exists(TEST_CSV):\n",
    "    print(f\"ðŸ”® Loading test CSV: {TEST_CSV}\")\n",
    "    dft = pd.read_csv(TEST_CSV)\n",
    "    if ID_COL not in dft.columns or TEXT_COL not in dft.columns:\n",
    "        raise ValueError(f\"Test file must contain '{ID_COL}' and '{TEXT_COL}'. Found: {dft.columns.tolist()}\")\n",
    "    dft[TEXT_COL] = dft[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    class InferDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts; self.tok = tokenizer; self.max_len = max_len\n",
    "        def __len__(self): return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False, return_tensors=\"pt\")\n",
    "            return {\"input_ids\": enc[\"input_ids\"].squeeze(0), \"attention_mask\": enc[\"attention_mask\"].squeeze(0)}\n",
    "\n",
    "    @dataclass\n",
    "    class InferCollate:\n",
    "        pad_id: int\n",
    "        def __call__(self, batch):\n",
    "            maxlen = max(x[\"input_ids\"].size(0) for x in batch)\n",
    "            def pad(key, pad_val):\n",
    "                arr = []\n",
    "                for x in batch:\n",
    "                    v = x[key]\n",
    "                    if v.size(0) < maxlen:\n",
    "                        pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\n",
    "                        v = torch.cat([v, pad], dim=0)\n",
    "                    arr.append(v.unsqueeze(0))\n",
    "                return torch.cat(arr, dim=0)\n",
    "            return {\n",
    "                \"input_ids\": pad(\"input_ids\", self.pad_id),\n",
    "                \"attention_mask\": pad(\"attention_mask\", 0),\n",
    "            }\n",
    "\n",
    "    infer_ds = InferDataset(dft[TEXT_COL].tolist(), tokenizer, MAX_LEN)\n",
    "    infer_loader = DataLoader(\n",
    "        infer_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2,\n",
    "        pin_memory=True, collate_fn=InferCollate(pad_id=tokenizer.pad_token_id or tokenizer.eos_token_id)\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in infer_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            # two forward_once passes (dropout stochasticity) averaged for stability\n",
    "            y1, _ = model.forward_once(input_ids, attention_mask)\n",
    "            y2, _ = model.forward_once(input_ids, attention_mask)\n",
    "            yhat_log = ((y1 + y2) / 2.0).detach().float().cpu().numpy()\n",
    "            preds.append(yhat_log)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    price_pred = delog2(preds)\n",
    "\n",
    "    out_df = pd.DataFrame({ID_COL: dft[ID_COL].values, \"price_pred\": price_pred})\n",
    "    out_path = os.path.join(OUTPUT_DIR, PRED_CSV_NAME)\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    print(f\"ðŸ“¤ Wrote predictions to: {out_path}\")\n",
    "\n",
    "    # optional: compute test SMAPE if ground-truth price exists\n",
    "    if PRICE_COL in dft.columns:\n",
    "        gt = pd.to_numeric(dft[PRICE_COL], errors=\"coerce\")\n",
    "        mask = gt.notnull()\n",
    "        if mask.any():\n",
    "            smape_test = smape_np(gt[mask].values.astype(float), out_df.loc[mask, \"price_pred\"].values.astype(float))\n",
    "            with open(os.path.join(OUTPUT_DIR, \"metrics_test.json\"), \"w\") as f:\n",
    "                json.dump({\"test_smape\": float(smape_test)}, f, indent=2)\n",
    "            print(f\"ðŸ§ª Test SMAPE = {smape_test:.3f}% (computed only for rows with ground truth)\")\n",
    "else:\n",
    "        print(\"â„¹ï¸ TEST_CSV not set or file not found â€” skipping prediction.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "834f8501-09bd-4d18-9f58-3b68be33e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¤ Wrote predictions to: price_distilbert_contrastive_full/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- TESTING / INFERENCE ONLY (fixed PAD) ----------\n",
    "import os, json\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# required from your training script:\n",
    "# - DistilBertPriceModel\n",
    "# - delog2()\n",
    "# - smape_np()\n",
    "# - constants: MODEL_ID, OUTPUT_DIR, TEST_CSV, ID_COL, TEXT_COL, PRICE_COL, MAX_LEN, BATCH_SIZE\n",
    "# If not defined above, set sensible defaults here:\n",
    "MODEL_ID   = globals().get(\"MODEL_ID\", \"distilbert-base-uncased\")\n",
    "OUTPUT_DIR = globals().get(\"OUTPUT_DIR\", \"price_distilbert_contrastive_full\")\n",
    "TEST_CSV   = globals().get(\"TEST_CSV\", \"jl_fstest.csv\")\n",
    "ID_COL     = globals().get(\"ID_COL\", \"sample_id\")\n",
    "TEXT_COL   = globals().get(\"TEXT_COL\", \"catalog_content\")\n",
    "PRICE_COL  = globals().get(\"PRICE_COL\", \"price\")\n",
    "MAX_LEN    = globals().get(\"MAX_LEN\", 192)\n",
    "BATCH_SIZE = globals().get(\"BATCH_SIZE\", 16)\n",
    "\n",
    "ckpt_path = os.path.join(OUTPUT_DIR, \"final_full.pt\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Load tokenizer (PAD fallback -> 0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0  # <- fix\n",
    "\n",
    "# 2) Recreate model and load weights\n",
    "model = DistilBertPriceModel(MODEL_ID).to(device)\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state[\"model_state\"], strict=True)\n",
    "model.eval()\n",
    "\n",
    "# 3) Load test CSV\n",
    "dft = pd.read_csv(TEST_CSV)\n",
    "if ID_COL not in dft.columns or TEXT_COL not in dft.columns:\n",
    "    raise ValueError(f\"Test file must contain '{ID_COL}' and '{TEXT_COL}'. Got: {dft.columns.tolist()}\")\n",
    "dft[TEXT_COL] = dft[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# 4) Inference dataset + collate (uses pad_id fallback)\n",
    "class InferDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts; self.tok = tokenizer; self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len,\n",
    "                       padding=False, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class InferCollate:\n",
    "    pad_id: int\n",
    "    def __call__(self, batch):\n",
    "        maxlen = max(x[\"input_ids\"].size(0) for x in batch)\n",
    "        def pad(key, pad_val):\n",
    "            arr = []\n",
    "            for x in batch:\n",
    "                v = x[key]\n",
    "                if v.size(0) < maxlen:\n",
    "                    pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\n",
    "                    v = torch.cat([v, pad], dim=0)\n",
    "                arr.append(v.unsqueeze(0))\n",
    "            return torch.cat(arr, dim=0)\n",
    "        return {\n",
    "            \"input_ids\": pad(\"input_ids\", self.pad_id),\n",
    "            \"attention_mask\": pad(\"attention_mask\", 0),\n",
    "        }\n",
    "\n",
    "infer_ds = InferDataset(dft[TEXT_COL].tolist(), tokenizer, MAX_LEN)\n",
    "infer_loader = DataLoader(\n",
    "    infer_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2,\n",
    "    pin_memory=True, collate_fn=InferCollate(pad_id=pad_id)\n",
    ")\n",
    "\n",
    "# 5) Run inference (two passes averaged for stability)\n",
    "preds_log = []\n",
    "with torch.no_grad():\n",
    "    for batch in infer_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        y1, _ = model.forward_once(input_ids, attention_mask)\n",
    "        y2, _ = model.forward_once(input_ids, attention_mask)\n",
    "        yhat_log = ((y1 + y2) / 2.0).detach().float().cpu().numpy()\n",
    "        preds_log.append(yhat_log)\n",
    "\n",
    "preds_log = np.concatenate(preds_log, axis=0)\n",
    "price_pred = delog2(preds_log)\n",
    "\n",
    "# 6) Save CSV\n",
    "out_df = pd.DataFrame({ID_COL: dft[ID_COL].values, \"price_pred\": price_pred})\n",
    "pred_path = os.path.join(OUTPUT_DIR, \"predictions.csv\")\n",
    "out_df.to_csv(pred_path, index=False)\n",
    "print(f\"ðŸ“¤ Wrote predictions to: {pred_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77a1d8-563e-4bc5-81ea-bd4ae3fecd96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
