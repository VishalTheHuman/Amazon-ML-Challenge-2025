{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa8ac66-e73f-439c-9d04-2f6e9b7fbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c524d04-698a-4049-8b57-e0d5ad664ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files (change here or via env)\n",
    "TRAIN_CSV       = os.environ.get(\"TRAIN_CSV\", \"jl_fs/train.csv\")\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"jl_fs/test.csv\")          # set if you have a test file\n",
    "ID_COL          = os.environ.get(\"ID_COL\", \"sample_id\")\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "PRICE_COL       = os.environ.get(\"PRICE_COL\", \"price\")\n",
    "\n",
    "# Model/output\n",
    "MODEL_ID        = os.environ.get(\"MODEL_ID\", \"google-bert/bert-large-uncased\")\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_large_bert_contrastive\")\n",
    "BEST_CKPT_NAME  = \"best_val.pt\"\n",
    "FULL_CKPT_NAME  = \"final_full.pt\"\n",
    "PRED_CSV_NAME   = \"predictions.csv\"\n",
    "\n",
    "# Training knobs\n",
    "SEED            = 42\n",
    "VAL_FRAC        = 0.0          # set to 0.0 to skip validation and just train on 100%\n",
    "MAX_LEN         = 192\n",
    "BATCH_SIZE      = 16\n",
    "LR              = 3e-5\n",
    "WEIGHT_DECAY    = 0.01\n",
    "EPOCHS          = 5            # epochs for Stage A (with validation)\n",
    "EPOCHS_FULL     = 2            # extra epochs when training on 100% (Stage B)\n",
    "WARMUP_RATIO    = 0.06\n",
    "GRAD_ACCUM      = 1\n",
    "MAX_GRAD_NORM   = 1.0\n",
    "FP16            = True\n",
    "\n",
    "# Loss mixing\n",
    "ALPHA_CONTRAST  = 0.25         # 0..1\n",
    "TAU             = 0.05         # InfoNCE temperature\n",
    "\n",
    "# Light data augmentation for the 2nd view\n",
    "WORD_MASK_P     = 0.08\n",
    "DROPOUT_PROB    = 0.1\n",
    "\n",
    "EARLY_STOP_ROUNDS = 3          # only used if VAL_FRAC > 0.0\n",
    "MIN_PRICE       = 1e-6         # for log2 transform\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3056c82e-aa4c-4d02-9dcb-f65c04930469",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_CONTRAST  = 0.25              # weight for contrastive loss (0..1). 0.25 is a good start.\n",
    "TAU             = 0.05                  # contrastive temperature\n",
    "\n",
    "# Augmentations\n",
    "WORD_MASK_P     = 0.08                  # probability to randomly mask an input token (2nd view only)\n",
    "DROPOUT_PROB    = 0.1                   # Dropout already inside the transformer; can adjust here in heads.\n",
    "\n",
    "EARLY_STOP_ROUNDS = 3                   # stop if val SMAPE hasn't improved for these epochs\n",
    "\n",
    "# Price clipping for log transform\n",
    "MIN_PRICE       = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d52918e-b72d-46e9-beb4-6f4420a2c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def log2_price(p: np.ndarray) -> np.ndarray:\n",
    "    return np.log2(np.clip(p, MIN_PRICE, None))\n",
    "\n",
    "def delog2(x: np.ndarray) -> np.ndarray:\n",
    "    return np.power(2.0, x)\n",
    "\n",
    "def split_train_val(df: pd.DataFrame, frac_val: float = 0.1, seed: int = SEED):\n",
    "    if frac_val <= 0.0:\n",
    "        return df.copy().reset_index(drop=True), pd.DataFrame(columns=df.columns)\n",
    "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    n_val = int(len(df) * frac_val)\n",
    "    df_val = df.iloc[:n_val].reset_index(drop=True)\n",
    "    df_tr  = df.iloc[n_val:].reset_index(drop=True)\n",
    "    return df_tr, df_val\n",
    "\n",
    "class PriceTextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], prices_log2: Optional[np.ndarray], tokenizer, max_len: int):\n",
    "        self.texts = texts\n",
    "        self.prices_log2 = prices_log2\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "        return self.tok(\n",
    "            text if isinstance(text, str) else \"\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def _random_word_mask(self, input_ids: torch.Tensor, mask_token_id: int, prob: float) -> torch.Tensor:\n",
    "        if prob <= 0.0:\n",
    "            return input_ids\n",
    "        ids = input_ids.clone()\n",
    "        special = set(self.tok.all_special_ids)\n",
    "        for i in range(ids.size(0)):\n",
    "            for j in range(ids.size(1)):\n",
    "                if ids[i, j].item() in special:\n",
    "                    continue\n",
    "                if random.random() < prob:\n",
    "                    ids[i, j] = mask_token_id\n",
    "        return ids\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc1 = self._tokenize(self.texts[idx])\n",
    "        enc2 = {k: v.clone() for k, v in enc1.items()}\n",
    "        enc2[\"input_ids\"] = self._random_word_mask(enc2[\"input_ids\"], self.tok.mask_token_id, WORD_MASK_P)\n",
    "        item = {\n",
    "            \"input_ids_1\": enc1[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_1\": enc1[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_2\": enc2[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_2\": enc2[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        if self.prices_log2 is not None:\n",
    "            item[\"target\"] = torch.tensor(self.prices_log2[idx], dtype=torch.float32)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c44720d9-686d-4df9-aaa8-3c345652fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Collate:\n",
    "    pad_id: int\n",
    "    def __call__(self, batch):\n",
    "        keys1 = [\"input_ids_1\", \"attention_mask_1\"]\n",
    "        keys2 = [\"input_ids_2\", \"attention_mask_2\"]\n",
    "\n",
    "        def pad_stack(keylist):\n",
    "            maxlen = max(x[keylist[0]].size(0) for x in batch)\n",
    "            out = {}\n",
    "            for k in keylist:\n",
    "                pad_val = self.pad_id if \"input_ids\" in k else 0\n",
    "                tensors = []\n",
    "                for x in batch:\n",
    "                    v = x[k]\n",
    "                    if v.size(0) < maxlen:\n",
    "                        pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\n",
    "                        v = torch.cat([v, pad], dim=0)\n",
    "                    tensors.append(v.unsqueeze(0))\n",
    "                out[k] = torch.cat(tensors, dim=0)\n",
    "            return out\n",
    "\n",
    "        out1 = pad_stack(keys1)\n",
    "        out2 = pad_stack(keys2)\n",
    "        res = {**out1, **out2}\n",
    "        if \"target\" in batch[0]:\n",
    "            res[\"target\"] = torch.stack([x[\"target\"] for x in batch], dim=0)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20db4377-4de6-4f75-adf4-2a86145d2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPriceModel(nn.Module):\n",
    "    def __init__(self, model_id: str, proj_dim: int = 256, dropout: float = DROPOUT_PROB):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_id)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        yhat = self.regressor(cls).squeeze(-1)\n",
    "        z = self.proj(cls)\n",
    "        return yhat, z\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        y1, z1 = self.forward_once(input_ids_1, attention_mask_1)\n",
    "        y2, z2 = self.forward_once(input_ids_2, attention_mask_2)\n",
    "        return (y1 + y2) / 2.0, z1, z2\n",
    "\n",
    "def info_nce(z1: torch.Tensor, z2: torch.Tensor, tau: float = TAU) -> torch.Tensor:\n",
    "    z1 = F.normalize(z1, dim=-1)\n",
    "    z2 = F.normalize(z2, dim=-1)\n",
    "    logits = torch.matmul(z1, z2.t()) / tau\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    loss1 = F.cross_entropy(logits, labels)\n",
    "    loss2 = F.cross_entropy(logits.t(), labels)\n",
    "    return 0.5 * (loss1 + loss2)\n",
    "\n",
    "def huber_loss(pred, target, delta=1.0):\n",
    "    return F.huber_loss(pred, target, delta=delta)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def make_loader(texts, y_log2, tokenizer, batch_size, shuffle):\n",
    "    ds = PriceTextDataset(texts=texts, prices_log2=y_log2, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "    collate = Collate(pad_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7773fc65-fc29-474c-925b-20d752c8d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_stage(model, loader_tr, loader_val, tokenizer, device, out_dir, track_val=True, epochs=EPOCHS):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    grouped = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(grouped, lr=LR)\n",
    "    steps_per_epoch = max(1, math.ceil(len(loader_tr) / GRAD_ACCUM))\n",
    "    num_training_steps = epochs * steps_per_epoch\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=int(num_training_steps * WARMUP_RATIO),\n",
    "                                                num_training_steps=num_training_steps)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "    best_smape = float(\"inf\")\n",
    "    patience = 0\n",
    "    metrics_history = []  # epoch-wise SMAPE logging\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        run_loss = run_reg = run_con = 0.0\n",
    "        for step, batch in enumerate(loader_tr, 1):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.cuda.amp.autocast(enabled=FP16):\n",
    "                yhat, z1, z2 = model(\n",
    "                    input_ids_1=batch[\"input_ids_1\"],\n",
    "                    attention_mask_1=batch[\"attention_mask_1\"],\n",
    "                    input_ids_2=batch[\"input_ids_2\"],\n",
    "                    attention_mask_2=batch[\"attention_mask_2\"],\n",
    "                )\n",
    "                loss_reg = huber_loss(yhat, batch[\"target\"])\n",
    "                loss_con = info_nce(z1, z2, tau=TAU)\n",
    "                loss = (1.0 - ALPHA_CONTRAST) * loss_reg + ALPHA_CONTRAST * loss_con\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if step % GRAD_ACCUM == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "\n",
    "            run_loss += loss.item(); run_reg += loss_reg.item(); run_con += loss_con.item()\n",
    "            if step % 300 == 0:\n",
    "                print(f\"epoch {epoch} step {step}/{len(loader_tr)} loss={run_loss/step:.4f} reg={run_reg/step:.4f} con={run_con/step:.4f}\")\n",
    "\n",
    "        # ---- Evaluate (val or train) ----\n",
    "        model.eval()\n",
    "        preds_log = []\n",
    "        tgts_log  = []\n",
    "        with torch.no_grad():\n",
    "            loader_eval = loader_val if (track_val and loader_val is not None and len(loader_val) > 0) else loader_tr\n",
    "            for batch in loader_eval:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                yhat, _, _ = model(\n",
    "                    input_ids_1=batch[\"input_ids_1\"],\n",
    "                    attention_mask_1=batch[\"attention_mask_1\"],\n",
    "                    input_ids_2=batch[\"input_ids_2\"],\n",
    "                    attention_mask_2=batch[\"attention_mask_2\"],\n",
    "                )\n",
    "                preds_log.append(yhat.detach().float().cpu().numpy())\n",
    "                tgts_log.append(batch[\"target\"].detach().float().cpu().numpy())\n",
    "        preds_log = np.concatenate(preds_log, axis=0)\n",
    "        tgts_log  = np.concatenate(tgts_log, axis=0)\n",
    "        preds = delog2(preds_log); tgts = delog2(tgts_log)\n",
    "        smape = smape_np(tgts, preds)\n",
    "        print(f\"‚úÖ Epoch {epoch}: {'VAL' if (track_val and loader_val is not None and len(loader_val)>0) else 'TRAIN'} SMAPE = {smape:.3f}%\")\n",
    "\n",
    "        # ---- Log epoch-wise SMAPE to history JSON ----\n",
    "        metrics_history.append({\"epoch\": int(epoch), \"val_smape\" if (track_val and loader_val is not None and len(loader_val)>0) else \"train_smape\": float(smape)})\n",
    "        with open(os.path.join(out_dir, \"metrics_history.json\"), \"w\") as f:\n",
    "            json.dump(metrics_history, f, indent=2)\n",
    "\n",
    "        # ---- Save best (only when tracking val) ----\n",
    "        if track_val and loader_val is not None and len(loader_val) > 0:\n",
    "            if smape < best_smape - 1e-6:\n",
    "                best_smape = smape\n",
    "                torch.save({\"model_state\": model.state_dict(), \"tokenizer\": MODEL_ID}, os.path.join(out_dir, BEST_CKPT_NAME))\n",
    "                patience = 0\n",
    "                print(f\"üíæ Saved new best to {os.path.join(out_dir, BEST_CKPT_NAME)}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                print(f\"‚è∏Ô∏è No improvement. Patience {patience}/{EARLY_STOP_ROUNDS}\")\n",
    "                if patience >= EARLY_STOP_ROUNDS:\n",
    "                    print(\"üõë Early stopping.\")\n",
    "                    break\n",
    "\n",
    "    # Final metrics file (best val when available, else last epoch train SMAPE)\n",
    "    with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\n",
    "        payload = {\"history\": metrics_history}\n",
    "        if track_val and loader_val is not None and len(loader_val) > 0:\n",
    "            payload[\"best_val_smape\"] = float(min([h[\"val_smape\"] for h in metrics_history]))\n",
    "        else:\n",
    "            payload[\"last_train_smape\"] = float(metrics_history[-1][\"train_smape\"])\n",
    "        json.dump(payload, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4e136b9-c33e-41fe-abaf-c1553984c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Device: cuda\n",
      "üîß Loading train CSV: jl_fs/train.csv\n",
      "üìä Split -> train=75000 | valid=0 (VAL_FRAC=0.0)\n",
      "üßÆ Trainable params: 337,504,513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3412/3665728768.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "/tmp/ipykernel_3412/3665728768.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 300/4688 loss=1.3099 reg=1.4714 con=0.8252\n",
      "epoch 1 step 600/4688 loss=0.9119 reg=1.0753 con=0.4217\n",
      "epoch 1 step 900/4688 loss=0.7697 reg=0.9312 con=0.2851\n",
      "epoch 1 step 1200/4688 loss=0.6848 reg=0.8408 con=0.2168\n",
      "epoch 1 step 1500/4688 loss=0.6321 reg=0.7843 con=0.1754\n",
      "epoch 1 step 1800/4688 loss=0.5938 reg=0.7423 con=0.1482\n",
      "epoch 1 step 2100/4688 loss=0.5659 reg=0.7114 con=0.1294\n",
      "epoch 1 step 2400/4688 loss=0.5424 reg=0.6852 con=0.1141\n",
      "epoch 1 step 2700/4688 loss=0.5235 reg=0.6639 con=0.1020\n",
      "epoch 1 step 3000/4688 loss=0.5086 reg=0.6473 con=0.0925\n",
      "epoch 1 step 3300/4688 loss=0.4960 reg=0.6330 con=0.0849\n",
      "epoch 1 step 3600/4688 loss=0.4852 reg=0.6207 con=0.0786\n",
      "epoch 1 step 3900/4688 loss=0.4750 reg=0.6090 con=0.0731\n",
      "epoch 1 step 4200/4688 loss=0.4661 reg=0.5986 con=0.0684\n",
      "epoch 1 step 4500/4688 loss=0.4578 reg=0.5890 con=0.0642\n",
      "‚úÖ Epoch 1: TRAIN SMAPE = 46.763%\n",
      "epoch 2 step 300/4688 loss=0.2995 reg=0.3976 con=0.0053\n",
      "epoch 2 step 600/4688 loss=0.2969 reg=0.3941 con=0.0055\n",
      "epoch 2 step 900/4688 loss=0.2962 reg=0.3929 con=0.0060\n",
      "epoch 2 step 1200/4688 loss=0.2975 reg=0.3938 con=0.0086\n",
      "epoch 2 step 1500/4688 loss=0.2986 reg=0.3954 con=0.0081\n",
      "epoch 2 step 1800/4688 loss=0.3001 reg=0.3976 con=0.0075\n",
      "epoch 2 step 2100/4688 loss=0.2984 reg=0.3956 con=0.0070\n",
      "epoch 2 step 2400/4688 loss=0.2987 reg=0.3959 con=0.0068\n",
      "epoch 2 step 2700/4688 loss=0.2985 reg=0.3959 con=0.0064\n",
      "epoch 2 step 3000/4688 loss=0.2989 reg=0.3964 con=0.0061\n",
      "epoch 2 step 3300/4688 loss=0.2978 reg=0.3951 con=0.0059\n",
      "epoch 2 step 3600/4688 loss=0.2971 reg=0.3943 con=0.0057\n",
      "epoch 2 step 3900/4688 loss=0.2963 reg=0.3932 con=0.0056\n",
      "epoch 2 step 4200/4688 loss=0.2961 reg=0.3930 con=0.0055\n",
      "epoch 2 step 4500/4688 loss=0.2959 reg=0.3927 con=0.0053\n",
      "‚úÖ Epoch 2: TRAIN SMAPE = 38.666%\n",
      "epoch 3 step 300/4688 loss=0.2267 reg=0.3008 con=0.0043\n",
      "epoch 3 step 600/4688 loss=0.2258 reg=0.2999 con=0.0035\n",
      "epoch 3 step 900/4688 loss=0.2270 reg=0.3016 con=0.0033\n",
      "epoch 3 step 1200/4688 loss=0.2265 reg=0.3009 con=0.0032\n",
      "epoch 3 step 1500/4688 loss=0.2256 reg=0.2998 con=0.0031\n",
      "epoch 3 step 1800/4688 loss=0.2247 reg=0.2986 con=0.0032\n",
      "epoch 3 step 2100/4688 loss=0.2254 reg=0.2994 con=0.0033\n",
      "epoch 3 step 2400/4688 loss=0.2248 reg=0.2987 con=0.0033\n",
      "epoch 3 step 2700/4688 loss=0.2231 reg=0.2964 con=0.0033\n",
      "epoch 3 step 3000/4688 loss=0.2234 reg=0.2967 con=0.0034\n",
      "epoch 3 step 3300/4688 loss=0.2235 reg=0.2968 con=0.0034\n",
      "epoch 3 step 3600/4688 loss=0.2234 reg=0.2967 con=0.0033\n",
      "epoch 3 step 3900/4688 loss=0.2239 reg=0.2974 con=0.0032\n",
      "epoch 3 step 4200/4688 loss=0.2236 reg=0.2971 con=0.0032\n",
      "epoch 3 step 4500/4688 loss=0.2237 reg=0.2972 con=0.0032\n",
      "‚úÖ Epoch 3: TRAIN SMAPE = 32.489%\n",
      "epoch 4 step 300/4688 loss=0.1582 reg=0.2101 con=0.0025\n",
      "epoch 4 step 600/4688 loss=0.1570 reg=0.2083 con=0.0033\n",
      "epoch 4 step 900/4688 loss=0.1575 reg=0.2089 con=0.0032\n",
      "epoch 4 step 1200/4688 loss=0.1601 reg=0.2124 con=0.0031\n",
      "epoch 4 step 1500/4688 loss=0.1587 reg=0.2107 con=0.0028\n",
      "epoch 4 step 1800/4688 loss=0.1589 reg=0.2109 con=0.0028\n",
      "epoch 4 step 2100/4688 loss=0.1582 reg=0.2101 con=0.0027\n",
      "epoch 4 step 2400/4688 loss=0.1579 reg=0.2096 con=0.0027\n",
      "epoch 4 step 2700/4688 loss=0.1574 reg=0.2090 con=0.0027\n",
      "epoch 4 step 3000/4688 loss=0.1578 reg=0.2095 con=0.0027\n",
      "epoch 4 step 3300/4688 loss=0.1581 reg=0.2099 con=0.0027\n",
      "epoch 4 step 3600/4688 loss=0.1580 reg=0.2098 con=0.0027\n",
      "epoch 4 step 3900/4688 loss=0.1580 reg=0.2099 con=0.0026\n",
      "epoch 4 step 4200/4688 loss=0.1579 reg=0.2097 con=0.0026\n",
      "epoch 4 step 4500/4688 loss=0.1586 reg=0.2106 con=0.0025\n",
      "‚úÖ Epoch 4: TRAIN SMAPE = 24.564%\n",
      "epoch 5 step 300/4688 loss=0.1070 reg=0.1420 con=0.0020\n",
      "epoch 5 step 600/4688 loss=0.1088 reg=0.1441 con=0.0030\n",
      "epoch 5 step 900/4688 loss=0.1095 reg=0.1452 con=0.0026\n",
      "epoch 5 step 1200/4688 loss=0.1105 reg=0.1466 con=0.0025\n",
      "epoch 5 step 1500/4688 loss=0.1101 reg=0.1461 con=0.0023\n",
      "epoch 5 step 1800/4688 loss=0.1105 reg=0.1465 con=0.0023\n",
      "epoch 5 step 2100/4688 loss=0.1104 reg=0.1464 con=0.0023\n",
      "epoch 5 step 2400/4688 loss=0.1095 reg=0.1453 con=0.0022\n",
      "epoch 5 step 2700/4688 loss=0.1095 reg=0.1452 con=0.0021\n",
      "epoch 5 step 3000/4688 loss=0.1092 reg=0.1449 con=0.0022\n",
      "epoch 5 step 3300/4688 loss=0.1092 reg=0.1448 con=0.0023\n",
      "epoch 5 step 3600/4688 loss=0.1087 reg=0.1442 con=0.0023\n",
      "epoch 5 step 3900/4688 loss=0.1087 reg=0.1442 con=0.0023\n",
      "epoch 5 step 4200/4688 loss=0.1085 reg=0.1439 con=0.0023\n",
      "epoch 5 step 4500/4688 loss=0.1081 reg=0.1434 con=0.0023\n",
      "‚úÖ Epoch 5: TRAIN SMAPE = 21.936%\n",
      "üîÅ Retraining on 100% of training data...\n",
      "epoch 1 step 300/4688 loss=0.0902 reg=0.1195 con=0.0022\n",
      "epoch 1 step 600/4688 loss=0.1030 reg=0.1365 con=0.0025\n",
      "epoch 1 step 900/4688 loss=0.1156 reg=0.1533 con=0.0026\n",
      "epoch 1 step 1200/4688 loss=0.1270 reg=0.1683 con=0.0031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m y_all_log \u001b[38;5;241m=\u001b[39m log2_price(df[PRICE_COL]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     52\u001b[0m loader_all \u001b[38;5;241m=\u001b[39m make_loader(df[TEXT_COL]\u001b[38;5;241m.\u001b[39mtolist(), y_all_log, tokenizer, BATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrain_one_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_FULL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Save final full-data checkpoint\u001b[39;00m\n\u001b[1;32m     57\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: MODEL_ID},\n\u001b[1;32m     58\u001b[0m            os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, FULL_CKPT_NAME))\n",
      "Cell \u001b[0;32mIn[20], line 35\u001b[0m, in \u001b[0;36mtrain_one_stage\u001b[0;34m(model, loader_tr, loader_val, tokenizer, device, out_dir, track_val, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss_con \u001b[38;5;241m=\u001b[39m info_nce(z1, z2, tau\u001b[38;5;241m=\u001b[39mTAU)\n\u001b[1;32m     34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m ALPHA_CONTRAST) \u001b[38;5;241m*\u001b[39m loss_reg \u001b[38;5;241m+\u001b[39m ALPHA_CONTRAST \u001b[38;5;241m*\u001b[39m loss_con\n\u001b[0;32m---> 35\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m GRAD_ACCUM \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "print(f\"üîß Loading train CSV: {TRAIN_CSV}\")\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Checks & cleaning\n",
    "for col in [ID_COL, TEXT_COL, PRICE_COL]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' missing in {TRAIN_CSV}. Found: {df.columns.tolist()}\")\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "df = df.loc[pd.to_numeric(df[PRICE_COL], errors=\"coerce\").notnull()].copy()\n",
    "df[PRICE_COL] = df[PRICE_COL].astype(float)\n",
    "df = df.loc[df[PRICE_COL] >= 0.0].reset_index(drop=True)\n",
    "\n",
    "# Optional validation split (Stage A)\n",
    "df_tr, df_va = split_train_val(df, frac_val=VAL_FRAC, seed=SEED)\n",
    "print(f\"üìä Split -> train={len(df_tr)} | valid={len(df_va)} (VAL_FRAC={VAL_FRAC})\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.mask_token is None:\n",
    "    tokenizer.add_special_tokens({\"mask_token\": \"[MASK]\"})\n",
    "\n",
    "# Stage A: train with validation (if VAL_FRAC>0)\n",
    "model = BertPriceModel(MODEL_ID).to(device)\n",
    "if tokenizer.vocab_size != model.backbone.get_input_embeddings().weight.size(0):\n",
    "    model.backbone.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"üßÆ Trainable params: {count_parameters(model):,}\")\n",
    "\n",
    "y_tr_log = log2_price(df_tr[PRICE_COL].values)\n",
    "loader_tr = make_loader(df_tr[TEXT_COL].tolist(), y_tr_log, tokenizer, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "if len(df_va) > 0:\n",
    "    y_va_log = log2_price(df_va[PRICE_COL].values)\n",
    "    loader_va = make_loader(df_va[TEXT_COL].tolist(), y_va_log, tokenizer, BATCH_SIZE, shuffle=False)\n",
    "else:\n",
    "    loader_va = None\n",
    "\n",
    "train_one_stage(model, loader_tr, loader_va, tokenizer, device, OUTPUT_DIR,\n",
    "                track_val=(len(df_va) > 0), epochs=EPOCHS)\n",
    "\n",
    "# If we had a val set, reload best val checkpoint before Stage B\n",
    "if len(df_va) > 0 and os.path.exists(os.path.join(OUTPUT_DIR, BEST_CKPT_NAME)):\n",
    "    ckpt = torch.load(os.path.join(OUTPUT_DIR, BEST_CKPT_NAME), map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    print(\"‚úÖ Loaded best validation checkpoint for full-data training.\")\n",
    "\n",
    "# Stage B: train on 100% of data (no validation) for a couple of epochs\n",
    "print(\"üîÅ Retraining on 100% of training data...\")\n",
    "y_all_log = log2_price(df[PRICE_COL].values)\n",
    "loader_all = make_loader(df[TEXT_COL].tolist(), y_all_log, tokenizer, BATCH_SIZE, shuffle=True)\n",
    "train_one_stage(model, loader_all, loader_val=None, tokenizer=tokenizer, device=device,\n",
    "                out_dir=OUTPUT_DIR, track_val=False, epochs=EPOCHS_FULL)\n",
    "\n",
    "# Save final full-data checkpoint\n",
    "torch.save({\"model_state\": model.state_dict(), \"tokenizer\": MODEL_ID},\n",
    "           os.path.join(OUTPUT_DIR, FULL_CKPT_NAME))\n",
    "print(f\"üíæ Saved final full-data model to {os.path.join(OUTPUT_DIR, FULL_CKPT_NAME)}\")\n",
    "\n",
    "# -------------------- Stage C: Predict on test file --------------------\n",
    "if TEST_CSV and os.path.exists(TEST_CSV):\n",
    "    print(f\"üîÆ Loading test CSV: {TEST_CSV}\")\n",
    "    dft = pd.read_csv(TEST_CSV)\n",
    "    if ID_COL not in dft.columns or TEXT_COL not in dft.columns:\n",
    "        raise ValueError(f\"Test file must contain '{ID_COL}' and '{TEXT_COL}'. Found: {dft.columns.tolist()}\")\n",
    "    dft[TEXT_COL] = dft[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    # Inference loader (no targets)\n",
    "    class InferDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts; self.tok = tokenizer; self.max_len = max_len\n",
    "        def __len__(self): return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False, return_tensors=\"pt\")\n",
    "            return {\n",
    "                \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            }\n",
    "    @dataclass\n",
    "    class InferCollate:\n",
    "        pad_id: int\n",
    "        def __call__(self, batch):\n",
    "            maxlen = max(x[\"input_ids\"].size(0) for x in batch)\n",
    "            def pad(key, pad_val):\n",
    "                arr = []\n",
    "                for x in batch:\n",
    "                    v = x[key]\n",
    "                    if v.size(0) < maxlen:\n",
    "                        pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\n",
    "                        v = torch.cat([v, pad], dim=0)\n",
    "                    arr.append(v.unsqueeze(0))\n",
    "                return torch.cat(arr, dim=0)\n",
    "            return {\n",
    "                \"input_ids\": pad(\"input_ids\", self.pad_id),\n",
    "                \"attention_mask\": pad(\"attention_mask\", 0),\n",
    "            }\n",
    "\n",
    "    infer_ds = InferDataset(dft[TEXT_COL].tolist(), tokenizer, MAX_LEN)\n",
    "    infer_loader = DataLoader(\n",
    "        infer_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2,\n",
    "        pin_memory=True, collate_fn=InferCollate(pad_id=tokenizer.pad_token_id or tokenizer.eos_token_id)\n",
    "    )\n",
    "\n",
    "    # Build a small head-only forward for inference\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in infer_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            # two passes for stability (reusing model.forward_once)\n",
    "            y1, _ = model.forward_once(input_ids, attention_mask)\n",
    "            y2, _ = model.forward_once(input_ids, attention_mask)\n",
    "            yhat_log = ((y1 + y2) / 2.0).detach().float().cpu().numpy()\n",
    "            preds.append(yhat_log)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    price_pred = delog2(preds)\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        ID_COL: dft[ID_COL].values,\n",
    "        \"price_pred\": price_pred\n",
    "    })\n",
    "    out_path = os.path.join(OUTPUT_DIR, PRED_CSV_NAME)\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    print(f\"üì§ Wrote predictions to: {out_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è TEST_CSV not set or file not found ‚Äî skipping prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65e63897-87d3-4d9c-b33b-21e8828de089",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state\": model.state_dict(), \"tokenizer\": MODEL_ID},\n",
    "           os.path.join(OUTPUT_DIR, FULL_CKPT_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23c3680f-a985-43d5-bd95-06e4b58a2247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Loading checkpoint: price_large_bert_contrastive/final_full.pt\n",
      "üî§ Using tokenizer/model id: google-bert/bert-large-uncased\n",
      "üîÆ Loading test CSV: jl_fs/test.csv\n",
      "üì§ Wrote predictions to: price_large_bert_contrastive/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Stage C: Load checkpoint & predict on test file --------------------\n",
    "import glob\n",
    "\n",
    "# Resolve checkpoint path\n",
    "if \"FULL_CKPT_NAME\" in globals() and FULL_CKPT_NAME:\n",
    "    CKPT_PATH = os.path.join(OUTPUT_DIR, FULL_CKPT_NAME)\n",
    "else:\n",
    "    # fallback: pick most recent .pt in OUTPUT_DIR\n",
    "    cands = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"*.pt\")), key=os.path.getmtime)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No .pt checkpoints found in {OUTPUT_DIR}\")\n",
    "    CKPT_PATH = cands[-1]\n",
    "\n",
    "print(f\"üß© Loading checkpoint: {CKPT_PATH}\")\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "\n",
    "# Rebuild tokenizer/model from checkpoint metadata\n",
    "saved_model_id = ckpt.get(\"tokenizer\", MODEL_ID)  # we stored MODEL_ID under 'tokenizer'\n",
    "print(f\"üî§ Using tokenizer/model id: {saved_model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_id, use_fast=True)\n",
    "\n",
    "# If pad token is missing for any reason, set one\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "# Recreate model and align embedding matrix size to tokenizer vocab BEFORE loading state_dict\n",
    "model = BertPriceModel(saved_model_id).to(device)\n",
    "if tokenizer.vocab_size != model.backbone.get_input_embeddings().weight.size(0):\n",
    "    print(f\"‚ÑπÔ∏è Resizing token embeddings to match tokenizer: {tokenizer.vocab_size}\")\n",
    "    model.backbone.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load weights (strict=False in case resize introduced size diffs in embeddings)\n",
    "missing, unexpected = model.load_state_dict(ckpt[\"model_state\"], strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"‚ö†Ô∏è load_state_dict: missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "    if missing:   print(\"  missing:\", missing[:8], \"...\" if len(missing) > 8 else \"\")\n",
    "    if unexpected:print(\"  unexpected:\", unexpected[:8], \"...\" if len(unexpected) > 8 else \"\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Names/defaults\n",
    "if \"PRED_CSV_NAME\" not in globals() or not PRED_CSV_NAME:\n",
    "    PRED_CSV_NAME = \"predictions.csv\"\n",
    "if \"ID_COL\" not in globals() or not ID_COL:\n",
    "    ID_COL = \"sample_id\"\n",
    "\n",
    "# Inference\n",
    "if \"TEST_CSV\" in globals() and TEST_CSV and os.path.exists(TEST_CSV):\n",
    "    print(f\"üîÆ Loading test CSV: {TEST_CSV}\")\n",
    "    dft = pd.read_csv(TEST_CSV)\n",
    "    if ID_COL not in dft.columns or TEXT_COL not in dft.columns:\n",
    "        raise ValueError(f\"Test file must contain '{ID_COL}' and '{TEXT_COL}'. Found: {dft.columns.tolist()}\")\n",
    "    dft[TEXT_COL] = dft[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    class InferDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts; self.tok = tokenizer; self.max_len = max_len\n",
    "        def __len__(self): return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False, return_tensors=\"pt\")\n",
    "            return {\n",
    "                \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            }\n",
    "\n",
    "    @dataclass\n",
    "    class InferCollate:\n",
    "        pad_id: int\n",
    "        def __call__(self, batch):\n",
    "            maxlen = max(x[\"input_ids\"].size(0) for x in batch)\n",
    "            def pad(key, pad_val):\n",
    "                arr = []\n",
    "                for x in batch:\n",
    "                    v = x[key]\n",
    "                    if v.size(0) < maxlen:\n",
    "                        pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\n",
    "                        v = torch.cat([v, pad], dim=0)\n",
    "                    arr.append(v.unsqueeze(0))\n",
    "                return torch.cat(arr, dim=0)\n",
    "            return {\n",
    "                \"input_ids\": pad(\"input_ids\", self.pad_id),\n",
    "                \"attention_mask\": pad(\"attention_mask\", 0),\n",
    "            }\n",
    "\n",
    "    infer_ds = InferDataset(dft[TEXT_COL].tolist(), tokenizer, MAX_LEN)\n",
    "    infer_loader = DataLoader(\n",
    "        infer_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        collate_fn=InferCollate(pad_id=tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "    preds_log2 = []\n",
    "    with torch.no_grad():\n",
    "        for batch in infer_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            # Two stochastic passes (dropout) and average for stability\n",
    "            y1, _ = model.forward_once(input_ids, attention_mask)\n",
    "            y2, _ = model.forward_once(input_ids, attention_mask)\n",
    "            yhat = ((y1 + y2) / 2.0).detach().float().cpu().numpy()\n",
    "            preds_log2.append(yhat)\n",
    "\n",
    "    preds_log2 = np.concatenate(preds_log2, axis=0)\n",
    "    price_pred = delog2(preds_log2)\n",
    "\n",
    "    out_df = pd.DataFrame({ID_COL: dft[ID_COL].values, \"price\": price_pred})\n",
    "    out_path = os.path.join(OUTPUT_DIR, PRED_CSV_NAME)\n",
    "    out_df.to_csv(\"prediction-bert-large.csv\", index=False)\n",
    "    print(f\"üì§ Wrote predictions to: {out_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è TEST_CSV not set or file not found ‚Äî skipping prediction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8f0e1-3625-4eb7-b94f-c68089986ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
