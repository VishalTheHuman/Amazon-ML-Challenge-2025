{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f82cc5a-72e0-415b-89e7-57e483af25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "\n",
    "df = pd.read_csv(\"jl_fs/train.csv\")\n",
    "df[\"image_path\"] = df[\"sample_id\"].apply(lambda x : f\"jl_fs/images/train/{x}.jpg\")\n",
    "df.to_csv(\"train_updated.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6537ead-bd3b-48cf-994b-511e24fd87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    CLIPModel,\n",
    "    AutoProcessor,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98cf885c-9990-4c1d-bd76-7c121631c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "CSV_PATH        = os.environ.get(\"TRAIN_CSV\", \"train_updated.csv\")     # must contain text + price + image path\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "PRICE_COL       = os.environ.get(\"PRICE_COL\", \"price\")\n",
    "IMG_COL         = os.environ.get(\"IMG_COL\",  \"image_path\")             # local jpg path column\n",
    "ID_COL          = os.environ.get(\"ID_COL\",  \"sample_id\")\n",
    "\n",
    "MODEL_ID        = os.environ.get(\"MODEL_ID\", \"openai/clip-vit-large-patch14\")\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_clip_fulltrain\")\n",
    "\n",
    "SEED            = int(os.environ.get(\"SEED\", \"42\"))\n",
    "MAX_LEN         = int(os.environ.get(\"MAX_LEN\", \"64\"))                 # CLIP text context is shorter\n",
    "BATCH_SIZE      = int(os.environ.get(\"BATCH_SIZE\", \"32\"))\n",
    "LR              = float(os.environ.get(\"LR\", \"2e-5\"))\n",
    "WEIGHT_DECAY    = float(os.environ.get(\"WEIGHT_DECAY\", \"0.01\"))\n",
    "EPOCHS          = int(os.environ.get(\"EPOCHS\", \"10\"))\n",
    "WARMUP_RATIO    = float(os.environ.get(\"WARMUP_RATIO\", \"0.06\"))\n",
    "GRAD_ACCUM      = int(os.environ.get(\"GRAD_ACCUM\", \"1\"))\n",
    "MAX_GRAD_NORM   = float(os.environ.get(\"MAX_GRAD_NORM\", \"1.0\"))\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "# Loss & regularization\n",
    "ALPHA_CONTRAST  = float(os.environ.get(\"ALPHA_CONTRAST\", \"0.25\"))      # weight for contrastive loss (0..1)\n",
    "TAU             = float(os.environ.get(\"TAU\", \"0.07\"))                 # InfoNCE temperature\n",
    "HUBER_DELTA     = float(os.environ.get(\"HUBER_DELTA\", \"1.0\"))\n",
    "\n",
    "# Price/log transform\n",
    "MIN_PRICE       = float(os.environ.get(\"MIN_PRICE\", \"1e-6\"))\n",
    "\n",
    "# Missing image policy for TRAIN: zero | text_only | drop\n",
    "IMG_MISSING_POLICY = os.environ.get(\"IMG_MISSING_POLICY\", \"zero\").lower()\n",
    "assert IMG_MISSING_POLICY in {\"zero\", \"text_only\", \"drop\"}\n",
    "\n",
    "# Inference/Test config (optional)\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"jl_fs/test.csv\").strip()               # if \"\", inference is skipped\n",
    "TEST_IMG_DIR    = os.environ.get(\"TEST_IMG_DIR\", \"jl_fs/images/test\")  # used if test CSV lacks image_path\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc64e3e-cb0d-43ea-b394-13a89a64981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- Utils ---------------------------\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def log2_price(p: np.ndarray) -> np.ndarray:\n",
    "    return np.log2(np.clip(p, MIN_PRICE, None))\n",
    "\n",
    "def delog2(x: np.ndarray) -> np.ndarray:\n",
    "    return np.power(2.0, x)\n",
    "\n",
    "# --------------------------- Dataset & Collate ---------------------------\n",
    "class ClipPriceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TRAIN:\n",
    "      policy 'zero':      returns dummy pixel for missing images; later masked to zeros.\n",
    "      policy 'text_only': returns no pixel_values for missing images; vision forward skipped.\n",
    "      policy 'drop':      drops rows with missing images at dataset build time.\n",
    "\n",
    "    TEST:\n",
    "      We will ALWAYS behave like 'zero' (never drop predictions).\n",
    "\n",
    "    Items may contain: input_ids, attention_mask, (pixel_values), img_missing, (target)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, text_col: str, img_col: str,\n",
    "                 prices_log2: Optional[np.ndarray],\n",
    "                 processor: AutoProcessor, max_len: int, policy: str, is_test: bool = False):\n",
    "        self.processor = processor\n",
    "        self.max_len = max_len\n",
    "        self.policy = policy\n",
    "        self.is_test = is_test\n",
    "\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "        if (policy == \"drop\") and (not is_test):\n",
    "            before = len(df)\n",
    "            df = df[df[img_col].apply(lambda p: isinstance(p, str) and len(p) > 0 and os.path.exists(p))]\n",
    "            self.dropped_missing = before - len(df)\n",
    "        else:\n",
    "            self.dropped_missing = 0\n",
    "\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.img_paths = df[img_col].fillna(\"\").astype(str).tolist()\n",
    "        self.prices_log2 = prices_log2\n",
    "        self.missing_img_count = 0\n",
    "\n",
    "        # Dummy pixel to get correct shape\n",
    "        dummy = self.processor(images=Image.new(\"RGB\", (224, 224)), return_tensors=\"pt\")\n",
    "        self._dummy_pixel = dummy[\"pixel_values\"].squeeze(0)  # (C,H,W)\n",
    "\n",
    "        # For exporting IDs in test predictions\n",
    "        self.ids = df[ID_COL].tolist() if (ID_COL in df.columns) else list(range(len(df)))\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def _load_image(self, path: str):\n",
    "        if isinstance(path, str) and path and os.path.exists(path):\n",
    "            try:\n",
    "                return Image.open(path).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.missing_img_count += 1\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        img  = self._load_image(self.img_paths[idx])\n",
    "\n",
    "        enc_text = self.processor(text=[text], padding=False, truncation=True,\n",
    "                                  max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        img_missing = 0\n",
    "        pixel_values = None\n",
    "\n",
    "        if img is None:\n",
    "            img_missing = 1\n",
    "            if self.is_test:\n",
    "                # For test we NEVER drop; force zero-like behavior\n",
    "                pixel_values = self._dummy_pixel.clone()\n",
    "            else:\n",
    "                if self.policy == \"zero\":\n",
    "                    pixel_values = self._dummy_pixel.clone()\n",
    "                elif self.policy == \"text_only\":\n",
    "                    pixel_values = None\n",
    "                elif self.policy == \"drop\":\n",
    "                    # should not occur because drop was handled in __init__\n",
    "                    pixel_values = self._dummy_pixel.clone()\n",
    "        else:\n",
    "            enc_img = self.processor(images=img, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": enc_text[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc_text[\"attention_mask\"].squeeze(0),\n",
    "            \"img_missing\": torch.tensor(img_missing, dtype=torch.uint8),\n",
    "            \"row_id\": torch.tensor(self.ids[idx], dtype=torch.long)\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            item[\"pixel_values\"] = pixel_values\n",
    "        if self.prices_log2 is not None:\n",
    "            item[\"target\"] = torch.tensor(self.prices_log2[idx], dtype=torch.float32)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de9393c3-6394-48d6-bbd2-aa7b457974b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class CollateClip:\n",
    "    processor: AutoProcessor\n",
    "    def __call__(self, batch):\n",
    "        # pad text\n",
    "        input_ids = [b[\"input_ids\"] for b in batch]\n",
    "        attention = [b[\"attention_mask\"] for b in batch]\n",
    "        text_padded = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention},\n",
    "            padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        # images: some may be absent (text_only policy)\n",
    "        has_pix = [(\"pixel_values\" in b) for b in batch]\n",
    "        pixel_values = None\n",
    "        if any(has_pix):\n",
    "            shapes = [b[\"pixel_values\"].shape for b in batch if \"pixel_values\" in b]\n",
    "            C,H,W = shapes[0]\n",
    "            stacked = []\n",
    "            for b in batch:\n",
    "                if \"pixel_values\" in b:\n",
    "                    stacked.append(b[\"pixel_values\"])\n",
    "                else:\n",
    "                    stacked.append(torch.zeros((C,H,W), dtype=torch.float32))\n",
    "            pixel_values = torch.stack(stacked, dim=0)\n",
    "\n",
    "        res = {\n",
    "            \"input_ids\": text_padded[\"input_ids\"],\n",
    "            \"attention_mask\": text_padded[\"attention_mask\"],\n",
    "            \"img_missing\": torch.stack([b[\"img_missing\"] for b in batch], dim=0),\n",
    "            \"row_id\": torch.stack([b[\"row_id\"] for b in batch], dim=0),\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            res[\"pixel_values\"] = pixel_values\n",
    "        if \"target\" in batch[0]:\n",
    "            res[\"target\"] = torch.stack([b[\"target\"] for b in batch], dim=0)\n",
    "        return res\n",
    "\n",
    "# --------------------------- Model & Loss ---------------------------\n",
    "class ClipRegressionHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        in_dim = 2 * embed_dim  # concat image+text\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_dim, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "def info_nce(z_img: torch.Tensor, z_txt: torch.Tensor, tau: float = 0.07) -> torch.Tensor:\n",
    "    z_img = F.normalize(z_img, dim=-1)\n",
    "    z_txt = F.normalize(z_txt, dim=-1)\n",
    "    logits = torch.matmul(z_img, z_txt.t()) / tau  # (B,B)\n",
    "    labels = torch.arange(z_img.size(0), device=z_img.device)\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.t(), labels)\n",
    "    return 0.5 * (loss_i + loss_t)\n",
    "\n",
    "def huber_loss(pred, target, delta=1.0):\n",
    "    return F.huber_loss(pred, target, delta=delta)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72670a55-1fc0-4b25-94a0-10a3ee80a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Loading TRAIN CSV: train_updated.csv\n",
      "ðŸ“¦ Train rows: 75000\n",
      "ðŸ–¥ï¸ Device: cuda\n",
      "ðŸ§® Trainable params CLIP=427,616,513 | head=2,362,369\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Load TRAIN data ---------------------------\n",
    "print(f\"ðŸ”§ Loading TRAIN CSV: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# checks\n",
    "for col, name in [(TEXT_COL, \"TEXT_COL\"), (PRICE_COL, \"PRICE_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"{name} '{col}' not in CSV columns={df.columns.tolist()}\")\n",
    "\n",
    "# clean\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "df = df.loc[pd.to_numeric(df[PRICE_COL], errors=\"coerce\").notnull()].copy()\n",
    "df[PRICE_COL] = df[PRICE_COL].astype(float)\n",
    "df = df.loc[df[PRICE_COL] >= 0.0].reset_index(drop=True)\n",
    "\n",
    "print(f\"ðŸ“¦ Train rows: {len(df)}\")\n",
    "\n",
    "# targets (log2)\n",
    "y_log = log2_price(df[PRICE_COL].values)\n",
    "\n",
    "# --------------------------- CLIP backbone ---------------------------\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device)\n",
    "price_head = ClipRegressionHead(embed_dim=clip_model.config.projection_dim, dropout=0.1).to(device)\n",
    "\n",
    "print(f\"ðŸ–¥ï¸ Device: {device}\")\n",
    "# Train end-to-end (set to False to freeze CLIP)\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"ðŸ§® Trainable params CLIP={count_parameters(clip_model):,} | head={count_parameters(price_head):,}\")\n",
    "\n",
    "train_ds = ClipPriceDataset(df, TEXT_COL, IMG_COL, y_log, processor, MAX_LEN, IMG_MISSING_POLICY, is_test=False)\n",
    "collate  = CollateClip(processor)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c40fc094-607f-4d5f-a195-5d8cd8222840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Warmup batch to tally missing imagesâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133/2161992421.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ TRAIN missing images: 0\n",
      "ðŸ—‘ï¸ TRAIN dropped (policy=zero): 0\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "params = list(clip_model.named_parameters()) + [(f\"head.{n}\", p) for n, p in price_head.named_parameters()]\n",
    "grouped = [\n",
    "    {\"params\": [p for n, p in params if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": [p for n, p in params if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(grouped, lr=LR)\n",
    "num_training_steps = EPOCHS * max(1, math.ceil(len(train_loader) / max(1, GRAD_ACCUM)))\n",
    "num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "# Warmup batch to tally missing images\n",
    "print(\"ðŸ”Ž Warmup batch to tally missing imagesâ€¦\")\n",
    "if len(train_loader) > 0:\n",
    "    _ = next(iter(train_loader))\n",
    "print(f\"âš ï¸ TRAIN missing images: {train_ds.missing_img_count}\")\n",
    "print(f\"ðŸ—‘ï¸ TRAIN dropped (policy={IMG_MISSING_POLICY}): {getattr(train_ds,'dropped_missing',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c3e7bb-d4c8-46ae-8b20-5205bd23a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_133/864095044.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 200/2344 loss=1.9227 reg=2.4379 con=0.3771\n",
      "epoch 1 step 400/2344 loss=1.5964 reg=1.9708 con=0.4733\n",
      "epoch 1 step 600/2344 loss=1.3037 reg=1.5771 con=0.4835\n",
      "epoch 1 step 800/2344 loss=1.1081 reg=1.3277 con=0.4492\n",
      "epoch 1 step 1000/2344 loss=0.9790 reg=1.1677 con=0.4127\n",
      "epoch 1 step 1200/2344 loss=0.8868 reg=1.0545 con=0.3837\n",
      "epoch 1 step 1400/2344 loss=0.8195 reg=0.9732 con=0.3584\n",
      "epoch 1 step 1600/2344 loss=0.7667 reg=0.9086 con=0.3410\n",
      "epoch 1 step 1800/2344 loss=0.7253 reg=0.8588 con=0.3248\n",
      "epoch 1 step 2000/2344 loss=0.6892 reg=0.8156 con=0.3099\n",
      "epoch 1 step 2200/2344 loss=0.6608 reg=0.7819 con=0.2973\n",
      "âœ… Epoch 1 done. avg_loss=0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 step 200/2344 loss=0.3039 reg=0.3685 con=0.1100\n",
      "epoch 2 step 400/2344 loss=0.2982 reg=0.3612 con=0.1094\n",
      "epoch 2 step 600/2344 loss=0.2981 reg=0.3611 con=0.1091\n",
      "epoch 2 step 800/2344 loss=0.2969 reg=0.3600 con=0.1075\n",
      "epoch 2 step 1000/2344 loss=0.2945 reg=0.3574 con=0.1058\n",
      "epoch 2 step 1200/2344 loss=0.2921 reg=0.3548 con=0.1040\n",
      "epoch 2 step 1400/2344 loss=0.2914 reg=0.3538 con=0.1044\n",
      "epoch 2 step 1600/2344 loss=0.2908 reg=0.3531 con=0.1039\n",
      "epoch 2 step 1800/2344 loss=0.2907 reg=0.3531 con=0.1036\n",
      "epoch 2 step 2000/2344 loss=0.2913 reg=0.3541 con=0.1031\n",
      "epoch 2 step 2200/2344 loss=0.2903 reg=0.3530 con=0.1022\n",
      "âœ… Epoch 2 done. avg_loss=0.2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 step 200/2344 loss=0.1991 reg=0.2433 con=0.0666\n",
      "epoch 3 step 400/2344 loss=0.1932 reg=0.2347 con=0.0684\n",
      "epoch 3 step 600/2344 loss=0.1982 reg=0.2414 con=0.0688\n",
      "epoch 3 step 800/2344 loss=0.1977 reg=0.2411 con=0.0673\n",
      "epoch 3 step 1000/2344 loss=0.1969 reg=0.2403 con=0.0668\n",
      "epoch 3 step 1200/2344 loss=0.1978 reg=0.2414 con=0.0670\n",
      "epoch 3 step 1400/2344 loss=0.1987 reg=0.2425 con=0.0674\n",
      "epoch 3 step 1600/2344 loss=0.1995 reg=0.2435 con=0.0674\n",
      "epoch 3 step 1800/2344 loss=0.2003 reg=0.2447 con=0.0672\n",
      "epoch 3 step 2000/2344 loss=0.2009 reg=0.2454 con=0.0672\n",
      "epoch 3 step 2200/2344 loss=0.2014 reg=0.2462 con=0.0669\n",
      "âœ… Epoch 3 done. avg_loss=0.2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 step 200/2344 loss=0.1359 reg=0.1642 con=0.0509\n",
      "epoch 4 step 400/2344 loss=0.1341 reg=0.1612 con=0.0528\n",
      "epoch 4 step 600/2344 loss=0.1329 reg=0.1595 con=0.0531\n",
      "epoch 4 step 800/2344 loss=0.1335 reg=0.1606 con=0.0522\n",
      "epoch 4 step 1000/2344 loss=0.1333 reg=0.1604 con=0.0521\n",
      "epoch 4 step 1200/2344 loss=0.1336 reg=0.1610 con=0.0513\n",
      "epoch 4 step 1400/2344 loss=0.1338 reg=0.1614 con=0.0512\n",
      "epoch 4 step 1600/2344 loss=0.1347 reg=0.1626 con=0.0511\n",
      "epoch 4 step 1800/2344 loss=0.1350 reg=0.1631 con=0.0505\n",
      "epoch 4 step 2000/2344 loss=0.1354 reg=0.1637 con=0.0506\n",
      "epoch 4 step 2200/2344 loss=0.1357 reg=0.1642 con=0.0505\n",
      "âœ… Epoch 4 done. avg_loss=0.1357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 step 200/2344 loss=0.0910 reg=0.1072 con=0.0423\n",
      "epoch 5 step 400/2344 loss=0.0920 reg=0.1089 con=0.0412\n",
      "epoch 5 step 600/2344 loss=0.0916 reg=0.1087 con=0.0404\n",
      "epoch 5 step 800/2344 loss=0.0917 reg=0.1089 con=0.0401\n",
      "epoch 5 step 1000/2344 loss=0.0914 reg=0.1086 con=0.0398\n",
      "epoch 5 step 1200/2344 loss=0.0923 reg=0.1098 con=0.0399\n",
      "epoch 5 step 1400/2344 loss=0.0926 reg=0.1102 con=0.0396\n",
      "epoch 5 step 1600/2344 loss=0.0922 reg=0.1098 con=0.0393\n",
      "epoch 5 step 1800/2344 loss=0.0925 reg=0.1102 con=0.0393\n",
      "epoch 5 step 2000/2344 loss=0.0928 reg=0.1106 con=0.0393\n",
      "epoch 5 step 2200/2344 loss=0.0931 reg=0.1111 con=0.0393\n",
      "âœ… Epoch 5 done. avg_loss=0.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 step 200/2344 loss=0.0668 reg=0.0784 con=0.0319\n",
      "epoch 6 step 400/2344 loss=0.0664 reg=0.0778 con=0.0321\n",
      "epoch 6 step 600/2344 loss=0.0660 reg=0.0774 con=0.0317\n",
      "epoch 6 step 800/2344 loss=0.0663 reg=0.0778 con=0.0318\n",
      "epoch 6 step 1000/2344 loss=0.0660 reg=0.0774 con=0.0316\n",
      "epoch 6 step 1200/2344 loss=0.0658 reg=0.0773 con=0.0314\n",
      "epoch 6 step 1400/2344 loss=0.0660 reg=0.0776 con=0.0310\n",
      "epoch 6 step 1600/2344 loss=0.0657 reg=0.0774 con=0.0309\n",
      "epoch 6 step 1800/2344 loss=0.0657 reg=0.0774 con=0.0308\n",
      "epoch 6 step 2000/2344 loss=0.0658 reg=0.0775 con=0.0306\n",
      "epoch 6 step 2200/2344 loss=0.0656 reg=0.0774 con=0.0304\n",
      "âœ… Epoch 6 done. avg_loss=0.0657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 step 200/2344 loss=0.0481 reg=0.0559 con=0.0246\n",
      "epoch 7 step 400/2344 loss=0.0476 reg=0.0550 con=0.0252\n",
      "epoch 7 step 600/2344 loss=0.0482 reg=0.0560 con=0.0249\n",
      "epoch 7 step 800/2344 loss=0.0475 reg=0.0550 con=0.0248\n",
      "epoch 7 step 1000/2344 loss=0.0469 reg=0.0544 con=0.0245\n",
      "epoch 7 step 1200/2344 loss=0.0466 reg=0.0539 con=0.0244\n",
      "epoch 7 step 1400/2344 loss=0.0463 reg=0.0536 con=0.0243\n",
      "epoch 7 step 1600/2344 loss=0.0462 reg=0.0534 con=0.0246\n",
      "epoch 7 step 1800/2344 loss=0.0460 reg=0.0533 con=0.0243\n",
      "epoch 7 step 2000/2344 loss=0.0460 reg=0.0533 con=0.0241\n",
      "epoch 7 step 2200/2344 loss=0.0456 reg=0.0528 con=0.0240\n",
      "âœ… Epoch 7 done. avg_loss=0.0457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 step 200/2344 loss=0.0326 reg=0.0366 con=0.0206\n",
      "epoch 8 step 400/2344 loss=0.0332 reg=0.0375 con=0.0202\n",
      "epoch 8 step 600/2344 loss=0.0333 reg=0.0379 con=0.0195\n",
      "epoch 8 step 800/2344 loss=0.0332 reg=0.0377 con=0.0194\n",
      "epoch 8 step 1000/2344 loss=0.0328 reg=0.0372 con=0.0196\n",
      "epoch 8 step 1200/2344 loss=0.0328 reg=0.0373 con=0.0195\n",
      "epoch 8 step 1400/2344 loss=0.0328 reg=0.0372 con=0.0193\n",
      "epoch 8 step 1600/2344 loss=0.0325 reg=0.0370 con=0.0190\n",
      "epoch 8 step 1800/2344 loss=0.0324 reg=0.0369 con=0.0190\n",
      "epoch 8 step 2000/2344 loss=0.0323 reg=0.0367 con=0.0189\n",
      "epoch 8 step 2200/2344 loss=0.0321 reg=0.0366 con=0.0189\n",
      "âœ… Epoch 8 done. avg_loss=0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 step 200/2344 loss=0.0235 reg=0.0257 con=0.0168\n",
      "epoch 9 step 400/2344 loss=0.0224 reg=0.0245 con=0.0160\n",
      "epoch 9 step 600/2344 loss=0.0220 reg=0.0241 con=0.0158\n",
      "epoch 9 step 800/2344 loss=0.0220 reg=0.0240 con=0.0158\n",
      "epoch 9 step 1000/2344 loss=0.0219 reg=0.0239 con=0.0158\n",
      "epoch 9 step 1200/2344 loss=0.0219 reg=0.0240 con=0.0158\n",
      "epoch 9 step 1400/2344 loss=0.0217 reg=0.0237 con=0.0156\n",
      "epoch 9 step 1600/2344 loss=0.0217 reg=0.0237 con=0.0156\n",
      "epoch 9 step 1800/2344 loss=0.0216 reg=0.0236 con=0.0156\n",
      "epoch 9 step 2000/2344 loss=0.0216 reg=0.0236 con=0.0155\n",
      "epoch 9 step 2200/2344 loss=0.0214 reg=0.0234 con=0.0155\n",
      "âœ… Epoch 9 done. avg_loss=0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 step 200/2344 loss=0.0146 reg=0.0146 con=0.0145\n",
      "epoch 10 step 400/2344 loss=0.0150 reg=0.0152 con=0.0145\n",
      "epoch 10 step 600/2344 loss=0.0148 reg=0.0148 con=0.0147\n",
      "epoch 10 step 800/2344 loss=0.0147 reg=0.0149 con=0.0142\n",
      "epoch 10 step 1000/2344 loss=0.0147 reg=0.0148 con=0.0143\n",
      "epoch 10 step 1200/2344 loss=0.0146 reg=0.0147 con=0.0143\n",
      "epoch 10 step 1400/2344 loss=0.0146 reg=0.0147 con=0.0141\n",
      "epoch 10 step 1600/2344 loss=0.0146 reg=0.0147 con=0.0141\n",
      "epoch 10 step 1800/2344 loss=0.0145 reg=0.0147 con=0.0140\n",
      "epoch 10 step 2000/2344 loss=0.0144 reg=0.0146 con=0.0139\n",
      "epoch 10 step 2200/2344 loss=0.0144 reg=0.0146 con=0.0138\n",
      "âœ… Epoch 10 done. avg_loss=0.0144\n",
      "ðŸ’¾ Saved full-data checkpoint to: price_clip_fulltrain/full_clip.pt\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- TRAIN (Full data) ---------------------------\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    clip_model.train(); price_head.train()\n",
    "    loss_run = reg_run = con_run = 0.0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        targets        = batch[\"target\"].to(device, non_blocking=True).float()\n",
    "        img_missing    = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            # Text features\n",
    "            txt_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Image features\n",
    "            do_vision = (\"pixel_values\" in batch) and (IMG_MISSING_POLICY != \"text_only\")\n",
    "            if do_vision:\n",
    "                pixel_values = batch[\"pixel_values\"].to(\n",
    "                    device,\n",
    "                    dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "                    non_blocking=True\n",
    "                )\n",
    "                img_feat = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "                if img_missing.any():\n",
    "                    img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "            else:\n",
    "                img_feat = torch.zeros_like(txt_feat)\n",
    "\n",
    "            # Normalize + fuse\n",
    "            txt_n = F.normalize(txt_feat, dim=-1)\n",
    "            img_n = F.normalize(img_feat, dim=-1)\n",
    "            fused = torch.cat([img_n, txt_n], dim=-1)\n",
    "\n",
    "            # Losses\n",
    "            pred_log = price_head(fused)\n",
    "            reg_loss = huber_loss(pred_log, targets, delta=HUBER_DELTA)\n",
    "\n",
    "            con_loss = torch.tensor(0.0, device=device, dtype=txt_n.dtype)\n",
    "            valid_idx = (img_missing == 0).nonzero(as_tuple=False).squeeze(-1)\n",
    "            if do_vision and valid_idx.numel() > 1 and ALPHA_CONTRAST > 0:\n",
    "                con_loss = info_nce(img_n[valid_idx], txt_n[valid_idx], tau=TAU)\n",
    "\n",
    "            loss = (1.0 - ALPHA_CONTRAST) * reg_loss + ALPHA_CONTRAST * con_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % GRAD_ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(list(clip_model.parameters()) + list(price_head.parameters()), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_run += float(loss.item())\n",
    "        reg_run  += float(reg_loss.item())\n",
    "        con_run  += float(con_loss.item()) if isinstance(con_loss, torch.Tensor) else float(con_loss)\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"epoch {epoch} step {step}/{len(train_loader)} \"\n",
    "                  f\"loss={loss_run/step:.4f} reg={reg_run/step:.4f} con={con_run/step:.4f}\")\n",
    "\n",
    "    print(f\"âœ… Epoch {epoch} done. avg_loss={loss_run/max(1,len(train_loader)):.4f}\")\n",
    "\n",
    "# Save final full-data checkpoint\n",
    "full_ckpt = os.path.join(OUTPUT_DIR, \"full_clip.pt\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"clip_state\": clip_model.state_dict(),\n",
    "        \"head_state\": price_head.state_dict(),\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"config\": {\n",
    "            \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "            \"TAU\": TAU,\n",
    "            \"MAX_LEN\": MAX_LEN,\n",
    "            \"projection_dim\": clip_model.config.projection_dim,\n",
    "            \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "            \"HUBER_DELTA\": HUBER_DELTA\n",
    "        },\n",
    "        \"columns\": {\"id\": ID_COL, \"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "    },\n",
    "    full_ckpt\n",
    ")\n",
    "print(f\"ðŸ’¾ Saved full-data checkpoint to: {full_ckpt}\")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_fulltrain.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_rows\": int(len(df)),\n",
    "        \"train_missing_images\": int(train_ds.missing_img_count),\n",
    "        \"dropped_train\": int(getattr(train_ds, \"dropped_missing\", 0)),\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"grad_accum\": GRAD_ACCUM,\n",
    "        \"fp16\": FP16,\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8830a29b-6df5-44e6-af98-14a01812b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loaded checkpoint from: price_clip_fulltrain/full_clip.pt\n",
      "ðŸ”¤ MODEL_ID=openai/clip-vit-large-patch14 | projection_dim=768 | IMG_MISSING_POLICY=zero | MAX_LEN=64\n",
      "ðŸ–¥ Device: cuda\n",
      "ðŸ§ª Test rows: 75000 | Missing images encountered (during getitem): 0\n",
      "ðŸ—‘ Dropped due to policy=drop: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3725dc1ff9f4fc3bfb8abc2aa492bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_133/3650335273.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# --- Inference: load best checkpoint and predict on TEST_CSV ---\n",
    "\n",
    "# %%\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "\n",
    "# ---- Config / paths ----\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"jl_fs/test.csv\")   # must contain ID + text + image path\n",
    "ID_COL          = os.environ.get(\"ID_COL\", \"sample_id\")\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "IMG_COL         = os.environ.get(\"IMG_COL\",  \"image_path\")\n",
    "\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_clip_fulltrain\")\n",
    "CKPT_PATH       = os.environ.get(\"CKPT_PATH\", os.path.join(OUTPUT_DIR, \"full_clip.pt\"))\n",
    "\n",
    "BATCH_SIZE      = int(os.environ.get(\"INF_BATCH_SIZE\", \"64\"))\n",
    "MAX_LEN_ENV     = os.environ.get(\"MAX_LEN\", None)  # if you want to override tokenizer max len\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "assert os.path.exists(CKPT_PATH), f\"Checkpoint not found at {CKPT_PATH}\"\n",
    "assert os.path.exists(TEST_CSV),  f\"Test CSV not found at {TEST_CSV}\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- Load checkpoint ----\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "model_id = ckpt.get(\"model_id\", \"openai/clip-vit-large-patch14\")\n",
    "cfg = ckpt.get(\"config\", {})\n",
    "projection_dim = cfg.get(\"projection_dim\")\n",
    "img_missing_policy = cfg.get(\"IMG_MISSING_POLICY\", \"zero\")\n",
    "max_len = int(cfg.get(\"MAX_LEN\", 64)) if MAX_LEN_ENV is None else int(MAX_LEN_ENV)\n",
    "\n",
    "print(f\"ðŸ“¦ Loaded checkpoint from: {CKPT_PATH}\")\n",
    "print(f\"ðŸ”¤ MODEL_ID={model_id} | projection_dim={projection_dim} | IMG_MISSING_POLICY={img_missing_policy} | MAX_LEN={max_len}\")\n",
    "\n",
    "# ---- Recreate processor & models ----\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "clip_model = CLIPModel.from_pretrained(model_id)\n",
    "clip_model.load_state_dict(ckpt[\"clip_state\"], strict=True)\n",
    "clip_model.to(device).eval()\n",
    "\n",
    "# Recreate and load regression head (same class as training cell)\n",
    "price_head = ClipRegressionHead(embed_dim=projection_dim, dropout=0.0)\n",
    "price_head.load_state_dict(ckpt[\"head_state\"], strict=True)\n",
    "price_head.to(device).eval()\n",
    "\n",
    "# ---- Load test data ----\n",
    "dft = pd.read_csv(TEST_CSV)\n",
    "dft[\"image_path\"] = dft[\"sample_id\"].apply(lambda x : f\"jl_fs/images/test/{x}.jpg\")\n",
    "for col, name in [(ID_COL, \"ID_COL\"), (TEXT_COL, \"TEXT_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in dft.columns:\n",
    "        raise ValueError(f\"{name} '{col}' missing from test CSV. Columns={dft.columns.tolist()}\")\n",
    "\n",
    "# Basic clean\n",
    "dft[TEXT_COL] = dft[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "dft[IMG_COL]  = dft[IMG_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# Build dataset/dataloader with no targets\n",
    "test_ds = ClipPriceDataset(\n",
    "    df=dft[[ID_COL, TEXT_COL, IMG_COL]].copy(),\n",
    "    text_col=TEXT_COL,\n",
    "    img_col=IMG_COL,\n",
    "    prices_log2=None,\n",
    "    processor=processor,\n",
    "    max_len=max_len,\n",
    "    policy=img_missing_policy\n",
    ")\n",
    "collate = CollateClip(processor)\n",
    "\n",
    "dl_te = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=2, pin_memory=True, collate_fn=collate\n",
    ")\n",
    "\n",
    "print(f\"ðŸ–¥ Device: {device}\")\n",
    "print(f\"ðŸ§ª Test rows: {len(test_ds)} | Missing images encountered (during getitem): {test_ds.missing_img_count}\")\n",
    "print(f\"ðŸ—‘ Dropped due to policy=drop: {getattr(test_ds, 'dropped_missing', 0)}\")\n",
    "\n",
    "# ---- Inference loop ----\n",
    "clip_model_dtype = next(clip_model.vision_model.parameters()).dtype\n",
    "preds_log2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl_te, total = len(dl_te)):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        img_missing    = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        # Text features\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            txt_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Image features depending on policy\n",
    "            do_vision = (\"pixel_values\" in batch) and (img_missing_policy != \"text_only\")\n",
    "            if do_vision:\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=clip_model_dtype, non_blocking=True)\n",
    "                img_feat = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "                if img_missing.any():\n",
    "                    img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "            else:\n",
    "                img_feat = torch.zeros_like(txt_feat)\n",
    "\n",
    "            # Normalize + fuse\n",
    "            txt_n = F.normalize(txt_feat, dim=-1)\n",
    "            img_n = F.normalize(img_feat, dim=-1)\n",
    "            fused = torch.cat([img_n, txt_n], dim=-1)\n",
    "\n",
    "            # Predict log2(price)\n",
    "            pred_log = price_head(fused)\n",
    "            preds_log2.append(pred_log.detach().float().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343e1695-40cb-4ba2-bfed-d1b32b3127f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done. Wrote 75000 predictions to: price_clip_fulltrain/test_predictions_clip_big_epoch-10_full_v2.csv\n",
      "   Missing images counted during dataset load: 0\n",
      "   Dropped rows (policy=drop): 0\n"
     ]
    }
   ],
   "source": [
    "# ---- Convert back to price (delog2) and save ----\n",
    "if len(preds_log2):\n",
    "    preds_log2 = np.concatenate(preds_log2, axis=0)\n",
    "    preds_log2_delog = np.pow(2,preds_log2)\n",
    "    preds_price = np.clip(preds_log2_delog, 0,2000)  # safe de-log clamp\n",
    "else:\n",
    "    preds_price = np.array([])\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    ID_COL: dft[ID_COL].values[: len(preds_price)],\n",
    "    \"price\": preds_price\n",
    "})\n",
    "pred_path = os.path.join(OUTPUT_DIR, \"test_predictions_clip_big_epoch-10_full_v2.csv\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "out.to_csv(pred_path, index=False)\n",
    "\n",
    "print(f\"âœ… Done. Wrote {len(out)} predictions to: {pred_path}\")\n",
    "print(f\"   Missing images counted during dataset load: {test_ds.missing_img_count}\")\n",
    "print(f\"   Dropped rows (policy=drop): {getattr(test_ds,'dropped_missing',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a918d4-4cc2-4e9e-a081-0c36c55da2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
