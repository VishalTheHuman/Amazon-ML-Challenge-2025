{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f82cc5a-72e0-415b-89e7-57e483af25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "\n",
    "df = pd.read_csv(\"jl_fs/train.csv\")\n",
    "df[\"image_path\"] = df[\"sample_id\"].apply(lambda x : f\"jl_fs/images/train/{x}.jpg\")\n",
    "df.to_csv(\"train_updated.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c0afab-6494-408e-b3dd-b1c35cd79075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "\n",
    "df = pd.read_csv(\"jl_fs/test.csv\")\n",
    "df[\"image_path\"] = df[\"sample_id\"].apply(lambda x : f\"jl_fs/images/test/{x}.jpg\")\n",
    "df.to_csv(\"test_updated.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6537ead-bd3b-48cf-994b-511e24fd87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    CLIPModel,\n",
    "    AutoProcessor,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98cf885c-9990-4c1d-bd76-7c121631c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "CSV_PATH        = os.environ.get(\"TRAIN_CSV\", \"train_updated.csv\")     # must contain text + price + image path\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "PRICE_COL       = os.environ.get(\"PRICE_COL\", \"price\")\n",
    "IMG_COL         = os.environ.get(\"IMG_COL\",  \"image_path\")             # local jpg path column\n",
    "ID_COL          = os.environ.get(\"ID_COL\",  \"sample_id\")\n",
    "\n",
    "# Model configurations\n",
    "CLIP_MODEL_ID   = os.environ.get(\"CLIP_MODEL_ID\", \"openai/clip-vit-large-patch14\")  # Using base CLIP\n",
    "BERT_MODEL_ID   = os.environ.get(\"BERT_MODEL_ID\", \"intfloat/e5-base-v2\")             # BERT for text\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_large_clip_infloat_bert_hybrid_10\")\n",
    "\n",
    "SEED            = int(os.environ.get(\"SEED\", \"42\"))\n",
    "MAX_LEN         = int(os.environ.get(\"MAX_LEN\", \"77\"))                 # CLIP text context is shorter\n",
    "BATCH_SIZE      = int(os.environ.get(\"BATCH_SIZE\", \"24\"))\n",
    "LR              = float(os.environ.get(\"LR\", \"2e-5\"))\n",
    "WEIGHT_DECAY    = float(os.environ.get(\"WEIGHT_DECAY\", \"0.01\"))\n",
    "EPOCHS          = int(os.environ.get(\"EPOCHS\", \"10\"))\n",
    "WARMUP_RATIO    = float(os.environ.get(\"WARMUP_RATIO\", \"0.06\"))\n",
    "GRAD_ACCUM      = int(os.environ.get(\"GRAD_ACCUM\", \"1\"))\n",
    "MAX_GRAD_NORM   = float(os.environ.get(\"MAX_GRAD_NORM\", \"1.0\"))\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "# Loss & regularization\n",
    "ALPHA_CONTRAST  = float(os.environ.get(\"ALPHA_CONTRAST\", \"0.25\"))      # weight for contrastive loss (0..1)\n",
    "TAU             = float(os.environ.get(\"TAU\", \"0.07\"))                 # InfoNCE temperature\n",
    "HUBER_DELTA     = float(os.environ.get(\"HUBER_DELTA\", \"1.0\"))\n",
    "\n",
    "# Cross-attention configuration\n",
    "NUM_ATTENTION_HEADS = int(os.environ.get(\"NUM_ATTENTION_HEADS\", \"8\"))  # Number of cross-attention heads\n",
    "ATTENTION_DROPOUT   = float(os.environ.get(\"ATTENTION_DROPOUT\", \"0.1\")) # Dropout for attention layers\n",
    "\n",
    "# Price/log transform\n",
    "MIN_PRICE       = float(os.environ.get(\"MIN_PRICE\", \"1e-6\"))\n",
    "\n",
    "# Missing image policy for TRAIN: zero | text_only | drop\n",
    "IMG_MISSING_POLICY = os.environ.get(\"IMG_MISSING_POLICY\", \"zero\").lower()\n",
    "assert IMG_MISSING_POLICY in {\"zero\", \"text_only\", \"drop\"}\n",
    "\n",
    "# Inference/Test config (optional)\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"test_updated.csv\").strip()               # if \"\", inference is skipped\n",
    "TEST_IMG_DIR    = os.environ.get(\"TEST_IMG_DIR\", \"jl_fs/images/test\")  # used if test CSV lacks image_path\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc64e3e-cb0d-43ea-b394-13a89a64981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- Utils ---------------------------\n",
    "\n",
    "# --------------------------- New Loss: SMAPE on de-logged values ---------------------------\n",
    "\n",
    "import math\n",
    "\n",
    "# ---------- Helper losses ----------\n",
    "def smape_on_prices(pred_price: torch.Tensor, target_price: torch.Tensor, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    SMAPE fraction in [0, 2]. Multiply by 100 for percent.\n",
    "    pred_price, target_price: same shape, >=0\n",
    "    \"\"\"\n",
    "    num = torch.abs(pred_price - target_price)\n",
    "    denom = (torch.abs(pred_price) + torch.abs(target_price) + eps) / 2.0\n",
    "    return (num / denom).mean()  # fraction\n",
    "\n",
    "def pseudo_huber_loss(pred: torch.Tensor, target: torch.Tensor, delta: float = 1.0):\n",
    "    \"\"\"\n",
    "    Pseudo-Huber on inputs (works similar to Huber but smoother).\n",
    "    Inputs are assumed to be log-space (differences additive).\n",
    "    \"\"\"\n",
    "    x = pred - target\n",
    "    scale = delta\n",
    "    return torch.mean(scale**2 * (torch.sqrt(1.0 + (x/scale)**2) - 1.0))\n",
    "\n",
    "# ---------- Composite regression loss (with learned uncertainties + curriculum) ----------\n",
    "class CompositeRegLoss(nn.Module):\n",
    "    def __init__(self, base: float = 2.0, init_log_var_huber: float = 0.0, init_log_var_smape: float = 0.0,\n",
    "                 use_pseudo_huber: bool = False):\n",
    "        \"\"\"\n",
    "        base: base for log transform (e.g., 2.0 if using log2).\n",
    "        init_log_var_*: initial log-variance scalars (learnable) to scale task losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        # learnable log-variances (s1, s2). Initialize near 0.\n",
    "        self.log_var_huber = nn.Parameter(torch.tensor(init_log_var_huber, dtype=torch.float32))\n",
    "        self.log_var_smape = nn.Parameter(torch.tensor(init_log_var_smape, dtype=torch.float32))\n",
    "        self.use_pseudo_huber = use_pseudo_huber\n",
    "\n",
    "    def forward(self, pred_log: torch.Tensor, target_log: torch.Tensor,\n",
    "                epoch: int = 0, total_epochs: int = 1,\n",
    "                huber_delta: float = 1.0):\n",
    "        \"\"\"\n",
    "        Returns: reg_loss (scalar), dict of components {huber, smape, reg_loss_unweighted, s1, s2}\n",
    "        pred_log, target_log: tensors in log_{base} space (shape [B])\n",
    "        epoch, total_epochs: used for curriculum annealing (0..1)\n",
    "        \"\"\"\n",
    "        # 1) Huber-like on log-space\n",
    "        if self.use_pseudo_huber:\n",
    "            L_H = pseudo_huber_loss(pred_log, target_log, delta=huber_delta)\n",
    "        else:\n",
    "            L_H = F.huber_loss(pred_log, target_log, delta=huber_delta, reduction=\"mean\")\n",
    "\n",
    "        # 2) SMAPE computed on de-logged prices\n",
    "        # Convert back to price-space differentiably:\n",
    "        pred_price = torch.pow(self.base, pred_log)          # 2**log2\n",
    "        target_price = torch.pow(self.base, target_log)\n",
    "        L_S = smape_on_prices(pred_price, target_price)      # fraction 0..2\n",
    "\n",
    "        # Optionally scale SMAPE to similar magnitude as huber early on:\n",
    "        # We normalize L_S by a running scalar if you want; but learned log_var takes care of scale.\n",
    "\n",
    "        # 3) Curriculum scheduling: gradually shift weight from Huber -> SMAPE\n",
    "        # schedule parameter tau in [0,1]: tau=0 => only huber; tau=1 => only smape\n",
    "        if total_epochs <= 1:\n",
    "            tau = 0.5\n",
    "        else:\n",
    "            tau = float(epoch) / float(max(1, total_epochs - 1))  # linearly 0..1 over epochs\n",
    "\n",
    "        # combine with curriculum: weighted linear blend (can be tuned)\n",
    "        # compute weighted components:\n",
    "        # You can change blend formula; this is simple and works well in practice.\n",
    "        L_H_eff = (1.0 - tau) * L_H\n",
    "        L_S_eff = (tau) * L_S\n",
    "\n",
    "        # 4) Kendall-style learned uncertainty weighting\n",
    "        s1 = self.log_var_huber   # scalar\n",
    "        s2 = self.log_var_smape\n",
    "\n",
    "        # NLL style combination:\n",
    "        loss_term_h = 0.5 * torch.exp(-s1) * L_H_eff + 0.5 * s1\n",
    "        loss_term_s = 0.5 * torch.exp(-s2) * L_S_eff + 0.5 * s2\n",
    "\n",
    "        reg_loss = loss_term_h + loss_term_s\n",
    "\n",
    "        return reg_loss, {\n",
    "            \"L_H\": L_H.detach(),\n",
    "            \"L_S\": L_S.detach(),\n",
    "            \"L_H_eff\": L_H_eff.detach(),\n",
    "            \"L_S_eff\": L_S_eff.detach(),\n",
    "            \"s1\": s1.detach(),\n",
    "            \"s2\": s2.detach(),\n",
    "            \"reg_loss_unweighted\": (L_H_eff + L_S_eff).detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def log2_price(p: np.ndarray) -> np.ndarray:\n",
    "    return np.log2(np.clip(p, MIN_PRICE, None))\n",
    "\n",
    "def delog2(x: np.ndarray) -> np.ndarray:\n",
    "    return np.power(2.0, x)\n",
    "\n",
    "# --------------------------- Dataset & Collate ---------------------------\n",
    "class ClipBertPriceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TRAIN:\n",
    "      policy 'zero':      returns dummy pixel for missing images; later masked to zeros.\n",
    "      policy 'text_only': returns no pixel_values for missing images; vision forward skipped.\n",
    "      policy 'drop':      drops rows with missing images at dataset build time.\n",
    "\n",
    "    TEST:\n",
    "      We will ALWAYS behave like 'zero' (never drop predictions).\n",
    "\n",
    "    Items may contain: clip_input_ids, clip_attention_mask, bert_input_ids, bert_attention_mask, (pixel_values), img_missing, (target)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, text_col: str, img_col: str,\n",
    "                 prices_log2: Optional[np.ndarray],\n",
    "                 clip_processor: AutoProcessor, bert_tokenizer: AutoTokenizer, \n",
    "                 max_len: int, policy: str, is_test: bool = False):\n",
    "        self.clip_processor = clip_processor\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.policy = policy\n",
    "        self.is_test = is_test\n",
    "\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "        if (policy == \"drop\") and (not is_test):\n",
    "            before = len(df)\n",
    "            df = df[df[img_col].apply(lambda p: isinstance(p, str) and len(p) > 0 and os.path.exists(p))]\n",
    "            self.dropped_missing = before - len(df)\n",
    "        else:\n",
    "            self.dropped_missing = 0\n",
    "\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.img_paths = df[img_col].fillna(\"\").astype(str).tolist()\n",
    "        self.prices_log2 = prices_log2\n",
    "        self.missing_img_count = 0\n",
    "\n",
    "        # Dummy pixel to get correct shape\n",
    "        dummy = self.clip_processor(images=Image.new(\"RGB\", (224, 224)), return_tensors=\"pt\")\n",
    "        self._dummy_pixel = dummy[\"pixel_values\"].squeeze(0)  # (C,H,W)\n",
    "\n",
    "        # For exporting IDs in test predictions\n",
    "        self.ids = df[ID_COL].tolist() if (ID_COL in df.columns) else list(range(len(df)))\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def _load_image(self, path: str):\n",
    "        if isinstance(path, str) and path and os.path.exists(path):\n",
    "            try:\n",
    "                return Image.open(path).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.missing_img_count += 1\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        img  = self._load_image(self.img_paths[idx])\n",
    "\n",
    "        # CLIP text encoding\n",
    "        clip_enc_text = self.clip_processor(text=[text], padding=False, truncation=True,\n",
    "                                           max_length=self.max_len, return_tensors=\"pt\")\n",
    "        \n",
    "        # BERT text encoding\n",
    "        bert_enc_text = self.bert_tokenizer(text, padding=False, truncation=True,\n",
    "                                           max_length=192, return_tensors=\"pt\")\n",
    "\n",
    "        img_missing = 0\n",
    "        pixel_values = None\n",
    "\n",
    "        if img is None:\n",
    "            img_missing = 1\n",
    "            if self.is_test:\n",
    "                # For test we NEVER drop; force zero-like behavior\n",
    "                pixel_values = self._dummy_pixel.clone()\n",
    "            else:\n",
    "                if self.policy == \"zero\":\n",
    "                    pixel_values = self._dummy_pixel.clone()\n",
    "                elif self.policy == \"text_only\":\n",
    "                    pixel_values = None\n",
    "                elif self.policy == \"drop\":\n",
    "                    # should not occur because drop was handled in __init__\n",
    "                    pixel_values = self._dummy_pixel.clone()\n",
    "        else:\n",
    "            enc_img = self.clip_processor(images=img, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        item = {\n",
    "            \"clip_input_ids\": clip_enc_text[\"input_ids\"].squeeze(0),\n",
    "            \"clip_attention_mask\": clip_enc_text[\"attention_mask\"].squeeze(0),\n",
    "            \"bert_input_ids\": bert_enc_text[\"input_ids\"].squeeze(0),\n",
    "            \"bert_attention_mask\": bert_enc_text[\"attention_mask\"].squeeze(0),\n",
    "            \"img_missing\": torch.tensor(img_missing, dtype=torch.uint8),\n",
    "            \"row_id\": torch.tensor(self.ids[idx], dtype=torch.long)\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            item[\"pixel_values\"] = pixel_values\n",
    "        if self.prices_log2 is not None:\n",
    "            item[\"target\"] = torch.tensor(self.prices_log2[idx], dtype=torch.float32)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de9393c3-6394-48d6-bbd2-aa7b457974b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class CollateClipBert:\n",
    "    clip_processor: AutoProcessor\n",
    "    bert_tokenizer: AutoTokenizer\n",
    "    def __call__(self, batch):\n",
    "        # pad CLIP text\n",
    "        clip_input_ids = [b[\"clip_input_ids\"] for b in batch]\n",
    "        clip_attention = [b[\"clip_attention_mask\"] for b in batch]\n",
    "        clip_text_padded = self.clip_processor.tokenizer.pad(\n",
    "            {\"input_ids\": clip_input_ids, \"attention_mask\": clip_attention},\n",
    "            padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # pad BERT text\n",
    "        bert_input_ids = [b[\"bert_input_ids\"] for b in batch]\n",
    "        bert_attention = [b[\"bert_attention_mask\"] for b in batch]\n",
    "        bert_text_padded = self.bert_tokenizer.pad(\n",
    "            {\"input_ids\": bert_input_ids, \"attention_mask\": bert_attention},\n",
    "            padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # images: some may be absent (text_only policy)\n",
    "        has_pix = [(\"pixel_values\" in b) for b in batch]\n",
    "        pixel_values = None\n",
    "        if any(has_pix):\n",
    "            shapes = [b[\"pixel_values\"].shape for b in batch if \"pixel_values\" in b]\n",
    "            C,H,W = shapes[0]\n",
    "            stacked = []\n",
    "            for b in batch:\n",
    "                if \"pixel_values\" in b:\n",
    "                    stacked.append(b[\"pixel_values\"])\n",
    "                else:\n",
    "                    stacked.append(torch.zeros((C,H,W), dtype=torch.float32))\n",
    "            pixel_values = torch.stack(stacked, dim=0)\n",
    "\n",
    "        res = {\n",
    "            \"clip_input_ids\": clip_text_padded[\"input_ids\"],\n",
    "            \"clip_attention_mask\": clip_text_padded[\"attention_mask\"],\n",
    "            \"bert_input_ids\": bert_text_padded[\"input_ids\"],\n",
    "            \"bert_attention_mask\": bert_text_padded[\"attention_mask\"],\n",
    "            \"img_missing\": torch.stack([b[\"img_missing\"] for b in batch], dim=0),\n",
    "            \"row_id\": torch.stack([b[\"row_id\"] for b in batch], dim=0),\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            res[\"pixel_values\"] = pixel_values\n",
    "        if \"target\" in batch[0]:\n",
    "            res[\"target\"] = torch.stack([b[\"target\"] for b in batch], dim=0)\n",
    "        return res\n",
    "\n",
    "# --------------------------- Model & Loss ---------------------------\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, attention_mask=None):\n",
    "        B, N, D = query.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.q_proj(query).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(key).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(value).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "            \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, N, D)\n",
    "        \n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "class ClipBertHybridModel(nn.Module):\n",
    "    def __init__(self, clip_model, bert_model, clip_dim: int, bert_dim: int, \n",
    "                 num_attention_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.bert_model = bert_model\n",
    "        \n",
    "        # Project BERT features to CLIP dimension for cross-attention\n",
    "        self.bert_proj = nn.Linear(bert_dim, clip_dim)\n",
    "        \n",
    "        # Cross-attention layers\n",
    "        self.cross_attn_1 = MultiHeadCrossAttention(clip_dim, num_attention_heads, dropout)\n",
    "        self.cross_attn_2 = MultiHeadCrossAttention(clip_dim, num_attention_heads, dropout)\n",
    "        \n",
    "        # Final regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(3 * clip_dim, clip_dim),  # clip_img + clip_txt + bert_txt\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(clip_dim, clip_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(clip_dim // 2, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, clip_input_ids, clip_attention_mask, bert_input_ids, bert_attention_mask, \n",
    "                pixel_values, img_missing):\n",
    "        # Get CLIP features\n",
    "        clip_txt_feat = self.clip_model.get_text_features(\n",
    "            input_ids=clip_input_ids, attention_mask=clip_attention_mask\n",
    "        )\n",
    "        clip_img_feat = self.clip_model.get_image_features(pixel_values=pixel_values)\n",
    "        \n",
    "        # Mask image features for missing images\n",
    "        if img_missing.any():\n",
    "            clip_img_feat = clip_img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "        \n",
    "        # Get BERT/DistilBERT features\n",
    "        bert_outputs = self.bert_model(\n",
    "            input_ids=bert_input_ids, \n",
    "            attention_mask=bert_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # DistilBERT doesn't have pooler_output, use last_hidden_state[:, 0, :] instead\n",
    "        if hasattr(bert_outputs, 'pooler_output') and bert_outputs.pooler_output is not None:\n",
    "            # For BERT models with pooler\n",
    "            bert_feat = bert_outputs.pooler_output\n",
    "        else:\n",
    "            # For DistilBERT and models without pooler - use [CLS] token\n",
    "            bert_feat = bert_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        bert_feat = self.bert_proj(bert_feat)  # Project to CLIP dimension\n",
    "        \n",
    "        # Normalize features\n",
    "        clip_txt_norm = F.normalize(clip_txt_feat, dim=-1)\n",
    "        clip_img_norm = F.normalize(clip_img_feat, dim=-1)\n",
    "        bert_norm = F.normalize(bert_feat, dim=-1)\n",
    "        \n",
    "        # Cross-attention between CLIP text and BERT features\n",
    "        # BERT attends to CLIP text\n",
    "        bert_attended = self.cross_attn_1(\n",
    "            query=bert_norm.unsqueeze(1),  # Add sequence dimension\n",
    "            key=clip_txt_norm.unsqueeze(1),\n",
    "            value=clip_txt_norm.unsqueeze(1)\n",
    "        ).squeeze(1)\n",
    "        \n",
    "        # CLIP text attends to BERT features\n",
    "        clip_txt_attended = self.cross_attn_2(\n",
    "            query=clip_txt_norm.unsqueeze(1),\n",
    "            key=bert_norm.unsqueeze(1),\n",
    "            value=bert_norm.unsqueeze(1)\n",
    "        ).squeeze(1)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        fused_features = torch.cat([clip_img_norm, clip_txt_attended, bert_attended], dim=-1)\n",
    "        \n",
    "        # Final prediction\n",
    "        price_pred = self.regression_head(fused_features).squeeze(-1)\n",
    "        \n",
    "        return price_pred, clip_txt_norm, clip_img_norm, bert_attended\n",
    "\n",
    "def info_nce(z_img: torch.Tensor, z_txt: torch.Tensor, tau: float = 0.07) -> torch.Tensor:\n",
    "    z_img = F.normalize(z_img, dim=-1)\n",
    "    z_txt = F.normalize(z_txt, dim=-1)\n",
    "    logits = torch.matmul(z_img, z_txt.t()) / tau  # (B,B)\n",
    "    labels = torch.arange(z_img.size(0), device=z_img.device)\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.t(), labels)\n",
    "    return 0.5 * (loss_i + loss_t)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72670a55-1fc0-4b25-94a0-10a3ee80a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading TRAIN CSV: train_updated.csv\n",
      "üì¶ Train rows: 75000\n",
      "üìä Train split: 67500 | Val split: 7500\n",
      "üîÑ Loading CLIP model: openai/clip-vit-large-patch14\n",
      "üîÑ Loading BERT model: intfloat/e5-base-v2\n",
      "üñ•Ô∏è Device: cuda\n",
      "üßÆ Trainable params CLIP=427,616,513 | BERT=109,482,240 | Hybrid=544,480,002\n",
      "üîÑ Train batches: 2813 | Val batches: 313\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Load TRAIN data ---------------------------\n",
    "print(f\"üîß Loading TRAIN CSV: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# checks\n",
    "for col, name in [(TEXT_COL, \"TEXT_COL\"), (PRICE_COL, \"PRICE_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"{name} '{col}' not in CSV columns={df.columns.tolist()}\")\n",
    "# clean\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "df = df.loc[pd.to_numeric(df[PRICE_COL], errors=\"coerce\").notnull()].copy()\n",
    "df[PRICE_COL] = df[PRICE_COL].astype(float)\n",
    "df = df.loc[df[PRICE_COL] >= 0.0].reset_index(drop=True)\n",
    "print(f\"üì¶ Train rows: {len(df)}\")\n",
    "\n",
    "# --------------------------- Train/Val Split (90/10) ---------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.1, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Train split: {len(train_df)} | Val split: {len(val_df)}\")\n",
    "\n",
    "# targets (log2)\n",
    "y_train_log = log2_price(train_df[PRICE_COL].values)\n",
    "y_val_log = log2_price(val_df[PRICE_COL].values)\n",
    "\n",
    "# --------------------------- Load Models ---------------------------\n",
    "print(f\"üîÑ Loading CLIP model: {CLIP_MODEL_ID}\")\n",
    "clip_processor = AutoProcessor.from_pretrained(CLIP_MODEL_ID)\n",
    "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID)\n",
    "print(f\"üîÑ Loading BERT model: {BERT_MODEL_ID}\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_ID)\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_ID)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "# Create hybrid model\n",
    "hybrid_model = ClipBertHybridModel(\n",
    "    clip_model=clip_model,\n",
    "    bert_model=bert_model,\n",
    "    clip_dim=clip_model.config.projection_dim,\n",
    "    bert_dim=bert_model.config.hidden_size,\n",
    "    num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "    dropout=ATTENTION_DROPOUT\n",
    ").to(device)\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "\n",
    "# Train end-to-end (set to False to freeze models)\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in bert_model.parameters():\n",
    "    p.requires_grad = True\n",
    "    \n",
    "print(f\"üßÆ Trainable params CLIP={count_parameters(clip_model):,} | BERT={count_parameters(bert_model):,} | Hybrid={count_parameters(hybrid_model):,}\")\n",
    "\n",
    "# --------------------------- Create Datasets & Loaders ---------------------------\n",
    "train_ds = ClipBertPriceDataset(\n",
    "    train_df, TEXT_COL, IMG_COL, y_train_log, \n",
    "    clip_processor, bert_tokenizer, MAX_LEN, \n",
    "    IMG_MISSING_POLICY, is_test=False\n",
    ")\n",
    "val_ds = ClipBertPriceDataset(\n",
    "    val_df, TEXT_COL, IMG_COL, y_val_log, \n",
    "    clip_processor, bert_tokenizer, MAX_LEN, \n",
    "    IMG_MISSING_POLICY, is_test=False\n",
    ")\n",
    "\n",
    "composite_loss_fn = CompositeRegLoss(base=2.0, init_log_var_huber=0.0, init_log_var_smape=0.0,\n",
    "                                     use_pseudo_huber=False).to(device)\n",
    "\n",
    "collate = CollateClipBert(clip_processor, bert_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=2, \n",
    "    pin_memory=True, \n",
    "    collate_fn=collate\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,  # No shuffle for validation\n",
    "    num_workers=2, \n",
    "    pin_memory=True, \n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "print(f\"üîÑ Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40fc094-607f-4d5f-a195-5d8cd8222840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Warmup batch to tally missing images‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_compile.py:51: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n",
      "/tmp/ipykernel_4091/2621637727.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è TRAIN missing images: 0\n",
      "üóëÔ∏è TRAIN dropped (policy=zero): 0\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "params = (list(clip_model.named_parameters()) + \n",
    "          list(bert_model.named_parameters()) + \n",
    "          [(f\"hybrid.{n}\", p) for n, p in hybrid_model.named_parameters()])\n",
    "grouped = [\n",
    "    {\"params\": [p for n, p in params if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": [p for n, p in params if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(grouped, lr=LR)\n",
    "num_training_steps = EPOCHS * max(1, math.ceil(len(train_loader) / max(1, GRAD_ACCUM)))\n",
    "num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "# Warmup batch to tally missing images\n",
    "print(\"üîé Warmup batch to tally missing images‚Ä¶\")\n",
    "if len(train_loader) > 0:\n",
    "    _ = next(iter(train_loader))\n",
    "print(f\"‚ö†Ô∏è TRAIN missing images: {train_ds.missing_img_count}\")\n",
    "print(f\"üóëÔ∏è TRAIN dropped (policy={IMG_MISSING_POLICY}): {getattr(train_ds,'dropped_missing',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c3e7bb-d4c8-46ae-8b20-5205bd23a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------- TRAIN (Full data) ---------------------------\n",
    "# def evaluate_smape(model, data_loader, device, epoch):\n",
    "#     \"\"\"Evaluate SMAPE on a subset of training data\"\"\"\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     targets = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, batch in enumerate(data_loader):\n",
    "                \n",
    "#             clip_input_ids = batch[\"clip_input_ids\"].to(device, non_blocking=True)\n",
    "#             clip_attention_mask = batch[\"clip_attention_mask\"].to(device, non_blocking=True)\n",
    "#             bert_input_ids = batch[\"bert_input_ids\"].to(device, non_blocking=True)\n",
    "#             bert_attention_mask = batch[\"bert_attention_mask\"].to(device, non_blocking=True)\n",
    "#             targets_batch = batch[\"target\"].to(device, non_blocking=True).float()\n",
    "#             img_missing = batch[\"img_missing\"].to(device)\n",
    "            \n",
    "#             pixel_values = batch[\"pixel_values\"].to(\n",
    "#                 device,\n",
    "#                 dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "#                 non_blocking=True\n",
    "#             )\n",
    "            \n",
    "#             pred_log, _, _, _ = model(\n",
    "#                 clip_input_ids, clip_attention_mask, \n",
    "#                 bert_input_ids, bert_attention_mask,\n",
    "#                 pixel_values, img_missing\n",
    "#             )\n",
    "            \n",
    "#             # Convert back to original price scale\n",
    "#             pred_price = delog2(pred_log.cpu().numpy())\n",
    "#             target_price = delog2(targets_batch.cpu().numpy())\n",
    "            \n",
    "#             predictions.extend(pred_price)\n",
    "#             targets.extend(target_price)\n",
    "    \n",
    "#     model.train()\n",
    "#     return smape_np(targets, predictions)\n",
    "\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     hybrid_model.train()\n",
    "#     loss_run = reg_run = con_run = 0.0\n",
    "\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#     for step, batch in enumerate(train_loader, 1):\n",
    "#         clip_input_ids = batch[\"clip_input_ids\"].to(device, non_blocking=True)\n",
    "#         clip_attention_mask = batch[\"clip_attention_mask\"].to(device, non_blocking=True)\n",
    "#         bert_input_ids = batch[\"bert_input_ids\"].to(device, non_blocking=True)\n",
    "#         bert_attention_mask = batch[\"bert_attention_mask\"].to(device, non_blocking=True)\n",
    "#         targets = batch[\"target\"].to(device, non_blocking=True).float()\n",
    "#         img_missing = batch[\"img_missing\"].to(device)\n",
    "\n",
    "#         with torch.cuda.amp.autocast(enabled=FP16):\n",
    "#             # Get predictions from hybrid model\n",
    "#             pixel_values = batch[\"pixel_values\"].to(\n",
    "#                 device,\n",
    "#                 dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "#                 non_blocking=True\n",
    "#             )\n",
    "            \n",
    "#             pred_log, clip_txt_norm, clip_img_norm, bert_attended = hybrid_model(\n",
    "#                 clip_input_ids, clip_attention_mask,\n",
    "#                 bert_input_ids, bert_attention_mask,\n",
    "#                 pixel_values, img_missing\n",
    "#             )\n",
    "            \n",
    "#             # Regression loss\n",
    "#             reg_loss = huber_loss(pred_log, targets, delta=HUBER_DELTA)\n",
    "\n",
    "#             # Contrastive loss between CLIP image and text features\n",
    "#             con_loss = torch.tensor(0.0, device=device, dtype=clip_txt_norm.dtype)\n",
    "#             valid_idx = (img_missing == 0).nonzero(as_tuple=False).squeeze(-1)\n",
    "#             if valid_idx.numel() > 1 and ALPHA_CONTRAST > 0:\n",
    "#                 con_loss = info_nce(clip_img_norm[valid_idx], clip_txt_norm[valid_idx], tau=TAU)\n",
    "\n",
    "#             loss = (1.0 - ALPHA_CONTRAST) * reg_loss + ALPHA_CONTRAST * con_loss\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "\n",
    "#         if step % GRAD_ACCUM == 0:\n",
    "#             scaler.unscale_(optimizer)\n",
    "#             nn.utils.clip_grad_norm_(\n",
    "#                 list(clip_model.parameters()) + \n",
    "#                 list(bert_model.parameters()) + \n",
    "#                 list(hybrid_model.parameters()), \n",
    "#                 MAX_GRAD_NORM\n",
    "#             )\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             scheduler.step()\n",
    "\n",
    "#         loss_run += float(loss.item())\n",
    "#         reg_run  += float(reg_loss.item())\n",
    "#         con_run  += float(con_loss.item()) if isinstance(con_loss, torch.Tensor) else float(con_loss)\n",
    "\n",
    "#         if step % 200 == 0:\n",
    "#             print(f\"epoch {epoch} step {step}/{len(train_loader)} \"\n",
    "#                   f\"loss={loss_run/step:.4f} reg={reg_run/step:.4f} con={con_run/step:.4f}\")\n",
    "\n",
    "#     # Evaluate SMAPE at end of epoch\n",
    "#     smape_score = evaluate_smape(hybrid_model, val_loader, device, epoch)\n",
    "#     print(f\"‚úÖ Epoch {epoch} done. avg_loss={loss_run/max(1,len(train_loader)):.4f} | SMAPE={smape_score:.4f}\")\n",
    "\n",
    "# # Save final full-data checkpoint\n",
    "# full_ckpt = os.path.join(OUTPUT_DIR, \"clip_bert_hybrid.pt\")\n",
    "# torch.save(\n",
    "#     {\n",
    "#         \"clip_state\": clip_model.state_dict(),\n",
    "#         \"bert_state\": bert_model.state_dict(),\n",
    "#         \"hybrid_state\": hybrid_model.state_dict(),\n",
    "#         \"clip_model_id\": CLIP_MODEL_ID,\n",
    "#         \"bert_model_id\": BERT_MODEL_ID,\n",
    "#         \"config\": {\n",
    "#             \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "#             \"TAU\": TAU,\n",
    "#             \"MAX_LEN\": MAX_LEN,\n",
    "#             \"clip_projection_dim\": clip_model.config.projection_dim,\n",
    "#             \"bert_hidden_size\": bert_model.config.hidden_size,\n",
    "#             \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "#             \"attention_dropout\": ATTENTION_DROPOUT,\n",
    "#             \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "#             \"HUBER_DELTA\": HUBER_DELTA\n",
    "#         },\n",
    "#         \"columns\": {\"id\": ID_COL, \"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "#     },\n",
    "#     full_ckpt\n",
    "# )\n",
    "# print(f\"üíæ Saved hybrid model checkpoint to: {full_ckpt}\")\n",
    "\n",
    "# with open(os.path.join(OUTPUT_DIR, \"metrics_hybrid_train.json\"), \"w\") as f:\n",
    "#     json.dump({\n",
    "#         \"train_rows\": int(len(df)),\n",
    "#         \"train_missing_images\": int(train_ds.missing_img_count),\n",
    "#         \"dropped_train\": int(getattr(train_ds, \"dropped_missing\", 0)),\n",
    "#         \"epochs\": EPOCHS,\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"lr\": LR,\n",
    "#         \"weight_decay\": WEIGHT_DECAY,\n",
    "#         \"grad_accum\": GRAD_ACCUM,\n",
    "#         \"fp16\": FP16,\n",
    "#         \"clip_model\": CLIP_MODEL_ID,\n",
    "#         \"bert_model\": BERT_MODEL_ID,\n",
    "#         \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "#     }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee2f54-5e29-4d3d-9296-b1aa968fdba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_1584/2403332194.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 200/2813 loss=1.3338 reg=1.6646 con=0.3416\n",
      "epoch 1 step 400/2813 loss=1.1023 reg=1.3921 con=0.2330\n",
      "epoch 1 step 600/2813 loss=0.8322 reg=1.0447 con=0.1946\n",
      "epoch 1 step 800/2813 loss=0.6872 reg=0.8590 con=0.1717\n",
      "epoch 1 step 1000/2813 loss=0.5967 reg=0.7430 con=0.1576\n",
      "epoch 1 step 1200/2813 loss=0.5361 reg=0.6652 con=0.1485\n",
      "epoch 1 step 1400/2813 loss=0.4930 reg=0.6082 con=0.1471\n",
      "epoch 1 step 1600/2813 loss=0.4605 reg=0.5654 con=0.1459\n",
      "epoch 1 step 1800/2813 loss=0.4347 reg=0.5305 con=0.1471\n",
      "epoch 1 step 2000/2813 loss=0.4131 reg=0.5022 con=0.1459\n",
      "epoch 1 step 2200/2813 loss=0.3953 reg=0.4788 con=0.1447\n",
      "epoch 1 step 2400/2813 loss=0.3801 reg=0.4587 con=0.1442\n",
      "epoch 1 step 2600/2813 loss=0.3662 reg=0.4412 con=0.1412\n",
      "epoch 1 step 2800/2813 loss=0.3532 reg=0.4251 con=0.1376\n",
      "\n",
      "üìä Evaluating epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 completed:\n",
      "   Val   -> SMAPE: 48.9081 | MAE: 11.22 | RMSE: 25.67 | R¬≤: 0.3636\n",
      "\n",
      "üíæ Saved epoch 1 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_1.pt\n",
      "‚≠ê New best model! Val SMAPE: 48.9081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 step 200/2813 loss=0.1531 reg=0.1902 con=0.0421\n",
      "epoch 2 step 400/2813 loss=0.1555 reg=0.1930 con=0.0429\n",
      "epoch 2 step 600/2813 loss=0.1599 reg=0.1958 con=0.0522\n",
      "epoch 2 step 800/2813 loss=0.1603 reg=0.1960 con=0.0531\n",
      "epoch 2 step 1000/2813 loss=0.1642 reg=0.1989 con=0.0602\n",
      "epoch 2 step 1200/2813 loss=0.1670 reg=0.2015 con=0.0633\n",
      "epoch 2 step 1400/2813 loss=0.1682 reg=0.2020 con=0.0666\n",
      "epoch 2 step 1600/2813 loss=0.1695 reg=0.2031 con=0.0684\n",
      "epoch 2 step 1800/2813 loss=0.1702 reg=0.2037 con=0.0696\n",
      "epoch 2 step 2000/2813 loss=0.1701 reg=0.2033 con=0.0705\n",
      "epoch 2 step 2200/2813 loss=0.1703 reg=0.2034 con=0.0710\n",
      "epoch 2 step 2400/2813 loss=0.1706 reg=0.2032 con=0.0726\n",
      "epoch 2 step 2600/2813 loss=0.1705 reg=0.2032 con=0.0725\n",
      "epoch 2 step 2800/2813 loss=0.1710 reg=0.2037 con=0.0728\n",
      "\n",
      "üìä Evaluating epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 completed:\n",
      "   Val   -> SMAPE: 47.7622 | MAE: 10.98 | RMSE: 25.19 | R¬≤: 0.3872\n",
      "\n",
      "üíæ Saved epoch 2 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_2.pt\n",
      "‚≠ê New best model! Val SMAPE: 47.7622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 step 200/2813 loss=0.1316 reg=0.1632 con=0.0369\n",
      "epoch 3 step 400/2813 loss=0.1266 reg=0.1582 con=0.0319\n",
      "epoch 3 step 600/2813 loss=0.1261 reg=0.1579 con=0.0306\n",
      "epoch 3 step 800/2813 loss=0.1253 reg=0.1572 con=0.0297\n",
      "epoch 3 step 1000/2813 loss=0.1249 reg=0.1569 con=0.0287\n",
      "epoch 3 step 1200/2813 loss=0.1250 reg=0.1571 con=0.0289\n",
      "epoch 3 step 1400/2813 loss=0.1258 reg=0.1582 con=0.0289\n",
      "epoch 3 step 1600/2813 loss=0.1266 reg=0.1591 con=0.0292\n",
      "epoch 3 step 1800/2813 loss=0.1270 reg=0.1596 con=0.0294\n",
      "epoch 3 step 2000/2813 loss=0.1270 reg=0.1596 con=0.0294\n",
      "epoch 3 step 2200/2813 loss=0.1267 reg=0.1592 con=0.0292\n",
      "epoch 3 step 2400/2813 loss=0.1263 reg=0.1587 con=0.0289\n",
      "epoch 3 step 2600/2813 loss=0.1260 reg=0.1585 con=0.0285\n",
      "epoch 3 step 2800/2813 loss=0.1253 reg=0.1576 con=0.0284\n",
      "\n",
      "üìä Evaluating epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 completed:\n",
      "   Val   -> SMAPE: 44.2533 | MAE: 9.92 | RMSE: 23.38 | R¬≤: 0.4720\n",
      "\n",
      "üíæ Saved epoch 3 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_3.pt\n",
      "‚≠ê New best model! Val SMAPE: 44.2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 step 200/2813 loss=0.0901 reg=0.1139 con=0.0188\n",
      "epoch 4 step 400/2813 loss=0.0896 reg=0.1134 con=0.0184\n",
      "epoch 4 step 600/2813 loss=0.0898 reg=0.1136 con=0.0183\n",
      "epoch 4 step 800/2813 loss=0.0898 reg=0.1136 con=0.0185\n",
      "epoch 4 step 1000/2813 loss=0.0913 reg=0.1153 con=0.0193\n",
      "epoch 4 step 1200/2813 loss=0.0927 reg=0.1168 con=0.0205\n",
      "epoch 4 step 1400/2813 loss=0.0939 reg=0.1184 con=0.0203\n",
      "epoch 4 step 1600/2813 loss=0.0951 reg=0.1200 con=0.0205\n",
      "epoch 4 step 1800/2813 loss=0.0964 reg=0.1216 con=0.0207\n",
      "epoch 4 step 2000/2813 loss=0.0968 reg=0.1221 con=0.0208\n",
      "epoch 4 step 2200/2813 loss=0.0977 reg=0.1232 con=0.0211\n",
      "epoch 4 step 2400/2813 loss=0.0985 reg=0.1241 con=0.0215\n",
      "epoch 4 step 2600/2813 loss=0.0991 reg=0.1249 con=0.0217\n",
      "epoch 4 step 2800/2813 loss=0.0994 reg=0.1253 con=0.0218\n",
      "\n",
      "üìä Evaluating epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4 completed:\n",
      "   Val   -> SMAPE: 44.8528 | MAE: 10.12 | RMSE: 23.56 | R¬≤: 0.4641\n",
      "\n",
      "üíæ Saved epoch 4 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 step 200/2813 loss=0.0831 reg=0.1044 con=0.0191\n",
      "epoch 5 step 400/2813 loss=0.0817 reg=0.1028 con=0.0183\n",
      "epoch 5 step 600/2813 loss=0.0797 reg=0.1002 con=0.0182\n",
      "epoch 5 step 800/2813 loss=0.0789 reg=0.0993 con=0.0178\n",
      "epoch 5 step 1000/2813 loss=0.0782 reg=0.0985 con=0.0174\n",
      "epoch 5 step 1200/2813 loss=0.0782 reg=0.0986 con=0.0172\n",
      "epoch 5 step 1400/2813 loss=0.0783 reg=0.0987 con=0.0172\n",
      "epoch 5 step 1600/2813 loss=0.0784 reg=0.0988 con=0.0170\n",
      "epoch 5 step 1800/2813 loss=0.0782 reg=0.0986 con=0.0169\n",
      "epoch 5 step 2000/2813 loss=0.0779 reg=0.0982 con=0.0169\n",
      "epoch 5 step 2200/2813 loss=0.0776 reg=0.0978 con=0.0169\n",
      "epoch 5 step 2400/2813 loss=0.0775 reg=0.0977 con=0.0169\n",
      "epoch 5 step 2600/2813 loss=0.0776 reg=0.0978 con=0.0167\n",
      "epoch 5 step 2800/2813 loss=0.0773 reg=0.0976 con=0.0167\n",
      "\n",
      "üìä Evaluating epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5 completed:\n",
      "   Val   -> SMAPE: 43.8980 | MAE: 9.85 | RMSE: 22.70 | R¬≤: 0.5024\n",
      "\n",
      "üíæ Saved epoch 5 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_5.pt\n",
      "‚≠ê New best model! Val SMAPE: 43.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 step 200/2813 loss=0.0628 reg=0.0792 con=0.0136\n",
      "epoch 6 step 400/2813 loss=0.0631 reg=0.0794 con=0.0143\n",
      "epoch 6 step 600/2813 loss=0.0623 reg=0.0784 con=0.0142\n",
      "epoch 6 step 800/2813 loss=0.0623 reg=0.0784 con=0.0140\n",
      "epoch 6 step 1000/2813 loss=0.0628 reg=0.0791 con=0.0140\n",
      "epoch 6 step 1200/2813 loss=0.0631 reg=0.0794 con=0.0142\n",
      "epoch 6 step 1400/2813 loss=0.0632 reg=0.0795 con=0.0142\n",
      "epoch 6 step 1600/2813 loss=0.0629 reg=0.0791 con=0.0142\n",
      "epoch 6 step 1800/2813 loss=0.0627 reg=0.0789 con=0.0141\n",
      "epoch 6 step 2000/2813 loss=0.0623 reg=0.0784 con=0.0139\n",
      "epoch 6 step 2200/2813 loss=0.0620 reg=0.0781 con=0.0139\n",
      "epoch 6 step 2400/2813 loss=0.0619 reg=0.0779 con=0.0138\n",
      "epoch 6 step 2600/2813 loss=0.0621 reg=0.0782 con=0.0138\n",
      "epoch 6 step 2800/2813 loss=0.0625 reg=0.0787 con=0.0138\n",
      "\n",
      "üìä Evaluating epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6 completed:\n",
      "   Val   -> SMAPE: 44.2246 | MAE: 10.10 | RMSE: 23.40 | R¬≤: 0.4714\n",
      "\n",
      "üíæ Saved epoch 6 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 step 200/2813 loss=0.0673 reg=0.0846 con=0.0155\n",
      "epoch 7 step 400/2813 loss=0.0654 reg=0.0822 con=0.0149\n",
      "epoch 7 step 600/2813 loss=0.0659 reg=0.0831 con=0.0143\n",
      "epoch 7 step 800/2813 loss=0.0659 reg=0.0832 con=0.0142\n",
      "epoch 7 step 1000/2813 loss=0.0655 reg=0.0827 con=0.0140\n",
      "epoch 7 step 1200/2813 loss=0.0655 reg=0.0827 con=0.0139\n",
      "epoch 7 step 1400/2813 loss=0.0656 reg=0.0828 con=0.0140\n",
      "epoch 7 step 1600/2813 loss=0.0657 reg=0.0829 con=0.0142\n",
      "epoch 7 step 1800/2813 loss=0.0656 reg=0.0827 con=0.0142\n",
      "epoch 7 step 2000/2813 loss=0.0653 reg=0.0823 con=0.0143\n",
      "epoch 7 step 2200/2813 loss=0.0651 reg=0.0821 con=0.0141\n",
      "epoch 7 step 2400/2813 loss=0.0647 reg=0.0816 con=0.0139\n",
      "epoch 7 step 2600/2813 loss=0.0644 reg=0.0813 con=0.0138\n",
      "epoch 7 step 2800/2813 loss=0.0640 reg=0.0808 con=0.0137\n",
      "\n",
      "üìä Evaluating epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 7 completed:\n",
      "   Val   -> SMAPE: 43.3284 | MAE: 9.68 | RMSE: 22.28 | R¬≤: 0.5206\n",
      "\n",
      "üíæ Saved epoch 7 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_7.pt\n",
      "‚≠ê New best model! Val SMAPE: 43.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 step 200/2813 loss=0.0637 reg=0.0807 con=0.0125\n",
      "epoch 8 step 400/2813 loss=0.0638 reg=0.0810 con=0.0121\n",
      "epoch 8 step 600/2813 loss=0.0638 reg=0.0811 con=0.0121\n",
      "epoch 8 step 800/2813 loss=0.0642 reg=0.0812 con=0.0131\n",
      "epoch 8 step 1000/2813 loss=0.0644 reg=0.0814 con=0.0132\n",
      "epoch 8 step 1200/2813 loss=0.0644 reg=0.0815 con=0.0132\n",
      "epoch 8 step 1400/2813 loss=0.0643 reg=0.0814 con=0.0130\n",
      "epoch 8 step 1600/2813 loss=0.0642 reg=0.0813 con=0.0130\n",
      "epoch 8 step 1800/2813 loss=0.0641 reg=0.0811 con=0.0131\n",
      "epoch 8 step 2000/2813 loss=0.0640 reg=0.0811 con=0.0129\n",
      "epoch 8 step 2200/2813 loss=0.0640 reg=0.0810 con=0.0129\n",
      "epoch 8 step 2400/2813 loss=0.0639 reg=0.0809 con=0.0128\n",
      "epoch 8 step 2600/2813 loss=0.0637 reg=0.0807 con=0.0127\n",
      "epoch 8 step 2800/2813 loss=0.0634 reg=0.0803 con=0.0127\n",
      "\n",
      "üìä Evaluating epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 8 completed:\n",
      "   Val   -> SMAPE: 42.7806 | MAE: 9.59 | RMSE: 22.54 | R¬≤: 0.5095\n",
      "\n",
      "üíæ Saved epoch 8 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_8.pt\n",
      "‚≠ê New best model! Val SMAPE: 42.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 step 200/2813 loss=0.0556 reg=0.0707 con=0.0104\n",
      "epoch 9 step 400/2813 loss=0.0566 reg=0.0719 con=0.0109\n",
      "epoch 9 step 600/2813 loss=0.0564 reg=0.0717 con=0.0106\n",
      "epoch 9 step 800/2813 loss=0.0561 reg=0.0714 con=0.0105\n",
      "epoch 9 step 1000/2813 loss=0.0562 reg=0.0714 con=0.0106\n",
      "epoch 9 step 1200/2813 loss=0.0563 reg=0.0715 con=0.0106\n",
      "epoch 9 step 1400/2813 loss=0.0560 reg=0.0711 con=0.0105\n",
      "epoch 9 step 1600/2813 loss=0.0558 reg=0.0709 con=0.0105\n",
      "epoch 9 step 1800/2813 loss=0.0557 reg=0.0708 con=0.0103\n",
      "epoch 9 step 2000/2813 loss=0.0555 reg=0.0706 con=0.0104\n",
      "epoch 9 step 2200/2813 loss=0.0554 reg=0.0703 con=0.0104\n",
      "epoch 9 step 2400/2813 loss=0.0552 reg=0.0701 con=0.0104\n",
      "epoch 9 step 2600/2813 loss=0.0551 reg=0.0700 con=0.0103\n",
      "epoch 9 step 2800/2813 loss=0.0550 reg=0.0699 con=0.0104\n",
      "\n",
      "üìä Evaluating epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 9 completed:\n",
      "   Val   -> SMAPE: 42.3347 | MAE: 9.47 | RMSE: 22.35 | R¬≤: 0.5175\n",
      "\n",
      "üíæ Saved epoch 9 checkpoint to: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_9.pt\n",
      "‚≠ê New best model! Val SMAPE: 42.3347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 step 200/2813 loss=0.0502 reg=0.0637 con=0.0098\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- TRAIN (Full data) ---------------------------\n",
    "def evaluate_metrics(model, data_loader, device, split_name=\"val\"):\n",
    "    \"\"\"Evaluate comprehensive metrics on a data loader\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            clip_input_ids = batch[\"clip_input_ids\"].to(device, non_blocking=True)\n",
    "            clip_attention_mask = batch[\"clip_attention_mask\"].to(device, non_blocking=True)\n",
    "            bert_input_ids = batch[\"bert_input_ids\"].to(device, non_blocking=True)\n",
    "            bert_attention_mask = batch[\"bert_attention_mask\"].to(device, non_blocking=True)\n",
    "            targets_batch = batch[\"target\"].to(device, non_blocking=True).float()\n",
    "            img_missing = batch[\"img_missing\"].to(device)\n",
    "            \n",
    "            pixel_values = batch[\"pixel_values\"].to(\n",
    "                device,\n",
    "                dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "                non_blocking=True\n",
    "            )\n",
    "            \n",
    "            pred_log, _, _, _ = model(\n",
    "                clip_input_ids, clip_attention_mask, \n",
    "                bert_input_ids, bert_attention_mask,\n",
    "                pixel_values, img_missing\n",
    "            )\n",
    "            \n",
    "            # Convert back to original price scale\n",
    "            pred_price = delog2(pred_log.cpu().numpy())\n",
    "            target_price = delog2(targets_batch.cpu().numpy())\n",
    "            \n",
    "            predictions.extend(pred_price)\n",
    "            targets.extend(target_price)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    smape = smape_np(targets, predictions)\n",
    "    mae = np.mean(np.abs(targets - predictions))\n",
    "    rmse = np.sqrt(np.mean((targets - predictions) ** 2))\n",
    "    \n",
    "    # R¬≤ score\n",
    "    ss_res = np.sum((targets - predictions) ** 2)\n",
    "    ss_tot = np.sum((targets - np.mean(targets)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        f\"{split_name}_smape\": float(smape),\n",
    "        f\"{split_name}_mae\": float(mae),\n",
    "        f\"{split_name}_rmse\": float(rmse),\n",
    "        f\"{split_name}_r2\": float(r2)\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize metrics history\n",
    "metrics_history = []\n",
    "best_val_smape = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    hybrid_model.train()\n",
    "    loss_run = reg_run = con_run = 0.0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        clip_input_ids = batch[\"clip_input_ids\"].to(device, non_blocking=True)\n",
    "        clip_attention_mask = batch[\"clip_attention_mask\"].to(device, non_blocking=True)\n",
    "        bert_input_ids = batch[\"bert_input_ids\"].to(device, non_blocking=True)\n",
    "        bert_attention_mask = batch[\"bert_attention_mask\"].to(device, non_blocking=True)\n",
    "        targets = batch[\"target\"].to(device, non_blocking=True).float()\n",
    "        img_missing = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            pixel_values = batch[\"pixel_values\"].to(\n",
    "                device,\n",
    "                dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "                non_blocking=True\n",
    "            )\n",
    "            \n",
    "            pred_log, clip_txt_norm, clip_img_norm, bert_attended = hybrid_model(\n",
    "                clip_input_ids, clip_attention_mask,\n",
    "                bert_input_ids, bert_attention_mask,\n",
    "                pixel_values, img_missing\n",
    "            )\n",
    "            \n",
    "            # Regression loss\n",
    "            reg_loss = reg_loss, reg_info = composite_loss_fn(pred_log.squeeze(), targets.squeeze(),\n",
    "                                       epoch=epoch-1, total_epochs=EPOCHS,\n",
    "                                       huber_delta=HUBER_DELTA)\n",
    "            # huber_loss(pred_log, targets, delta=HUBER_DELTA)\n",
    "\n",
    "            # Contrastive loss between CLIP image and text features\n",
    "            con_loss = torch.tensor(0.0, device=device, dtype=clip_txt_norm.dtype)\n",
    "            valid_idx = (img_missing == 0).nonzero(as_tuple=False).squeeze(-1)\n",
    "            if valid_idx.numel() > 1 and ALPHA_CONTRAST > 0:\n",
    "                con_loss = info_nce(clip_img_norm[valid_idx], clip_txt_norm[valid_idx], tau=TAU)\n",
    "\n",
    "            loss = (1.0 - ALPHA_CONTRAST) * reg_loss + ALPHA_CONTRAST * con_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % GRAD_ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                list(clip_model.parameters()) + \n",
    "                list(bert_model.parameters()) + \n",
    "                list(hybrid_model.parameters()), \n",
    "                MAX_GRAD_NORM\n",
    "            )\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_run += float(loss.item())\n",
    "        reg_run  += float(reg_loss.item())\n",
    "        con_run  += float(con_loss.item()) if isinstance(con_loss, torch.Tensor) else float(con_loss)\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"epoch {epoch} step {step}/{len(train_loader)} \"\n",
    "                  f\"loss={loss_run/step:.4f} reg={reg_run/step:.4f} con={con_run/step:.4f}\")\n",
    "\n",
    "    # Evaluate metrics at end of epoch\n",
    "    print(f\"\\nüìä Evaluating epoch {epoch}...\")\n",
    "    # train_metrics = evaluate_metrics(hybrid_model, train_loader, device, split_name=\"train\")\n",
    "    val_metrics = evaluate_metrics(hybrid_model, val_loader, device, split_name=\"val\")\n",
    "    \n",
    "    # Combine all metrics for this epoch\n",
    "    epoch_metrics = {\n",
    "        \"epoch\": epoch,\n",
    "        \"avg_loss\": loss_run / max(1, len(train_loader)),\n",
    "        \"avg_reg_loss\": reg_run / max(1, len(train_loader)),\n",
    "        \"avg_con_loss\": con_run / max(1, len(train_loader)),\n",
    "        **val_metrics\n",
    "    }\n",
    "    metrics_history.append(epoch_metrics)\n",
    "    \n",
    "    print(f\"‚úÖ Epoch {epoch} completed:\")\n",
    "    # print(f\"   Train -> SMAPE: {train_metrics['train_smape']:.4f} | MAE: {train_metrics['train_mae']:.2f} | \"\n",
    "    #       f\"RMSE: {train_metrics['train_rmse']:.2f} | R¬≤: {train_metrics['train_r2']:.4f}\")\n",
    "    print(f\"   Val   -> SMAPE: {val_metrics['val_smape']:.4f} | MAE: {val_metrics['val_mae']:.2f} | \"\n",
    "          f\"RMSE: {val_metrics['val_rmse']:.2f} | R¬≤: {val_metrics['val_r2']:.4f}\\n\")\n",
    "    \n",
    "    # Save checkpoint for this epoch\n",
    "    epoch_ckpt = os.path.join(OUTPUT_DIR, f\"clip_bert_hybrid_epoch_{epoch}.pt\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"clip_state\": clip_model.state_dict(),\n",
    "            \"bert_state\": bert_model.state_dict(),\n",
    "            \"hybrid_state\": hybrid_model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict(),\n",
    "            \"scaler_state\": scaler.state_dict(),\n",
    "            \"clip_model_id\": CLIP_MODEL_ID,\n",
    "            \"bert_model_id\": BERT_MODEL_ID,\n",
    "            \"config\": {\n",
    "                \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "                \"TAU\": TAU,\n",
    "                \"MAX_LEN\": MAX_LEN,\n",
    "                \"clip_projection_dim\": clip_model.config.projection_dim,\n",
    "                \"bert_hidden_size\": bert_model.config.hidden_size,\n",
    "                \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "                \"attention_dropout\": ATTENTION_DROPOUT,\n",
    "                \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "                \"HUBER_DELTA\": HUBER_DELTA\n",
    "            },\n",
    "            \"metrics\": epoch_metrics,\n",
    "            \"columns\": {\"id\": ID_COL, \"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "        },\n",
    "        epoch_ckpt\n",
    "    )\n",
    "    print(f\"üíæ Saved epoch {epoch} checkpoint to: {epoch_ckpt}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if val_metrics['val_smape'] < best_val_smape:\n",
    "        best_val_smape = val_metrics['val_smape']\n",
    "        best_epoch = epoch\n",
    "        best_ckpt = os.path.join(OUTPUT_DIR, \"clip_bert_hybrid_best.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"clip_state\": clip_model.state_dict(),\n",
    "                \"bert_state\": bert_model.state_dict(),\n",
    "                \"hybrid_state\": hybrid_model.state_dict(),\n",
    "                \"clip_model_id\": CLIP_MODEL_ID,\n",
    "                \"bert_model_id\": BERT_MODEL_ID,\n",
    "                \"config\": {\n",
    "                    \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "                    \"TAU\": TAU,\n",
    "                    \"MAX_LEN\": MAX_LEN,\n",
    "                    \"clip_projection_dim\": clip_model.config.projection_dim,\n",
    "                    \"bert_hidden_size\": bert_model.config.hidden_size,\n",
    "                    \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "                    \"attention_dropout\": ATTENTION_DROPOUT,\n",
    "                    \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "                    \"HUBER_DELTA\": HUBER_DELTA\n",
    "                },\n",
    "                \"metrics\": epoch_metrics,\n",
    "                \"columns\": {\"id\": ID_COL, \"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "            },\n",
    "            best_ckpt\n",
    "        )\n",
    "        print(f\"‚≠ê New best model! Val SMAPE: {best_val_smape:.4f}\")\n",
    "\n",
    "# Save final checkpoint\n",
    "final_ckpt = os.path.join(OUTPUT_DIR, \"clip_bert_hybrid_final.pt\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": EPOCHS,\n",
    "        \"clip_state\": clip_model.state_dict(),\n",
    "        \"bert_state\": bert_model.state_dict(),\n",
    "        \"hybrid_state\": hybrid_model.state_dict(),\n",
    "        \"clip_model_id\": CLIP_MODEL_ID,\n",
    "        \"bert_model_id\": BERT_MODEL_ID,\n",
    "        \"config\": {\n",
    "            \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "            \"TAU\": TAU,\n",
    "            \"MAX_LEN\": MAX_LEN,\n",
    "            \"clip_projection_dim\": clip_model.config.projection_dim,\n",
    "            \"bert_hidden_size\": bert_model.config.hidden_size,\n",
    "            \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "            \"attention_dropout\": ATTENTION_DROPOUT,\n",
    "            \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "            \"HUBER_DELTA\": HUBER_DELTA\n",
    "        },\n",
    "        \"metrics\": metrics_history[-1],\n",
    "        \"columns\": {\"id\": ID_COL, \"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "    },\n",
    "    final_ckpt\n",
    ")\n",
    "print(f\"üíæ Saved final model checkpoint to: {final_ckpt}\")\n",
    "\n",
    "# Save comprehensive metrics history\n",
    "metrics_file = os.path.join(OUTPUT_DIR, \"training_metrics_history.json\")\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"training_info\": {\n",
    "            \"train_rows\": int(len(train_df)),\n",
    "            \"val_rows\": int(len(val_df)),\n",
    "            \"train_missing_images\": int(train_ds.missing_img_count),\n",
    "            \"val_missing_images\": int(val_ds.missing_img_count),\n",
    "            \"dropped_train\": int(getattr(train_ds, \"dropped_missing\", 0)),\n",
    "            \"dropped_val\": int(getattr(val_ds, \"dropped_missing\", 0)),\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"lr\": LR,\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "            \"grad_accum\": GRAD_ACCUM,\n",
    "            \"fp16\": FP16,\n",
    "            \"clip_model\": CLIP_MODEL_ID,\n",
    "            \"bert_model\": BERT_MODEL_ID,\n",
    "            \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_val_smape\": float(best_val_smape)\n",
    "        },\n",
    "        \"metrics_per_epoch\": metrics_history\n",
    "    }, f, indent=2)\n",
    "print(f\"üìà Saved training metrics history to: {metrics_file}\")\n",
    "\n",
    "print(f\"\\nüéØ Training completed! Best model at epoch {best_epoch} with Val SMAPE: {best_val_smape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0d522-a799-4552-8537-72af2afed5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#     train_ds, \n",
    "#     batch_size=48, \n",
    "#     shuffle=True,\n",
    "#     num_workers=2, \n",
    "#     pin_memory=True, \n",
    "#     collate_fn=collate\n",
    "# )\n",
    "# val_loader = DataLoader(\n",
    "#     val_ds, \n",
    "#     batch_size=48, \n",
    "#     shuffle=False,  # No shuffle for validation\n",
    "#     num_workers=2, \n",
    "#     pin_memory=True, \n",
    "#     collate_fn=collate\n",
    "# )\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     hybrid_model.train()\n",
    "#     loss_run = reg_run = con_run = 0.0\n",
    "\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#     for step, batch in enumerate(train_loader, 1):\n",
    "#         clip_input_ids = batch[\"clip_input_ids\"].to(device, non_blocking=True)\n",
    "#         clip_attention_mask = batch[\"clip_attention_mask\"].to(device, non_blocking=True)\n",
    "#         bert_input_ids = batch[\"bert_input_ids\"].to(device, non_blocking=True)\n",
    "#         bert_attention_mask = batch[\"bert_attention_mask\"].to(device, non_blocking=True)\n",
    "#         targets = batch[\"target\"].to(device, non_blocking=True).float()\n",
    "#         img_missing = batch[\"img_missing\"].to(device)\n",
    "\n",
    "#         with torch.cuda.amp.autocast(enabled=FP16):\n",
    "#             # Get predictions from hybrid model\n",
    "#             pixel_values = batch[\"pixel_values\"].to(\n",
    "#                 device,\n",
    "#                 dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "#                 non_blocking=True\n",
    "#             )\n",
    "            \n",
    "#             pred_log, clip_txt_norm, clip_img_norm, bert_attended = hybrid_model(\n",
    "#                 clip_input_ids, clip_attention_mask,\n",
    "#                 bert_input_ids, bert_attention_mask,\n",
    "#                 pixel_values, img_missing\n",
    "#             )\n",
    "            \n",
    "#             # Regression loss\n",
    "#             reg_loss = huber_loss(pred_log, targets, delta=HUBER_DELTA)\n",
    "\n",
    "#             # Contrastive loss between CLIP image and text features\n",
    "#             con_loss = torch.tensor(0.0, device=device, dtype=clip_txt_norm.dtype)\n",
    "#             valid_idx = (img_missing == 0).nonzero(as_tuple=False).squeeze(-1)\n",
    "#             if valid_idx.numel() > 1 and ALPHA_CONTRAST > 0:\n",
    "#                 con_loss = info_nce(clip_img_norm[valid_idx], clip_txt_norm[valid_idx], tau=TAU)\n",
    "\n",
    "#             loss = (1.0 - ALPHA_CONTRAST) * reg_loss + ALPHA_CONTRAST * con_loss\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "\n",
    "#         if step % GRAD_ACCUM == 0:\n",
    "#             scaler.unscale_(optimizer)\n",
    "#             nn.utils.clip_grad_norm_(\n",
    "#                 list(clip_model.parameters()) + \n",
    "#                 list(bert_model.parameters()) + \n",
    "#                 list(hybrid_model.parameters()), \n",
    "#                 MAX_GRAD_NORM\n",
    "#             )\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             scheduler.step()\n",
    "\n",
    "#         loss_run += float(loss.item())\n",
    "#         reg_run  += float(reg_loss.item())\n",
    "#         con_run  += float(con_loss.item()) if isinstance(con_loss, torch.Tensor) else float(con_loss)\n",
    "\n",
    "#         if step % 200 == 0:\n",
    "#             print(f\"epoch {epoch} step {step}/{len(train_loader)} \"\n",
    "#                   f\"loss={loss_run/step:.4f} reg={reg_run/step:.4f} con={con_run/step:.4f}\")\n",
    "\n",
    "#     # Evaluate SMAPE at end of epoch\n",
    "#     smape_score = evaluate_smape(hybrid_model, val_loader, device, epoch)\n",
    "#     print(f\"‚úÖ Epoch {epoch} done. avg_loss={loss_run/max(1,len(train_loader)):.4f} | SMAPE={smape_score:.4f}\")\n",
    "\n",
    "# # Save final full-data checkpoint\n",
    "# full_ckpt = os.path.join(OUTPUT_DIR, \"clip_bert_hybrid.pt\")\n",
    "# torch.save(\n",
    "#     {\n",
    "#         \"clip_state\": clip_model.state_dict(),\n",
    "#         \"bert_state\": bert_model.state_dict(),\n",
    "#         \"hybrid_state\": hybrid_model.state_dict(),\n",
    "#         \"clip_model_id\": CLIP_MODEL_ID,\n",
    "#         \"bert_model_id\": BERT_MODEL_ID,\n",
    "#         \"config\": {\n",
    "#             \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "#             \"TAU\": TAU,\n",
    "#             \"MAX_LEN\": MAX_LEN,\n",
    "#             \"clip_projection_dim\": clip_model.config.projection_dim,\n",
    "#             \"bert_hidden_size\": bert_model.config.hidden_size,\n",
    "#             \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "#             \"attention_dropout\": ATTENTION_DROPOUT,\n",
    "#             \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "#             \"HUBER_DELTA\": HUBER_DELTA\n",
    "#         },\n",
    "#         \"columns\": {\"id\": ID_COL, \"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "#     },\n",
    "#     full_ckpt\n",
    "# )\n",
    "# print(f\"üíæ Saved hybrid model checkpoint to: {full_ckpt}\")\n",
    "\n",
    "# with open(os.path.join(OUTPUT_DIR, \"metrics_hybrid_train.json\"), \"w\") as f:\n",
    "#     json.dump({\n",
    "#         \"train_rows\": int(len(df)),\n",
    "#         \"train_missing_images\": int(train_ds.missing_img_count),\n",
    "#         \"dropped_train\": int(getattr(train_ds, \"dropped_missing\", 0)),\n",
    "#         \"epochs\": EPOCHS,\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"lr\": LR,\n",
    "#         \"weight_decay\": WEIGHT_DECAY,\n",
    "#         \"grad_accum\": GRAD_ACCUM,\n",
    "#         \"fp16\": FP16,\n",
    "#         \"clip_model\": CLIP_MODEL_ID,\n",
    "#         \"bert_model\": BERT_MODEL_ID,\n",
    "#         \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "#     }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82be1e22-e95f-4cb3-a69e-74f7493ccf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Loading TEST_CSV: test_updated.csv\n",
      "üìù Test rows: 75000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------- INFERENCE (TEST) ---------------------------\n",
    "def build_test_df(path: str, id_col: str, text_col: str, img_col: str, fallback_img_dir: str) -> pd.DataFrame:\n",
    "    dft = pd.read_csv(path)\n",
    "    # ensure id col exists\n",
    "    if id_col not in dft.columns:\n",
    "        raise ValueError(f\"ID_COL '{id_col}' not in TEST_CSV columns={dft.columns.tolist()}\")\n",
    "\n",
    "    # text column can be absent; if so, create empty\n",
    "    if text_col not in dft.columns:\n",
    "        dft[text_col] = \"\"\n",
    "\n",
    "    # image path column: create if absent\n",
    "    if img_col not in dft.columns:\n",
    "        dft[img_col] = dft[id_col].astype(str).apply(lambda x: os.path.join(fallback_img_dir, f\"{x}.jpg\"))\n",
    "\n",
    "    dft[text_col] = dft[text_col].fillna(\"\").astype(str).str.strip()\n",
    "    dft[img_col]  = dft[img_col].fillna(\"\").astype(str)\n",
    "    return dft\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_predictions(hybrid_model: ClipBertHybridModel, df_test: pd.DataFrame,\n",
    "                      text_col: str, img_col: str, batch_size: int = 64) -> np.ndarray:\n",
    "    hybrid_model.eval()\n",
    "    ds = ClipBertPriceDataset(df_test, text_col, img_col, prices_log2=None,\n",
    "                             clip_processor=clip_processor, bert_tokenizer=bert_tokenizer,\n",
    "                             max_len=MAX_LEN, policy=\"zero\", is_test=True)\n",
    "    collate = CollateClipBert(clip_processor, bert_tokenizer)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "    preds_log = []\n",
    "    for batch in dl:\n",
    "        clip_input_ids = batch[\"clip_input_ids\"].to(device, non_blocking=True)\n",
    "        clip_attention_mask = batch[\"clip_attention_mask\"].to(device, non_blocking=True)\n",
    "        bert_input_ids = batch[\"bert_input_ids\"].to(device, non_blocking=True)\n",
    "        bert_attention_mask = batch[\"bert_attention_mask\"].to(device, non_blocking=True)\n",
    "        img_missing = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(\n",
    "            device,\n",
    "            dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "            non_blocking=True\n",
    "        )\n",
    "        \n",
    "        pred_log, _, _, _ = hybrid_model(\n",
    "            clip_input_ids, clip_attention_mask,\n",
    "            bert_input_ids, bert_attention_mask,\n",
    "            pixel_values, img_missing\n",
    "        )\n",
    "        preds_log.append(pred_log.detach().float().cpu().numpy())\n",
    "\n",
    "    preds_log = np.concatenate(preds_log, axis=0) if len(preds_log) else np.array([])\n",
    "    return preds_log, ds.ids, ds.missing_img_count\n",
    "\n",
    "if TEST_CSV:\n",
    "    print(f\"üîÆ Loading TEST_CSV: {TEST_CSV}\")\n",
    "    dft = build_test_df(TEST_CSV, ID_COL, TEXT_COL, IMG_COL, TEST_IMG_DIR)\n",
    "    print(f\"üìù Test rows: {len(dft)}\")\n",
    "\n",
    "    preds_log, test_ids, miss_count = infer_predictions(hybrid_model, dft, TEXT_COL, IMG_COL, batch_size=max(32, BATCH_SIZE))\n",
    "    if preds_log.size == 0:\n",
    "        print(\"‚ö†Ô∏è No predictions generated for test.\")\n",
    "    else:\n",
    "        pred_price = delog2(preds_log.reshape(-1))\n",
    "        out_df = pd.DataFrame({ID_COL: test_ids, \"price\": pred_price})\n",
    "        out_path = os.path.join(OUTPUT_DIR, \"test_predictions.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        print(f\"üì§ Saved predictions to: {out_path}\")\n",
    "        print(f\"‚ö†Ô∏è Test missing images encountered: {miss_count}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è TEST_CSV not set ‚Äî skipping inference.\")\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8830a29b-6df5-44e6-af98-14a01812b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loaded checkpoint from: price_large_clip_infloat_bert_hybrid_10/clip_bert_hybrid_epoch_10.pt\n",
      "üî§ CLIP_MODEL=openai/clip-vit-large-patch14 | BERT_MODEL=intfloat/e5-base-v2\n",
      "üî§ clip_dim=768 | bert_dim=768 | heads=8 | MAX_LEN=77\n",
      "üñ• Device: cuda\n",
      "üß™ Test rows: 75000 | Missing images encountered (during getitem): 0\n",
      "üóë Dropped due to policy=drop: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ff280770b544d2a729fe7679f65516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_4091/1161067771.py:117: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# --- Inference: load best checkpoint and predict on TEST_CSV ---\n",
    "\n",
    "# %%\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "\n",
    "# ---- Config / paths ----\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"test_updated.csv\")   # must contain ID + text + image path\n",
    "ID_COL          = os.environ.get(\"ID_COL\", \"sample_id\")\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "IMG_COL         = os.environ.get(\"IMG_COL\",  \"image_path\")\n",
    "\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_large_clip_infloat_bert_hybrid_10\")\n",
    "CKPT_PATH       = os.environ.get(\"CKPT_PATH\", os.path.join(OUTPUT_DIR, \"clip_bert_hybrid_epoch_10.pt\"))\n",
    "\n",
    "BATCH_SIZE      = int(os.environ.get(\"INF_BATCH_SIZE\", \"64\"))\n",
    "MAX_LEN_ENV     = os.environ.get(\"MAX_LEN\", None)  # if you want to override tokenizer max len\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "assert os.path.exists(CKPT_PATH), f\"Checkpoint not found at {CKPT_PATH}\"\n",
    "assert os.path.exists(TEST_CSV),  f\"Test CSV not found at {TEST_CSV}\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- Load checkpoint ----\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "clip_model_id = ckpt.get(\"clip_model_id\", \"openai/clip-vit-large-patch14\")\n",
    "bert_model_id = ckpt.get(\"bert_model_id\", \"distilbert/distilbert-base-cased\")\n",
    "cfg = ckpt.get(\"config\", {})\n",
    "clip_projection_dim = cfg.get(\"clip_projection_dim\")\n",
    "bert_hidden_size = cfg.get(\"bert_hidden_size\")\n",
    "num_attention_heads = cfg.get(\"num_attention_heads\", 8)\n",
    "attention_dropout = cfg.get(\"attention_dropout\", 0.1)\n",
    "img_missing_policy = cfg.get(\"IMG_MISSING_POLICY\", \"zero\")\n",
    "max_len = int(cfg.get(\"MAX_LEN\", 64)) if MAX_LEN_ENV is None else int(MAX_LEN_ENV)\n",
    "\n",
    "print(f\"üì¶ Loaded checkpoint from: {CKPT_PATH}\")\n",
    "print(f\"üî§ CLIP_MODEL={clip_model_id} | BERT_MODEL={bert_model_id}\")\n",
    "print(f\"üî§ clip_dim={clip_projection_dim} | bert_dim={bert_hidden_size} | heads={num_attention_heads} | MAX_LEN={max_len}\")\n",
    "\n",
    "# ---- Recreate processors & models ----\n",
    "clip_processor = AutoProcessor.from_pretrained(clip_model_id)\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_id)\n",
    "clip_model.load_state_dict(ckpt[\"clip_state\"], strict=True)\n",
    "clip_model.to(device).eval()\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_id)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_id)\n",
    "bert_model.load_state_dict(ckpt[\"bert_state\"], strict=True)\n",
    "bert_model.to(device).eval()\n",
    "\n",
    "# Recreate and load hybrid model\n",
    "hybrid_model = ClipBertHybridModel(\n",
    "    clip_model=clip_model,\n",
    "    bert_model=bert_model,\n",
    "    clip_dim=clip_projection_dim,\n",
    "    bert_dim=bert_hidden_size,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    dropout=0.0  # No dropout during inference\n",
    ")\n",
    "hybrid_model.load_state_dict(ckpt[\"hybrid_state\"], strict=True)\n",
    "hybrid_model.to(device).eval()\n",
    "\n",
    "# ---- Load test data ----\n",
    "dft = pd.read_csv(TEST_CSV)\n",
    "dft[\"image_path\"] = dft[\"sample_id\"].apply(lambda x : f\"jl_fs/images/test/{x}.jpg\")\n",
    "for col, name in [(ID_COL, \"ID_COL\"), (TEXT_COL, \"TEXT_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in dft.columns:\n",
    "        raise ValueError(f\"{name} '{col}' missing from test CSV. Columns={dft.columns.tolist()}\")\n",
    "\n",
    "# Basic clean\n",
    "dft[TEXT_COL] = dft[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "dft[IMG_COL]  = dft[IMG_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# Build dataset/dataloader with no targets\n",
    "test_ds = ClipBertPriceDataset(\n",
    "    df=dft[[ID_COL, TEXT_COL, IMG_COL]].copy(),\n",
    "    text_col=TEXT_COL,\n",
    "    img_col=IMG_COL,\n",
    "    prices_log2=None,\n",
    "    clip_processor=clip_processor,\n",
    "    bert_tokenizer=bert_tokenizer,\n",
    "    max_len=max_len,\n",
    "    policy=img_missing_policy\n",
    ")\n",
    "collate = CollateClipBert(clip_processor, bert_tokenizer)\n",
    "\n",
    "dl_te = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=2, pin_memory=True, collate_fn=collate\n",
    ")\n",
    "\n",
    "print(f\"üñ• Device: {device}\")\n",
    "print(f\"üß™ Test rows: {len(test_ds)} | Missing images encountered (during getitem): {test_ds.missing_img_count}\")\n",
    "print(f\"üóë Dropped due to policy=drop: {getattr(test_ds, 'dropped_missing', 0)}\")\n",
    "\n",
    "# ---- Inference loop ----\n",
    "clip_model_dtype = next(clip_model.vision_model.parameters()).dtype\n",
    "preds_log2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl_te, total = len(dl_te)):\n",
    "        clip_input_ids = batch[\"clip_input_ids\"].to(device, non_blocking=True)\n",
    "        clip_attention_mask = batch[\"clip_attention_mask\"].to(device, non_blocking=True)\n",
    "        bert_input_ids = batch[\"bert_input_ids\"].to(device, non_blocking=True)\n",
    "        bert_attention_mask = batch[\"bert_attention_mask\"].to(device, non_blocking=True)\n",
    "        img_missing = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        # Get predictions from hybrid model\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, dtype=clip_model_dtype, non_blocking=True)\n",
    "            \n",
    "            pred_log, _, _, _ = hybrid_model(\n",
    "                clip_input_ids, clip_attention_mask,\n",
    "                bert_input_ids, bert_attention_mask,\n",
    "                pixel_values, img_missing\n",
    "            )\n",
    "            preds_log2.append(pred_log.detach().float().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "343e1695-40cb-4ba2-bfed-d1b32b3127f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done. Wrote 75000 predictions to: price_large_clip_infloat_bert_hybrid_10/test_predictions_large_clip_infloat_bert_hybrid_smapeloss.csv\n",
      "   Missing images counted during dataset load: 0\n",
      "   Dropped rows (policy=drop): 0\n"
     ]
    }
   ],
   "source": [
    "# ---- Convert back to price (delog2) and save ----\n",
    "if len(preds_log2):\n",
    "    preds_log2 = np.concatenate(preds_log2, axis=0)\n",
    "    preds_log2_delog = np.pow(2,preds_log2)\n",
    "    preds_price = np.clip(preds_log2_delog, 0,10000)  # safe de-log clamp\n",
    "else:\n",
    "    preds_price = np.array([])\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    ID_COL: dft[ID_COL].values[: len(preds_price)],\n",
    "    \"price\": preds_price\n",
    "})\n",
    "pred_path = os.path.join(OUTPUT_DIR, \"test_predictions_large_clip_infloat_bert_hybrid_smapeloss.csv\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "out.to_csv(pred_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Done. Wrote {len(out)} predictions to: {pred_path}\")\n",
    "print(f\"   Missing images counted during dataset load: {test_ds.missing_img_count}\")\n",
    "print(f\"   Dropped rows (policy=drop): {getattr(test_ds,'dropped_missing',0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f33f8",
   "metadata": {},
   "source": [
    "# CLIP-BERT Hybrid Model Architecture Summary\n",
    "\n",
    "## Key Changes Made:\n",
    "\n",
    "### 1. **Dual Model Architecture**\n",
    "- **CLIP Base Model**: `openai/clip-vit-base-patch32` (smaller, faster than large)\n",
    "- **BERT Model**: `bert-base-uncased` for enhanced text understanding\n",
    "- Both models are trained end-to-end\n",
    "\n",
    "### 2. **Cross-Attention Mechanism**\n",
    "- **Multi-Head Cross-Attention**: 8 attention heads by default\n",
    "- **Bidirectional Attention**: \n",
    "  - BERT features attend to CLIP text features\n",
    "  - CLIP text features attend to BERT features\n",
    "- **Feature Fusion**: Concatenates CLIP image + attended CLIP text + attended BERT features\n",
    "\n",
    "### 3. **Enhanced Dataset & Collation**\n",
    "- **Dual Tokenization**: Both CLIP and BERT tokenizers process the same text\n",
    "- **Separate Input Streams**: `clip_input_ids`, `bert_input_ids`, etc.\n",
    "- **Unified Collation**: Handles both tokenization schemes in batch processing\n",
    "\n",
    "### 4. **SMAPE Tracking**\n",
    "- **Per-Epoch Evaluation**: SMAPE calculated on training subset every epoch\n",
    "- **Real-time Monitoring**: Track model performance during training\n",
    "- **Early Stopping Potential**: Can be used to prevent overfitting\n",
    "\n",
    "### 5. **Model Architecture Flow**\n",
    "```\n",
    "Text Input ‚Üí [CLIP Tokenizer + BERT Tokenizer]\n",
    "Image Input ‚Üí [CLIP Vision Encoder]\n",
    "                    ‚Üì\n",
    "            [CLIP Text Features] ‚Üê‚Üí [BERT Features] (Cross-Attention)\n",
    "                    ‚Üì\n",
    "            [CLIP Image Features] + [Attended Features] ‚Üí [Regression Head] ‚Üí Price Prediction\n",
    "```\n",
    "\n",
    "### 6. **Training Improvements**\n",
    "- **Contrastive Loss**: Between CLIP image and text features\n",
    "- **Huber Loss**: Robust regression loss for price prediction\n",
    "- **Gradient Clipping**: Prevents exploding gradients\n",
    "- **Mixed Precision**: FP16 training for efficiency\n",
    "\n",
    "### 7. **Configuration Options**\n",
    "- `NUM_ATTENTION_HEADS`: Number of cross-attention heads (default: 8)\n",
    "- `ATTENTION_DROPOUT`: Dropout rate for attention layers (default: 0.1)\n",
    "- `ALPHA_CONTRAST`: Weight for contrastive loss (default: 0.25)\n",
    "- `TAU`: Temperature for InfoNCE loss (default: 0.07)\n",
    "\n",
    "This hybrid approach leverages the strengths of both CLIP (vision-language alignment) and BERT (deep text understanding) while using cross-attention to create rich, contextually-aware feature representations for price prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a918d4-4cc2-4e9e-a081-0c36c55da2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
