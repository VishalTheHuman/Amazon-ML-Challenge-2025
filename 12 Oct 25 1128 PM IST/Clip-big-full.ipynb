{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f82cc5a-72e0-415b-89e7-57e483af25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "\n",
    "df = pd.read_csv(\"jl_fs/train.csv\")\n",
    "df[\"image_path\"] = df[\"sample_id\"].apply(lambda x : f\"jl_fs/images/train/{x}.jpg\")\n",
    "df.to_csv(\"train_updated.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6537ead-bd3b-48cf-994b-511e24fd87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    CLIPModel,\n",
    "    AutoProcessor,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98cf885c-9990-4c1d-bd76-7c121631c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "CSV_PATH        = os.environ.get(\"TRAIN_CSV\", \"train_updated.csv\")     # must contain text + price + image path\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "PRICE_COL       = os.environ.get(\"PRICE_COL\", \"price\")\n",
    "IMG_COL         = os.environ.get(\"IMG_COL\",  \"image_path\")             # local jpg path column\n",
    "ID_COL          = os.environ.get(\"ID_COL\",  \"sample_id\")\n",
    "\n",
    "MODEL_ID        = os.environ.get(\"MODEL_ID\", \"openai/clip-vit-large-patch14\")\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_clip_fulltrain\")\n",
    "\n",
    "SEED            = int(os.environ.get(\"SEED\", \"42\"))\n",
    "MAX_LEN         = int(os.environ.get(\"MAX_LEN\", \"64\"))                 # CLIP text context is shorter\n",
    "BATCH_SIZE      = int(os.environ.get(\"BATCH_SIZE\", \"32\"))\n",
    "LR              = float(os.environ.get(\"LR\", \"2e-5\"))\n",
    "WEIGHT_DECAY    = float(os.environ.get(\"WEIGHT_DECAY\", \"0.01\"))\n",
    "EPOCHS          = int(os.environ.get(\"EPOCHS\", \"5\"))\n",
    "WARMUP_RATIO    = float(os.environ.get(\"WARMUP_RATIO\", \"0.06\"))\n",
    "GRAD_ACCUM      = int(os.environ.get(\"GRAD_ACCUM\", \"1\"))\n",
    "MAX_GRAD_NORM   = float(os.environ.get(\"MAX_GRAD_NORM\", \"1.0\"))\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "# Loss & regularization\n",
    "ALPHA_CONTRAST  = float(os.environ.get(\"ALPHA_CONTRAST\", \"0.25\"))      # weight for contrastive loss (0..1)\n",
    "TAU             = float(os.environ.get(\"TAU\", \"0.07\"))                 # InfoNCE temperature\n",
    "HUBER_DELTA     = float(os.environ.get(\"HUBER_DELTA\", \"1.0\"))\n",
    "\n",
    "# Price/log transform\n",
    "MIN_PRICE       = float(os.environ.get(\"MIN_PRICE\", \"1e-6\"))\n",
    "\n",
    "# Missing image policy for TRAIN: zero | text_only | drop\n",
    "IMG_MISSING_POLICY = os.environ.get(\"IMG_MISSING_POLICY\", \"zero\").lower()\n",
    "assert IMG_MISSING_POLICY in {\"zero\", \"text_only\", \"drop\"}\n",
    "\n",
    "# Inference/Test config (optional)\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"jl_fs/test.csv\").strip()               # if \"\", inference is skipped\n",
    "TEST_IMG_DIR    = os.environ.get(\"TEST_IMG_DIR\", \"jl_fs/images/test\")  # used if test CSV lacks image_path\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc64e3e-cb0d-43ea-b394-13a89a64981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- Utils ---------------------------\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def log2_price(p: np.ndarray) -> np.ndarray:\n",
    "    return np.log2(np.clip(p, MIN_PRICE, None))\n",
    "\n",
    "def delog2(x: np.ndarray) -> np.ndarray:\n",
    "    return np.power(2.0, x)\n",
    "\n",
    "# --------------------------- Dataset & Collate ---------------------------\n",
    "class ClipPriceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TRAIN:\n",
    "      policy 'zero':      returns dummy pixel for missing images; later masked to zeros.\n",
    "      policy 'text_only': returns no pixel_values for missing images; vision forward skipped.\n",
    "      policy 'drop':      drops rows with missing images at dataset build time.\n",
    "\n",
    "    TEST:\n",
    "      We will ALWAYS behave like 'zero' (never drop predictions).\n",
    "\n",
    "    Items may contain: input_ids, attention_mask, (pixel_values), img_missing, (target)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, text_col: str, img_col: str,\n",
    "                 prices_log2: Optional[np.ndarray],\n",
    "                 processor: AutoProcessor, max_len: int, policy: str, is_test: bool = False):\n",
    "        self.processor = processor\n",
    "        self.max_len = max_len\n",
    "        self.policy = policy\n",
    "        self.is_test = is_test\n",
    "\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "        if (policy == \"drop\") and (not is_test):\n",
    "            before = len(df)\n",
    "            df = df[df[img_col].apply(lambda p: isinstance(p, str) and len(p) > 0 and os.path.exists(p))]\n",
    "            self.dropped_missing = before - len(df)\n",
    "        else:\n",
    "            self.dropped_missing = 0\n",
    "\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.img_paths = df[img_col].fillna(\"\").astype(str).tolist()\n",
    "        self.prices_log2 = prices_log2\n",
    "        self.missing_img_count = 0\n",
    "\n",
    "        # Dummy pixel to get correct shape\n",
    "        dummy = self.processor(images=Image.new(\"RGB\", (224, 224)), return_tensors=\"pt\")\n",
    "        self._dummy_pixel = dummy[\"pixel_values\"].squeeze(0)  # (C,H,W)\n",
    "\n",
    "        # For exporting IDs in test predictions\n",
    "        self.ids = df[ID_COL].tolist() if (ID_COL in df.columns) else list(range(len(df)))\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def _load_image(self, path: str):\n",
    "        if isinstance(path, str) and path and os.path.exists(path):\n",
    "            try:\n",
    "                return Image.open(path).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.missing_img_count += 1\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        img  = self._load_image(self.img_paths[idx])\n",
    "\n",
    "        enc_text = self.processor(text=[text], padding=False, truncation=True,\n",
    "                                  max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        img_missing = 0\n",
    "        pixel_values = None\n",
    "\n",
    "        if img is None:\n",
    "            img_missing = 1\n",
    "            if self.is_test:\n",
    "                # For test we NEVER drop; force zero-like behavior\n",
    "                pixel_values = self._dummy_pixel.clone()\n",
    "            else:\n",
    "                if self.policy == \"zero\":\n",
    "                    pixel_values = self._dummy_pixel.clone()\n",
    "                elif self.policy == \"text_only\":\n",
    "                    pixel_values = None\n",
    "                elif self.policy == \"drop\":\n",
    "                    # should not occur because drop was handled in __init__\n",
    "                    pixel_values = self._dummy_pixel.clone()\n",
    "        else:\n",
    "            enc_img = self.processor(images=img, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": enc_text[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc_text[\"attention_mask\"].squeeze(0),\n",
    "            \"img_missing\": torch.tensor(img_missing, dtype=torch.uint8),\n",
    "            \"row_id\": torch.tensor(self.ids[idx], dtype=torch.long)\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            item[\"pixel_values\"] = pixel_values\n",
    "        if self.prices_log2 is not None:\n",
    "            item[\"target\"] = torch.tensor(self.prices_log2[idx], dtype=torch.float32)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9393c3-6394-48d6-bbd2-aa7b457974b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class CollateClip:\n",
    "    processor: AutoProcessor\n",
    "    def __call__(self, batch):\n",
    "        # pad text\n",
    "        input_ids = [b[\"input_ids\"] for b in batch]\n",
    "        attention = [b[\"attention_mask\"] for b in batch]\n",
    "        text_padded = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention},\n",
    "            padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        # images: some may be absent (text_only policy)\n",
    "        has_pix = [(\"pixel_values\" in b) for b in batch]\n",
    "        pixel_values = None\n",
    "        if any(has_pix):\n",
    "            shapes = [b[\"pixel_values\"].shape for b in batch if \"pixel_values\" in b]\n",
    "            C,H,W = shapes[0]\n",
    "            stacked = []\n",
    "            for b in batch:\n",
    "                if \"pixel_values\" in b:\n",
    "                    stacked.append(b[\"pixel_values\"])\n",
    "                else:\n",
    "                    stacked.append(torch.zeros((C,H,W), dtype=torch.float32))\n",
    "            pixel_values = torch.stack(stacked, dim=0)\n",
    "\n",
    "        res = {\n",
    "            \"input_ids\": text_padded[\"input_ids\"],\n",
    "            \"attention_mask\": text_padded[\"attention_mask\"],\n",
    "            \"img_missing\": torch.stack([b[\"img_missing\"] for b in batch], dim=0),\n",
    "            \"row_id\": torch.stack([b[\"row_id\"] for b in batch], dim=0),\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            res[\"pixel_values\"] = pixel_values\n",
    "        if \"target\" in batch[0]:\n",
    "            res[\"target\"] = torch.stack([b[\"target\"] for b in batch], dim=0)\n",
    "        return res\n",
    "\n",
    "# --------------------------- Model & Loss ---------------------------\n",
    "class ClipRegressionHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        in_dim = 2 * embed_dim  # concat image+text\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_dim, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "def info_nce(z_img: torch.Tensor, z_txt: torch.Tensor, tau: float = 0.07) -> torch.Tensor:\n",
    "    z_img = F.normalize(z_img, dim=-1)\n",
    "    z_txt = F.normalize(z_txt, dim=-1)\n",
    "    logits = torch.matmul(z_img, z_txt.t()) / tau  # (B,B)\n",
    "    labels = torch.arange(z_img.size(0), device=z_img.device)\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.t(), labels)\n",
    "    return 0.5 * (loss_i + loss_t)\n",
    "\n",
    "def huber_loss(pred, target, delta=1.0):\n",
    "    return F.huber_loss(pred, target, delta=delta)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72670a55-1fc0-4b25-94a0-10a3ee80a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading TRAIN CSV: train_updated.csv\n",
      "📦 Train rows: 75000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011cc8bf87e446429e3881ccc249d59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0890e7e4e4f7471282e4718c0857e001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b235ee9d42f4e38b091d925d2604441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ddd61896cc4f20881039262031185a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b83ed85f68f4bbba4dec91f2ac4bd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaafe9a7fed408380f86a004eb844c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ecff5ddc9c48338d63ba31c60dbb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3f2d880d6f4811a4b0084da9d454d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️ Device: cuda\n",
      "🧮 Trainable params CLIP=427,616,513 | head=2,362,369\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Load TRAIN data ---------------------------\n",
    "print(f\"🔧 Loading TRAIN CSV: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# checks\n",
    "for col, name in [(TEXT_COL, \"TEXT_COL\"), (PRICE_COL, \"PRICE_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"{name} '{col}' not in CSV columns={df.columns.tolist()}\")\n",
    "\n",
    "# clean\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "df = df.loc[pd.to_numeric(df[PRICE_COL], errors=\"coerce\").notnull()].copy()\n",
    "df[PRICE_COL] = df[PRICE_COL].astype(float)\n",
    "df = df.loc[df[PRICE_COL] >= 0.0].reset_index(drop=True)\n",
    "\n",
    "print(f\"📦 Train rows: {len(df)}\")\n",
    "\n",
    "# targets (log2)\n",
    "y_log = log2_price(df[PRICE_COL].values)\n",
    "\n",
    "# --------------------------- CLIP backbone ---------------------------\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device)\n",
    "price_head = ClipRegressionHead(embed_dim=clip_model.config.projection_dim, dropout=0.1).to(device)\n",
    "\n",
    "print(f\"🖥️ Device: {device}\")\n",
    "# Train end-to-end (set to False to freeze CLIP)\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"🧮 Trainable params CLIP={count_parameters(clip_model):,} | head={count_parameters(price_head):,}\")\n",
    "\n",
    "train_ds = ClipPriceDataset(df, TEXT_COL, IMG_COL, y_log, processor, MAX_LEN, IMG_MISSING_POLICY, is_test=False)\n",
    "collate  = CollateClip(processor)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40fc094-607f-4d5f-a195-5d8cd8222840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Warmup batch to tally missing images…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97/2161992421.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ TRAIN missing images: 0\n",
      "🗑️ TRAIN dropped (policy=zero): 0\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "params = list(clip_model.named_parameters()) + [(f\"head.{n}\", p) for n, p in price_head.named_parameters()]\n",
    "grouped = [\n",
    "    {\"params\": [p for n, p in params if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": [p for n, p in params if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(grouped, lr=LR)\n",
    "num_training_steps = EPOCHS * max(1, math.ceil(len(train_loader) / max(1, GRAD_ACCUM)))\n",
    "num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "# Warmup batch to tally missing images\n",
    "print(\"🔎 Warmup batch to tally missing images…\")\n",
    "if len(train_loader) > 0:\n",
    "    _ = next(iter(train_loader))\n",
    "print(f\"⚠️ TRAIN missing images: {train_ds.missing_img_count}\")\n",
    "print(f\"🗑️ TRAIN dropped (policy={IMG_MISSING_POLICY}): {getattr(train_ds,'dropped_missing',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c3e7bb-d4c8-46ae-8b20-5205bd23a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_97/864095044.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 200/2344 loss=2.4677 reg=3.1764 con=0.3417\n",
      "epoch 1 step 400/2344 loss=2.2198 reg=2.8369 con=0.3687\n",
      "epoch 1 step 600/2344 loss=1.8211 reg=2.2815 con=0.4402\n",
      "epoch 1 step 800/2344 loss=1.5155 reg=1.8749 con=0.4372\n",
      "epoch 1 step 1000/2344 loss=1.3087 reg=1.6081 con=0.4105\n",
      "epoch 1 step 1200/2344 loss=1.1635 reg=1.4236 con=0.3833\n",
      "epoch 1 step 1400/2344 loss=1.0549 reg=1.2864 con=0.3604\n",
      "epoch 1 step 1600/2344 loss=0.9722 reg=1.1834 con=0.3387\n",
      "epoch 1 step 1800/2344 loss=0.9060 reg=1.1014 con=0.3198\n",
      "epoch 1 step 2000/2344 loss=0.8526 reg=1.0357 con=0.3034\n",
      "epoch 1 step 2200/2344 loss=0.8078 reg=0.9805 con=0.2898\n",
      "✅ Epoch 1 done. avg_loss=0.7801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 step 200/2344 loss=0.2855 reg=0.3482 con=0.0975\n",
      "epoch 2 step 400/2344 loss=0.2816 reg=0.3421 con=0.1000\n",
      "epoch 2 step 600/2344 loss=0.2830 reg=0.3441 con=0.0997\n",
      "epoch 2 step 800/2344 loss=0.2844 reg=0.3463 con=0.0988\n",
      "epoch 2 step 1000/2344 loss=0.2840 reg=0.3462 con=0.0972\n",
      "epoch 2 step 1200/2344 loss=0.2850 reg=0.3477 con=0.0968\n",
      "epoch 2 step 1400/2344 loss=0.2834 reg=0.3459 con=0.0958\n",
      "epoch 2 step 1600/2344 loss=0.2822 reg=0.3446 con=0.0950\n",
      "epoch 2 step 1800/2344 loss=0.2821 reg=0.3445 con=0.0950\n",
      "epoch 2 step 2000/2344 loss=0.2826 reg=0.3455 con=0.0939\n",
      "epoch 2 step 2200/2344 loss=0.2815 reg=0.3442 con=0.0934\n",
      "✅ Epoch 2 done. avg_loss=0.2803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 step 200/2344 loss=0.1727 reg=0.2113 con=0.0569\n",
      "epoch 3 step 400/2344 loss=0.1767 reg=0.2160 con=0.0590\n",
      "epoch 3 step 600/2344 loss=0.1774 reg=0.2167 con=0.0594\n",
      "epoch 3 step 800/2344 loss=0.1759 reg=0.2151 con=0.0585\n",
      "epoch 3 step 1000/2344 loss=0.1765 reg=0.2159 con=0.0580\n",
      "epoch 3 step 1200/2344 loss=0.1770 reg=0.2167 con=0.0578\n",
      "epoch 3 step 1400/2344 loss=0.1772 reg=0.2172 con=0.0574\n",
      "epoch 3 step 1600/2344 loss=0.1771 reg=0.2171 con=0.0570\n",
      "epoch 3 step 1800/2344 loss=0.1770 reg=0.2172 con=0.0565\n",
      "epoch 3 step 2000/2344 loss=0.1760 reg=0.2161 con=0.0559\n",
      "epoch 3 step 2200/2344 loss=0.1767 reg=0.2170 con=0.0556\n",
      "✅ Epoch 3 done. avg_loss=0.1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 step 200/2344 loss=0.0991 reg=0.1187 con=0.0404\n",
      "epoch 4 step 400/2344 loss=0.1007 reg=0.1209 con=0.0402\n",
      "epoch 4 step 600/2344 loss=0.0994 reg=0.1191 con=0.0402\n",
      "epoch 4 step 800/2344 loss=0.1001 reg=0.1201 con=0.0402\n",
      "epoch 4 step 1000/2344 loss=0.1000 reg=0.1200 con=0.0401\n",
      "epoch 4 step 1200/2344 loss=0.0999 reg=0.1200 con=0.0396\n",
      "epoch 4 step 1400/2344 loss=0.0996 reg=0.1197 con=0.0393\n",
      "epoch 4 step 1600/2344 loss=0.0991 reg=0.1191 con=0.0390\n",
      "epoch 4 step 1800/2344 loss=0.0988 reg=0.1188 con=0.0386\n",
      "epoch 4 step 2000/2344 loss=0.0983 reg=0.1183 con=0.0383\n",
      "epoch 4 step 2200/2344 loss=0.0981 reg=0.1182 con=0.0379\n",
      "✅ Epoch 4 done. avg_loss=0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 step 200/2344 loss=0.0553 reg=0.0639 con=0.0295\n",
      "epoch 5 step 400/2344 loss=0.0563 reg=0.0653 con=0.0294\n",
      "epoch 5 step 600/2344 loss=0.0562 reg=0.0652 con=0.0289\n",
      "epoch 5 step 800/2344 loss=0.0549 reg=0.0635 con=0.0289\n",
      "epoch 5 step 1000/2344 loss=0.0544 reg=0.0631 con=0.0284\n",
      "epoch 5 step 1200/2344 loss=0.0541 reg=0.0627 con=0.0281\n",
      "epoch 5 step 1400/2344 loss=0.0532 reg=0.0616 con=0.0278\n",
      "epoch 5 step 1600/2344 loss=0.0529 reg=0.0613 con=0.0277\n",
      "epoch 5 step 1800/2344 loss=0.0525 reg=0.0609 con=0.0274\n",
      "epoch 5 step 2000/2344 loss=0.0523 reg=0.0607 con=0.0274\n",
      "epoch 5 step 2200/2344 loss=0.0521 reg=0.0603 con=0.0272\n",
      "✅ Epoch 5 done. avg_loss=0.0516\n",
      "💾 Saved full-data checkpoint to: price_clip_fulltrain/full_clip.pt\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- TRAIN (Full data) ---------------------------\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    clip_model.train(); price_head.train()\n",
    "    loss_run = reg_run = con_run = 0.0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        targets        = batch[\"target\"].to(device, non_blocking=True).float()\n",
    "        img_missing    = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            # Text features\n",
    "            txt_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Image features\n",
    "            do_vision = (\"pixel_values\" in batch) and (IMG_MISSING_POLICY != \"text_only\")\n",
    "            if do_vision:\n",
    "                pixel_values = batch[\"pixel_values\"].to(\n",
    "                    device,\n",
    "                    dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "                    non_blocking=True\n",
    "                )\n",
    "                img_feat = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "                if img_missing.any():\n",
    "                    img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "            else:\n",
    "                img_feat = torch.zeros_like(txt_feat)\n",
    "\n",
    "            # Normalize + fuse\n",
    "            txt_n = F.normalize(txt_feat, dim=-1)\n",
    "            img_n = F.normalize(img_feat, dim=-1)\n",
    "            fused = torch.cat([img_n, txt_n], dim=-1)\n",
    "\n",
    "            # Losses\n",
    "            pred_log = price_head(fused)\n",
    "            reg_loss = huber_loss(pred_log, targets, delta=HUBER_DELTA)\n",
    "\n",
    "            con_loss = torch.tensor(0.0, device=device, dtype=txt_n.dtype)\n",
    "            valid_idx = (img_missing == 0).nonzero(as_tuple=False).squeeze(-1)\n",
    "            if do_vision and valid_idx.numel() > 1 and ALPHA_CONTRAST > 0:\n",
    "                con_loss = info_nce(img_n[valid_idx], txt_n[valid_idx], tau=TAU)\n",
    "\n",
    "            loss = (1.0 - ALPHA_CONTRAST) * reg_loss + ALPHA_CONTRAST * con_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % GRAD_ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(list(clip_model.parameters()) + list(price_head.parameters()), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_run += float(loss.item())\n",
    "        reg_run  += float(reg_loss.item())\n",
    "        con_run  += float(con_loss.item()) if isinstance(con_loss, torch.Tensor) else float(con_loss)\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"epoch {epoch} step {step}/{len(train_loader)} \"\n",
    "                  f\"loss={loss_run/step:.4f} reg={reg_run/step:.4f} con={con_run/step:.4f}\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch} done. avg_loss={loss_run/max(1,len(train_loader)):.4f}\")\n",
    "\n",
    "# Save final full-data checkpoint\n",
    "full_ckpt = os.path.join(OUTPUT_DIR, \"full_clip.pt\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"clip_state\": clip_model.state_dict(),\n",
    "        \"head_state\": price_head.state_dict(),\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"config\": {\n",
    "            \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "            \"TAU\": TAU,\n",
    "            \"MAX_LEN\": MAX_LEN,\n",
    "            \"projection_dim\": clip_model.config.projection_dim,\n",
    "            \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "            \"HUBER_DELTA\": HUBER_DELTA\n",
    "        },\n",
    "        \"columns\": {\"id\": ID_COL, \"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "    },\n",
    "    full_ckpt\n",
    ")\n",
    "print(f\"💾 Saved full-data checkpoint to: {full_ckpt}\")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_fulltrain.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_rows\": int(len(df)),\n",
    "        \"train_missing_images\": int(train_ds.missing_img_count),\n",
    "        \"dropped_train\": int(getattr(train_ds, \"dropped_missing\", 0)),\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"grad_accum\": GRAD_ACCUM,\n",
    "        \"fp16\": FP16,\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82be1e22-e95f-4cb3-a69e-74f7493ccf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Loading TEST_CSV: jl_fs/test.csv\n",
      "📝 Test rows: 75000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m dft \u001b[38;5;241m=\u001b[39m build_test_df(TEST_CSV, ID_COL, TEXT_COL, IMG_COL, TEST_IMG_DIR)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📝 Test rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dft)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m preds_log, test_ids, miss_count \u001b[38;5;241m=\u001b[39m \u001b[43minfer_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprice_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEXT_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds_log\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ No predictions generated for test.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 46\u001b[0m, in \u001b[0;36minfer_predictions\u001b[0;34m(clip_model, price_head, processor, df_test, text_col, img_col, batch_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     41\u001b[0m     device,\n\u001b[1;32m     42\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(clip_model\u001b[38;5;241m.\u001b[39mvision_model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     43\u001b[0m     non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m img_feat \u001b[38;5;241m=\u001b[39m clip_model\u001b[38;5;241m.\u001b[39mget_image_features(pixel_values\u001b[38;5;241m=\u001b[39mpixel_values)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_missing\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     47\u001b[0m     img_feat \u001b[38;5;241m=\u001b[39m img_feat \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m img_missing\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     49\u001b[0m txt_n \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(txt_feat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------- INFERENCE (TEST) ---------------------------\n",
    "def build_test_df(path: str, id_col: str, text_col: str, img_col: str, fallback_img_dir: str) -> pd.DataFrame:\n",
    "    dft = pd.read_csv(path)\n",
    "    # ensure id col exists\n",
    "    if id_col not in dft.columns:\n",
    "        raise ValueError(f\"ID_COL '{id_col}' not in TEST_CSV columns={dft.columns.tolist()}\")\n",
    "\n",
    "    # text column can be absent; if so, create empty\n",
    "    if text_col not in dft.columns:\n",
    "        dft[text_col] = \"\"\n",
    "\n",
    "    # image path column: create if absent\n",
    "    if img_col not in dft.columns:\n",
    "        dft[img_col] = dft[id_col].astype(str).apply(lambda x: os.path.join(fallback_img_dir, f\"{x}.jpg\"))\n",
    "\n",
    "    dft[text_col] = dft[text_col].fillna(\"\").astype(str).str.strip()\n",
    "    dft[img_col]  = dft[img_col].fillna(\"\").astype(str)\n",
    "    return dft\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_predictions(clip_model: CLIPModel, price_head: nn.Module,\n",
    "                      processor: AutoProcessor, df_test: pd.DataFrame,\n",
    "                      text_col: str, img_col: str, batch_size: int = 64) -> np.ndarray:\n",
    "    clip_model.eval(); price_head.eval()\n",
    "    ds = ClipPriceDataset(df_test, text_col, img_col, prices_log2=None,\n",
    "                          processor=processor, max_len=MAX_LEN, policy=\"zero\", is_test=True)\n",
    "    collate = CollateClip(processor)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "    preds_log = []\n",
    "    for batch in dl:\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        img_missing    = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        # Text\n",
    "        txt_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Image (always present as zero-fallback in test)\n",
    "        pixel_values = batch[\"pixel_values\"].to(\n",
    "            device,\n",
    "            dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "            non_blocking=True\n",
    "        )\n",
    "        img_feat = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "        if img_missing.any():\n",
    "            img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "\n",
    "        txt_n = F.normalize(txt_feat, dim=-1)\n",
    "        img_n = F.normalize(img_feat, dim=-1)\n",
    "        fused = torch.cat([img_n, txt_n], dim=-1)\n",
    "        pred  = price_head(fused)  # log2 price\n",
    "        preds_log.append(pred.detach().float().cpu().numpy())\n",
    "\n",
    "    preds_log = np.concatenate(preds_log, axis=0) if len(preds_log) else np.array([])\n",
    "    return preds_log, ds.ids, ds.missing_img_count\n",
    "\n",
    "if TEST_CSV:\n",
    "    print(f\"🔮 Loading TEST_CSV: {TEST_CSV}\")\n",
    "    dft = build_test_df(TEST_CSV, ID_COL, TEXT_COL, IMG_COL, TEST_IMG_DIR)\n",
    "    print(f\"📝 Test rows: {len(dft)}\")\n",
    "\n",
    "    preds_log, test_ids, miss_count = infer_predictions(clip_model, price_head, processor, dft, TEXT_COL, IMG_COL, batch_size=max(32, BATCH_SIZE))\n",
    "    if preds_log.size == 0:\n",
    "        print(\"⚠️ No predictions generated for test.\")\n",
    "    else:\n",
    "        pred_price = delog2(preds_log.reshape(-1))\n",
    "        out_df = pd.DataFrame({ID_COL: test_ids, \"price\": pred_price})\n",
    "        out_path = os.path.join(OUTPUT_DIR, \"test_predictions.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        print(f\"📤 Saved predictions to: {out_path}\")\n",
    "        print(f\"⚠️ Test missing images encountered: {miss_count}\")\n",
    "else:\n",
    "    print(\"ℹ️ TEST_CSV not set — skipping inference.\")\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8830a29b-6df5-44e6-af98-14a01812b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loaded checkpoint from: price_clip_fulltrain/full_clip.pt\n",
      "🔤 MODEL_ID=openai/clip-vit-large-patch14 | projection_dim=768 | IMG_MISSING_POLICY=zero | MAX_LEN=64\n",
      "🖥 Device: cuda\n",
      "🧪 Test rows: 75000 | Missing images encountered (during getitem): 0\n",
      "🗑 Dropped due to policy=drop: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9604df8776845c6b82b773b1e34db6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_97/3650335273.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# --- Inference: load best checkpoint and predict on TEST_CSV ---\n",
    "\n",
    "# %%\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "\n",
    "# ---- Config / paths ----\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"jl_fs/test.csv\")   # must contain ID + text + image path\n",
    "ID_COL          = os.environ.get(\"ID_COL\", \"sample_id\")\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "IMG_COL         = os.environ.get(\"IMG_COL\",  \"image_path\")\n",
    "\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_clip_fulltrain\")\n",
    "CKPT_PATH       = os.environ.get(\"CKPT_PATH\", os.path.join(OUTPUT_DIR, \"full_clip.pt\"))\n",
    "\n",
    "BATCH_SIZE      = int(os.environ.get(\"INF_BATCH_SIZE\", \"64\"))\n",
    "MAX_LEN_ENV     = os.environ.get(\"MAX_LEN\", None)  # if you want to override tokenizer max len\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "assert os.path.exists(CKPT_PATH), f\"Checkpoint not found at {CKPT_PATH}\"\n",
    "assert os.path.exists(TEST_CSV),  f\"Test CSV not found at {TEST_CSV}\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- Load checkpoint ----\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "model_id = ckpt.get(\"model_id\", \"openai/clip-vit-large-patch14\")\n",
    "cfg = ckpt.get(\"config\", {})\n",
    "projection_dim = cfg.get(\"projection_dim\")\n",
    "img_missing_policy = cfg.get(\"IMG_MISSING_POLICY\", \"zero\")\n",
    "max_len = int(cfg.get(\"MAX_LEN\", 64)) if MAX_LEN_ENV is None else int(MAX_LEN_ENV)\n",
    "\n",
    "print(f\"📦 Loaded checkpoint from: {CKPT_PATH}\")\n",
    "print(f\"🔤 MODEL_ID={model_id} | projection_dim={projection_dim} | IMG_MISSING_POLICY={img_missing_policy} | MAX_LEN={max_len}\")\n",
    "\n",
    "# ---- Recreate processor & models ----\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "clip_model = CLIPModel.from_pretrained(model_id)\n",
    "clip_model.load_state_dict(ckpt[\"clip_state\"], strict=True)\n",
    "clip_model.to(device).eval()\n",
    "\n",
    "# Recreate and load regression head (same class as training cell)\n",
    "price_head = ClipRegressionHead(embed_dim=projection_dim, dropout=0.0)\n",
    "price_head.load_state_dict(ckpt[\"head_state\"], strict=True)\n",
    "price_head.to(device).eval()\n",
    "\n",
    "# ---- Load test data ----\n",
    "dft = pd.read_csv(TEST_CSV)\n",
    "dft[\"image_path\"] = dft[\"sample_id\"].apply(lambda x : f\"jl_fs/images/test/{x}.jpg\")\n",
    "for col, name in [(ID_COL, \"ID_COL\"), (TEXT_COL, \"TEXT_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in dft.columns:\n",
    "        raise ValueError(f\"{name} '{col}' missing from test CSV. Columns={dft.columns.tolist()}\")\n",
    "\n",
    "# Basic clean\n",
    "dft[TEXT_COL] = dft[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "dft[IMG_COL]  = dft[IMG_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# Build dataset/dataloader with no targets\n",
    "test_ds = ClipPriceDataset(\n",
    "    df=dft[[ID_COL, TEXT_COL, IMG_COL]].copy(),\n",
    "    text_col=TEXT_COL,\n",
    "    img_col=IMG_COL,\n",
    "    prices_log2=None,\n",
    "    processor=processor,\n",
    "    max_len=max_len,\n",
    "    policy=img_missing_policy\n",
    ")\n",
    "collate = CollateClip(processor)\n",
    "\n",
    "dl_te = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=2, pin_memory=True, collate_fn=collate\n",
    ")\n",
    "\n",
    "print(f\"🖥 Device: {device}\")\n",
    "print(f\"🧪 Test rows: {len(test_ds)} | Missing images encountered (during getitem): {test_ds.missing_img_count}\")\n",
    "print(f\"🗑 Dropped due to policy=drop: {getattr(test_ds, 'dropped_missing', 0)}\")\n",
    "\n",
    "# ---- Inference loop ----\n",
    "clip_model_dtype = next(clip_model.vision_model.parameters()).dtype\n",
    "preds_log2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl_te, total = len(dl_te)):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        img_missing    = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        # Text features\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            txt_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Image features depending on policy\n",
    "            do_vision = (\"pixel_values\" in batch) and (img_missing_policy != \"text_only\")\n",
    "            if do_vision:\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=clip_model_dtype, non_blocking=True)\n",
    "                img_feat = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "                if img_missing.any():\n",
    "                    img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "            else:\n",
    "                img_feat = torch.zeros_like(txt_feat)\n",
    "\n",
    "            # Normalize + fuse\n",
    "            txt_n = F.normalize(txt_feat, dim=-1)\n",
    "            img_n = F.normalize(img_feat, dim=-1)\n",
    "            fused = torch.cat([img_n, txt_n], dim=-1)\n",
    "\n",
    "            # Predict log2(price)\n",
    "            pred_log = price_head(fused)\n",
    "            preds_log2.append(pred_log.detach().float().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "343e1695-40cb-4ba2-bfed-d1b32b3127f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Wrote 75000 predictions to: price_clip_fulltrain/test_predictions_clip_big_full_v2.csv\n",
      "   Missing images counted during dataset load: 0\n",
      "   Dropped rows (policy=drop): 0\n"
     ]
    }
   ],
   "source": [
    "# ---- Convert back to price (delog2) and save ----\n",
    "if len(preds_log2):\n",
    "    preds_log2 = np.concatenate(preds_log2, axis=0)\n",
    "    preds_log2_delog = np.pow(2,preds_log2)\n",
    "    preds_price = np.clip(preds_log2_delog, 0,10000)  # safe de-log clamp\n",
    "else:\n",
    "    preds_price = np.array([])\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    ID_COL: dft[ID_COL].values[: len(preds_price)],\n",
    "    \"price\": preds_price\n",
    "})\n",
    "pred_path = os.path.join(OUTPUT_DIR, \"test_predictions_clip_big_full_v2.csv\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "out.to_csv(pred_path, index=False)\n",
    "\n",
    "print(f\"✅ Done. Wrote {len(out)} predictions to: {pred_path}\")\n",
    "print(f\"   Missing images counted during dataset load: {test_ds.missing_img_count}\")\n",
    "print(f\"   Dropped rows (policy=drop): {getattr(test_ds,'dropped_missing',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a918d4-4cc2-4e9e-a081-0c36c55da2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
