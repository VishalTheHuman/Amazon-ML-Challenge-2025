{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa8ac66-e73f-439c-9d04-2f6e9b7fbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c524d04-698a-4049-8b57-e0d5ad664ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Config ---------------------------\n",
    "CSV_PATH        = os.environ.get(\"TRAIN_CSV\", \"jl_fs/train.csv\")   # must contain text + price\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")         # e.g., \"text\" or \"catalog_content\"\n",
    "PRICE_COL       = os.environ.get(\"PRICE_COL\", \"price\")\n",
    "\n",
    "MODEL_ID        = os.environ.get(\"MODEL_ID\", \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_distilbert_contrastive\")\n",
    "\n",
    "SEED            = 42\n",
    "VAL_FRAC        = 0.1                   # 50/50 split as requested\n",
    "MAX_LEN         = 192                   # adjust if texts are long\n",
    "BATCH_SIZE      = 16\n",
    "LR              = 3e-5\n",
    "WEIGHT_DECAY    = 0.01\n",
    "EPOCHS          = 5\n",
    "WARMUP_RATIO    = 0.06\n",
    "GRAD_ACCUM      = 1\n",
    "MAX_GRAD_NORM   = 1.0\n",
    "FP16            = True                  # use AMP for speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3056c82e-aa4c-4d02-9dcb-f65c04930469",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_CONTRAST  = 0.25                  # weight for contrastive loss (0..1). 0.25 is a good start.\n",
    "TAU             = 0.05                  # contrastive temperature\n",
    "\n",
    "# Augmentations\n",
    "WORD_MASK_P     = 0.08                  # probability to randomly mask an input token (2nd view only)\n",
    "DROPOUT_PROB    = 0.1                   # Dropout already inside the transformer; can adjust here in heads.\n",
    "\n",
    "EARLY_STOP_ROUNDS = 3                   # stop if val SMAPE hasn't improved for these epochs\n",
    "\n",
    "# Price clipping for log transform\n",
    "MIN_PRICE       = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d52918e-b72d-46e9-beb4-6f4420a2c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def log2_price(p: np.ndarray) -> np.ndarray:\n",
    "    return np.log2(np.clip(p, MIN_PRICE, None))\n",
    "\n",
    "def delog2(x: np.ndarray) -> np.ndarray:\n",
    "    return np.power(2.0, x)\n",
    "\n",
    "def split_train_val(df: pd.DataFrame, frac_val: float = VAL_FRAC, seed: int = SEED):\n",
    "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    n_val = int(len(df) * frac_val)\n",
    "    df_val = df.iloc[:n_val].reset_index(drop=True)\n",
    "    df_tr  = df.iloc[n_val:].reset_index(drop=True)\n",
    "    return df_tr, df_val\n",
    "\n",
    "class PriceTextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], prices_log2: Optional[np.ndarray], tokenizer, max_len: int, training: bool):\n",
    "        self.texts = texts\n",
    "        self.prices_log2 = prices_log2  # None in pure inference; here we train/val so not None\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.training = training\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "        return self.tok(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def _random_word_mask(self, input_ids: torch.Tensor, mask_token_id: int, prob: float) -> torch.Tensor:\n",
    "        if prob <= 0.0:\n",
    "            return input_ids\n",
    "        ids = input_ids.clone()\n",
    "        # don't mask special tokens\n",
    "        special_tokens = set(self.tok.all_special_ids)\n",
    "        for i in range(ids.size(0)):\n",
    "            for j in range(ids.size(1)):\n",
    "                if ids[i, j].item() in special_tokens:\n",
    "                    continue\n",
    "                if random.random() < prob:\n",
    "                    ids[i, j] = mask_token_id\n",
    "        return ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.texts[idx] if isinstance(self.texts[idx], str) else \"\"\n",
    "        enc1 = self._tokenize(t)\n",
    "\n",
    "        # second \"view\": token masking as augmentation (SimCSE-style uses dropout only; we add light word masking)\n",
    "        enc2 = {k: v.clone() for k, v in enc1.items()}\n",
    "        enc2[\"input_ids\"] = self._random_word_mask(\n",
    "            enc2[\"input_ids\"], mask_token_id=self.tok.mask_token_id, prob=WORD_MASK_P\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids_1\": enc1[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_1\": enc1[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_2\": enc2[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_2\": enc2[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        if self.prices_log2 is not None:\n",
    "            item[\"target\"] = torch.tensor(self.prices_log2[idx], dtype=torch.float32)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44720d9-686d-4df9-aaa8-3c345652fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Collate:\n",
    "    pad_id: int\n",
    "    def __call__(self, batch):\n",
    "        # pad each view independently\n",
    "        keys1 = [\"input_ids_1\", \"attention_mask_1\"]\n",
    "        keys2 = [\"input_ids_2\", \"attention_mask_2\"]\n",
    "\n",
    "        def pad_stack(keylist):\n",
    "            maxlen = max(x[keylist[0]].size(0) for x in batch)\n",
    "            out = {}\n",
    "            for k in keylist:\n",
    "                if \"input_ids\" in k:\n",
    "                    pad_val = self.pad_id\n",
    "                else:\n",
    "                    pad_val = 0\n",
    "                tensors = []\n",
    "                for x in batch:\n",
    "                    v = x[k]\n",
    "                    if v.size(0) < maxlen:\n",
    "                        pad = torch.full((maxlen - v.size(0),), pad_val, dtype=v.dtype)\n",
    "                        v = torch.cat([v, pad], dim=0)\n",
    "                    tensors.append(v.unsqueeze(0))\n",
    "                out[k] = torch.cat(tensors, dim=0)\n",
    "            return out\n",
    "\n",
    "        out1 = pad_stack(keys1)\n",
    "        out2 = pad_stack(keys2)\n",
    "\n",
    "        res = {**out1, **out2}\n",
    "        if \"target\" in batch[0]:\n",
    "            res[\"target\"] = torch.stack([x[\"target\"] for x in batch], dim=0)\n",
    "        return res\n",
    "\n",
    "class DistilBertPriceModel(nn.Module):\n",
    "    def __init__(self, model_id: str, proj_dim: int = 256, dropout: float = DROPOUT_PROB):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_id)  # DistilBERT\n",
    "        hidden = self.backbone.config.dim\n",
    "\n",
    "        # Regression head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "        # Projection head for contrastive learning\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, proj_dim)\n",
    "        )\n",
    "\n",
    "        # Optional: layerwise LR decay could be added by parameter groups\n",
    "\n",
    "    def forward_once(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # DistilBERT: last_hidden_state; use first token as pooled (CLS analog at position 0)\n",
    "        cls = out.last_hidden_state[:, 0, :]  # (B, hidden)\n",
    "        yhat = self.regressor(cls).squeeze(-1)  # (B,)\n",
    "        z = self.proj(cls)  # (B, proj_dim)\n",
    "        return yhat, z\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        y1, z1 = self.forward_once(input_ids_1, attention_mask_1)\n",
    "        y2, z2 = self.forward_once(input_ids_2, attention_mask_2)\n",
    "        return (y1 + y2) / 2.0, z1, z2  # average predictions from two views for stability\n",
    "\n",
    "def info_nce(z1: torch.Tensor, z2: torch.Tensor, tau: float = TAU) -> torch.Tensor:\n",
    "    # Normalize\n",
    "    z1 = F.normalize(z1, dim=-1)\n",
    "    z2 = F.normalize(z2, dim=-1)\n",
    "\n",
    "    logits = torch.matmul(z1, z2.t()) / tau  # (B, B)\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    loss1 = F.cross_entropy(logits, labels)\n",
    "    loss2 = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss1 + loss2) * 0.5\n",
    "\n",
    "def huber_loss(pred, target, delta=1.0):\n",
    "    return F.huber_loss(pred, target, delta=delta)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20db4377-4de6-4f75-adf4-2a86145d2467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading CSV: jl_fs/train.csv\n",
      "📊 Split: train=67500 | valid=7500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc98efb459f44c28ec4a69584b586a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbca00b2e824472eab616eefee1c873f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b8ac465bae48788b15674f9f7a23a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️ Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b0097ff7784df7a690dd9800616271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Trainable params: 67,741,697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3999/1538774000.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "/tmp/ipykernel_3999/1538774000.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 300/4219 loss=1.5448 reg=1.9364 con=0.3700\n",
      "epoch 1 step 600/4219 loss=1.0299 reg=1.3091 con=0.1921\n",
      "epoch 1 step 900/4219 loss=0.8408 reg=1.0770 con=0.1324\n",
      "epoch 1 step 1200/4219 loss=0.7376 reg=0.9495 con=0.1018\n",
      "epoch 1 step 1500/4219 loss=0.6721 reg=0.8684 con=0.0832\n",
      "epoch 1 step 1800/4219 loss=0.6262 reg=0.8114 con=0.0708\n",
      "epoch 1 step 2100/4219 loss=0.5933 reg=0.7705 con=0.0617\n",
      "epoch 1 step 2400/4219 loss=0.5657 reg=0.7361 con=0.0547\n",
      "epoch 1 step 2700/4219 loss=0.5437 reg=0.7086 con=0.0490\n",
      "epoch 1 step 3000/4219 loss=0.5252 reg=0.6854 con=0.0446\n",
      "epoch 1 step 3300/4219 loss=0.5102 reg=0.6665 con=0.0412\n",
      "epoch 1 step 3600/4219 loss=0.4976 reg=0.6508 con=0.0381\n",
      "epoch 1 step 3900/4219 loss=0.4862 reg=0.6365 con=0.0355\n",
      "epoch 1 step 4200/4219 loss=0.4755 reg=0.6230 con=0.0332\n",
      "✅ Epoch 1: VAL SMAPE = 51.493%\n",
      "💾 Saved new best to price_distilbert_contrastive/best_90-10.pt\n",
      "epoch 2 step 300/4219 loss=0.2990 reg=0.3977 con=0.0031\n",
      "epoch 2 step 600/4219 loss=0.2992 reg=0.3976 con=0.0037\n",
      "epoch 2 step 900/4219 loss=0.2959 reg=0.3933 con=0.0035\n",
      "epoch 2 step 1200/4219 loss=0.2961 reg=0.3938 con=0.0031\n",
      "epoch 2 step 1500/4219 loss=0.2957 reg=0.3932 con=0.0031\n",
      "epoch 2 step 1800/4219 loss=0.2961 reg=0.3937 con=0.0033\n",
      "epoch 2 step 2100/4219 loss=0.2965 reg=0.3943 con=0.0033\n",
      "epoch 2 step 2400/4219 loss=0.2941 reg=0.3910 con=0.0032\n",
      "epoch 2 step 2700/4219 loss=0.2960 reg=0.3936 con=0.0032\n",
      "epoch 2 step 3000/4219 loss=0.2964 reg=0.3942 con=0.0032\n",
      "epoch 2 step 3300/4219 loss=0.2963 reg=0.3940 con=0.0032\n",
      "epoch 2 step 3600/4219 loss=0.2953 reg=0.3926 con=0.0031\n",
      "epoch 2 step 3900/4219 loss=0.2949 reg=0.3922 con=0.0031\n",
      "epoch 2 step 4200/4219 loss=0.2942 reg=0.3912 con=0.0030\n",
      "✅ Epoch 2: VAL SMAPE = 48.599%\n",
      "💾 Saved new best to price_distilbert_contrastive/best_90-10.pt\n",
      "epoch 3 step 300/4219 loss=0.2401 reg=0.3191 con=0.0032\n",
      "epoch 3 step 600/4219 loss=0.2360 reg=0.3136 con=0.0033\n",
      "epoch 3 step 900/4219 loss=0.2311 reg=0.3073 con=0.0027\n",
      "epoch 3 step 1200/4219 loss=0.2295 reg=0.3052 con=0.0027\n",
      "epoch 3 step 1500/4219 loss=0.2293 reg=0.3047 con=0.0030\n",
      "epoch 3 step 1800/4219 loss=0.2307 reg=0.3066 con=0.0031\n",
      "epoch 3 step 2100/4219 loss=0.2327 reg=0.3093 con=0.0029\n",
      "epoch 3 step 2400/4219 loss=0.2310 reg=0.3070 con=0.0029\n",
      "epoch 3 step 2700/4219 loss=0.2312 reg=0.3072 con=0.0029\n",
      "epoch 3 step 3000/4219 loss=0.2301 reg=0.3058 con=0.0028\n",
      "epoch 3 step 3300/4219 loss=0.2304 reg=0.3063 con=0.0028\n",
      "epoch 3 step 3600/4219 loss=0.2298 reg=0.3054 con=0.0028\n",
      "epoch 3 step 3900/4219 loss=0.2294 reg=0.3049 con=0.0027\n",
      "epoch 3 step 4200/4219 loss=0.2293 reg=0.3049 con=0.0026\n",
      "✅ Epoch 3: VAL SMAPE = 46.829%\n",
      "💾 Saved new best to price_distilbert_contrastive/best_90-10.pt\n",
      "epoch 4 step 300/4219 loss=0.1783 reg=0.2371 con=0.0019\n",
      "epoch 4 step 600/4219 loss=0.1738 reg=0.2310 con=0.0020\n",
      "epoch 4 step 900/4219 loss=0.1744 reg=0.2318 con=0.0022\n",
      "epoch 4 step 1200/4219 loss=0.1728 reg=0.2296 con=0.0022\n",
      "epoch 4 step 1500/4219 loss=0.1715 reg=0.2279 con=0.0021\n",
      "epoch 4 step 1800/4219 loss=0.1713 reg=0.2277 con=0.0022\n",
      "epoch 4 step 2100/4219 loss=0.1722 reg=0.2289 con=0.0022\n",
      "epoch 4 step 2400/4219 loss=0.1724 reg=0.2291 con=0.0022\n",
      "epoch 4 step 2700/4219 loss=0.1732 reg=0.2303 con=0.0021\n",
      "epoch 4 step 3000/4219 loss=0.1739 reg=0.2312 con=0.0021\n",
      "epoch 4 step 3300/4219 loss=0.1745 reg=0.2320 con=0.0020\n",
      "epoch 4 step 3600/4219 loss=0.1749 reg=0.2326 con=0.0020\n",
      "epoch 4 step 3900/4219 loss=0.1751 reg=0.2328 con=0.0021\n",
      "epoch 4 step 4200/4219 loss=0.1744 reg=0.2319 con=0.0021\n",
      "✅ Epoch 4: VAL SMAPE = 45.888%\n",
      "💾 Saved new best to price_distilbert_contrastive/best_90-10.pt\n",
      "epoch 5 step 300/4219 loss=0.1433 reg=0.1905 con=0.0016\n",
      "epoch 5 step 600/4219 loss=0.1431 reg=0.1901 con=0.0021\n",
      "epoch 5 step 900/4219 loss=0.1396 reg=0.1854 con=0.0022\n",
      "epoch 5 step 1200/4219 loss=0.1395 reg=0.1853 con=0.0020\n",
      "epoch 5 step 1500/4219 loss=0.1383 reg=0.1837 con=0.0019\n",
      "epoch 5 step 1800/4219 loss=0.1379 reg=0.1833 con=0.0017\n",
      "epoch 5 step 2100/4219 loss=0.1376 reg=0.1829 con=0.0017\n",
      "epoch 5 step 2400/4219 loss=0.1367 reg=0.1817 con=0.0017\n",
      "epoch 5 step 2700/4219 loss=0.1370 reg=0.1820 con=0.0018\n",
      "epoch 5 step 3000/4219 loss=0.1361 reg=0.1809 con=0.0018\n",
      "epoch 5 step 3300/4219 loss=0.1355 reg=0.1801 con=0.0018\n",
      "epoch 5 step 3600/4219 loss=0.1347 reg=0.1790 con=0.0019\n",
      "epoch 5 step 3900/4219 loss=0.1350 reg=0.1793 con=0.0019\n",
      "epoch 5 step 4200/4219 loss=0.1348 reg=0.1791 con=0.0018\n",
      "✅ Epoch 5: VAL SMAPE = 46.500%\n",
      "⏸️ No improvement. Patience 1/3\n",
      "🏁 Best VAL SMAPE: 45.888% | Checkpoint: price_distilbert_contrastive/best_90-10.pt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"🔧 Loading CSV: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise ValueError(f\"TEXT_COL '{TEXT_COL}' not in CSV columns={df.columns.tolist()}\")\n",
    "if PRICE_COL not in df.columns:\n",
    "    raise ValueError(f\"PRICE_COL '{PRICE_COL}' not in CSV columns={df.columns.tolist()}\")\n",
    "\n",
    "# Clean/guard text\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "# Filter rows with non-positive or missing price\n",
    "df = df.loc[pd.to_numeric(df[PRICE_COL], errors=\"coerce\").notnull()].copy()\n",
    "df[PRICE_COL] = df[PRICE_COL].astype(float)\n",
    "df = df.loc[df[PRICE_COL] >= 0.0].reset_index(drop=True)\n",
    "\n",
    "# 50/50 split\n",
    "df_tr, df_va = split_train_val(df, frac_val=VAL_FRAC, seed=SEED)\n",
    "print(f\"📊 Split: train={len(df_tr)} | valid={len(df_va)}\")\n",
    "\n",
    "y_tr_log = log2_price(df_tr[PRICE_COL].values)\n",
    "y_va_log = log2_price(df_va[PRICE_COL].values)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.mask_token is None:\n",
    "    # For uncased DistilBERT, mask token should exist; fallback guard\n",
    "    tokenizer.add_special_tokens({\"mask_token\": \"[MASK]\"})\n",
    "\n",
    "train_ds = PriceTextDataset(\n",
    "    texts=df_tr[TEXT_COL].tolist(),\n",
    "    prices_log2=y_tr_log,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN,\n",
    "    training=True\n",
    ")\n",
    "val_ds = PriceTextDataset(\n",
    "    texts=df_va[TEXT_COL].tolist(),\n",
    "    prices_log2=y_va_log,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN,\n",
    "    training=False\n",
    ")\n",
    "\n",
    "collate = Collate(pad_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"🖥️ Device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "model = DistilBertPriceModel(MODEL_ID).to(device)\n",
    "# If we added a new mask token or changed tokenizer vocab size, resize embeddings\n",
    "if tokenizer.vocab_size != model.backbone.get_input_embeddings().weight.size(0):\n",
    "    model.backbone.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"🧮 Trainable params: {count_parameters(model):,}\")\n",
    "\n",
    "# Optimizer & scheduler\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "grouped = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(grouped, lr=LR)\n",
    "\n",
    "num_training_steps = EPOCHS * math.ceil(len(train_loader) / GRAD_ACCUM)\n",
    "num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "best_smape = float(\"inf\")\n",
    "best_path = os.path.join(OUTPUT_DIR, \"best_90-10.pt\")\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss_running = 0.0\n",
    "    reg_loss_running = 0.0\n",
    "    con_loss_running = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            yhat, z1, z2 = model(\n",
    "                input_ids_1=batch[\"input_ids_1\"],\n",
    "                attention_mask_1=batch[\"attention_mask_1\"],\n",
    "                input_ids_2=batch[\"input_ids_2\"],\n",
    "                attention_mask_2=batch[\"attention_mask_2\"],\n",
    "            )\n",
    "            loss_reg = huber_loss(yhat, batch[\"target\"])\n",
    "            loss_con = info_nce(z1, z2, tau=TAU)\n",
    "            loss = (1.0 - ALPHA_CONTRAST) * loss_reg + ALPHA_CONTRAST * loss_con\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % GRAD_ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss_running += loss.item()\n",
    "        reg_loss_running += loss_reg.item()\n",
    "        con_loss_running += loss_con.item()\n",
    "\n",
    "        if step % 300 == 0:\n",
    "            print(f\"epoch {epoch} step {step}/{len(train_loader)} \"\n",
    "                  f\"loss={train_loss_running/step:.4f} reg={reg_loss_running/step:.4f} con={con_loss_running/step:.4f}\")\n",
    "\n",
    "    # ------------------ Validation ------------------\n",
    "    model.eval()\n",
    "    preds_log = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            yhat, _, _ = model(\n",
    "                input_ids_1=batch[\"input_ids_1\"],\n",
    "                attention_mask_1=batch[\"attention_mask_1\"],\n",
    "                input_ids_2=batch[\"input_ids_2\"],\n",
    "                attention_mask_2=batch[\"attention_mask_2\"],\n",
    "            )\n",
    "            preds_log.append(yhat.detach().float().cpu().numpy())\n",
    "\n",
    "    preds_log = np.concatenate(preds_log, axis=0)\n",
    "    # de-log for SMAPE\n",
    "    va_preds = delog2(preds_log)\n",
    "    va_true  = delog2(y_va_log)\n",
    "    smape = smape_np(va_true, va_preds)\n",
    "    print(f\"✅ Epoch {epoch}: VAL SMAPE = {smape:.3f}%\")\n",
    "\n",
    "    # Save best\n",
    "    if smape < best_smape - 1e-6:\n",
    "        best_smape = smape\n",
    "        patience = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"tokenizer\": MODEL_ID,\n",
    "                \"config\": {\n",
    "                    \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "                    \"TAU\": TAU,\n",
    "                    \"MAX_LEN\": MAX_LEN,\n",
    "                },\n",
    "            },\n",
    "            best_path\n",
    "        )\n",
    "        print(f\"💾 Saved new best to {best_path}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"⏸️ No improvement. Patience {patience}/{EARLY_STOP_ROUNDS}\")\n",
    "        if patience >= EARLY_STOP_ROUNDS:\n",
    "            print(\"🛑 Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f\"🏁 Best VAL SMAPE: {best_smape:.3f}% | Checkpoint: {best_path}\")\n",
    "\n",
    "# Save final artifacts\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics1.json\"), \"w\") as f:\n",
    "    json.dump({\"best_val_smape\": best_smape}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e136b9-c33e-41fe-abaf-c1553984c043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e63897-87d3-4d9c-b33b-21e8828de089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77bb75-ceeb-4426-9327-718dcd97c5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
