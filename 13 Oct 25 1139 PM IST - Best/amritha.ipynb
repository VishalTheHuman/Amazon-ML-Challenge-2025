{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b49fdf4-f10d-4546-a3ab-8c8283e7ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os, math, random, json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    CLIPModel,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243414e0-1f6d-471a-91c0-84a7871beb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================== Config ===========================\n",
    "CSV_PATH         = os.environ.get(\"TRAIN_CSV\", \"train_updated.csv\")   # must contain TEXT + PRICE + IMG path\n",
    "TEXT_COL         = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "PRICE_COL        = os.environ.get(\"PRICE_COL\", \"price\")\n",
    "IMG_COL          = os.environ.get(\"IMG_COL\",  \"image_path\")\n",
    "\n",
    "# Backbones\n",
    "CLIP_ID          = os.environ.get(\"CLIP_ID\",  \"openai/clip-vit-large-patch14\")\n",
    "DISTIL_ID_RAW    = os.environ.get(\"DISTIL_ID\", \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "OUTPUT_DIR       = os.environ.get(\"OUTPUT_DIR\", \"price_clip+distil_fusionv2\")\n",
    "\n",
    "SEED             = int(os.environ.get(\"SEED\", \"42\"))\n",
    "VAL_FRAC         = float(os.environ.get(\"VAL_FRAC\", \"0.01\"))           # set 0.5 for 50/50\n",
    "MAX_LEN_CLIP     = int(os.environ.get(\"MAX_LEN_CLIP\", \"64\"))          # CLIP tokenizer is shorter\n",
    "MAX_LEN_DISTIL   = int(os.environ.get(\"MAX_LEN_DISTIL\", \"192\"))\n",
    "\n",
    "BATCH_SIZE       = int(os.environ.get(\"BATCH_SIZE\", \"16\"))\n",
    "LR               = float(os.environ.get(\"LR\", \"2e-5\"))\n",
    "WEIGHT_DECAY     = float(os.environ.get(\"WEIGHT_DECAY\", \"0.01\"))\n",
    "EPOCHS           = int(os.environ.get(\"EPOCHS\", \"15\"))\n",
    "WARMUP_RATIO     = float(os.environ.get(\"WARMUP_RATIO\", \"0.06\"))\n",
    "GRAD_ACCUM       = int(os.environ.get(\"GRAD_ACCUM\", \"1\"))\n",
    "MAX_GRAD_NORM    = float(os.environ.get(\"MAX_GRAD_NORM\", \"1.0\"))\n",
    "FP16             = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "# Loss mixing\n",
    "ALPHA_CLIP_NCE   = float(os.environ.get(\"ALPHA_CLIP_NCE\", \"0.20\"))   # weight for CLIP img↔text InfoNCE\n",
    "ALPHA_TXT_NCE    = float(os.environ.get(\"ALPHA_TXT_NCE\", \"0.10\"))    # weight for Distil SimCSE NCE\n",
    "HUBER_DELTA      = float(os.environ.get(\"HUBER_DELTA\", \"1.0\"))\n",
    "TAU              = float(os.environ.get(\"TAU\", \"0.07\"))\n",
    "\n",
    "# DistilBERT augmentation (for contrastive)\n",
    "WORD_MASK_P      = float(os.environ.get(\"WORD_MASK_P\", \"0.06\"))\n",
    "\n",
    "EARLY_STOP_ROUNDS = int(os.environ.get(\"EARLY_STOP_ROUNDS\", \"5\"))\n",
    "MIN_PRICE        = float(os.environ.get(\"MIN_PRICE\", \"1e-6\"))\n",
    "\n",
    "# Missing image policy: zero => dummy pixels (vision forward), text_only => skip vision, drop => drop rows without image\n",
    "IMG_MISSING_POLICY = os.environ.get(\"IMG_MISSING_POLICY\", \"zero\").lower()  # zero | text_only | drop\n",
    "assert IMG_MISSING_POLICY in {\"zero\", \"text_only\", \"drop\"}\n",
    "\n",
    "# Freeze toggles (if VRAM tight)\n",
    "FREEZE_CLIP      = os.environ.get(\"FREEZE_CLIP\", \"false\").lower() == \"true\"\n",
    "FREEZE_DISTIL    = os.environ.get(\"FREEZE_DISTIL\", \"false\").lower() == \"true\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================== Utils ===========================\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def log2_price(p: np.ndarray) -> np.ndarray:\n",
    "    return np.log2(np.clip(p, MIN_PRICE, None))\n",
    "\n",
    "def delog2(x: np.ndarray) -> np.ndarray:\n",
    "    return np.power(2.0, x)\n",
    "\n",
    "def split_train_val(df: pd.DataFrame, frac_val: float = VAL_FRAC, seed: int = SEED):\n",
    "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    n_val = int(len(df) * frac_val)\n",
    "    return df.iloc[n_val:].reset_index(drop=True), df.iloc[:n_val].reset_index(drop=True)\n",
    "\n",
    "# =========================== Dataset ===========================\n",
    "class DualBackboneDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - CLIP: input_ids_clip, attention_mask_clip, pixel_values? (depends on policy)\n",
    "      - Distil (two augmented views): (ids1,mask1), (ids2,mask2)\n",
    "      - img_missing flag\n",
    "      - target (log2 price)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, text_col: str, img_col: str,\n",
    "                 y_log2: Optional[np.ndarray],\n",
    "                 clip_processor: AutoProcessor,\n",
    "                 distil_tok: AutoTokenizer,\n",
    "                 max_len_clip: int,\n",
    "                 max_len_distil: int,\n",
    "                 policy: str,\n",
    "                 training: bool):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.text_col = text_col\n",
    "        self.img_col  = img_col\n",
    "        self.y_log2   = y_log2\n",
    "        self.proc = clip_processor\n",
    "        self.tok  = distil_tok\n",
    "        self.max_len_clip   = max_len_clip\n",
    "        self.max_len_distil = max_len_distil\n",
    "        self.policy = policy\n",
    "        self.training = training\n",
    "\n",
    "        self.df[text_col] = self.df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "        if policy == \"drop\":\n",
    "            before = len(self.df)\n",
    "            self.df = self.df[self.df[img_col].apply(lambda p: isinstance(p, str) and len(p)>0 and os.path.exists(p))]\n",
    "            self.dropped_missing = before - len(self.df)\n",
    "        else:\n",
    "            self.dropped_missing = 0\n",
    "\n",
    "        # Pre-make a dummy pixel for 'zero' policy\n",
    "        dummy = self.proc(images=Image.new(\"RGB\", (224, 224)), return_tensors=\"pt\")\n",
    "        self._dummy_pixel = dummy[\"pixel_values\"].squeeze(0)  # (3,H,W)\n",
    "        self.missing_img_count = 0\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _load_image(self, pth: str):\n",
    "        if isinstance(pth, str) and pth and os.path.exists(pth):\n",
    "            try:\n",
    "                return Image.open(pth).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.missing_img_count += 1\n",
    "        return None\n",
    "\n",
    "    def _tok_clip_text(self, text: str):\n",
    "        enc = self.proc(text=[text], truncation=True, padding=False, max_length=self.max_len_clip, return_tensors=\"pt\")\n",
    "        return { \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "                 \"attention_mask\": enc[\"attention_mask\"].squeeze(0) }\n",
    "\n",
    "    def _tok_distil(self, text: str):\n",
    "        # two lightly different \"views\" via word masking\n",
    "        base = self.tok(text, truncation=True, padding=False, max_length=self.max_len_distil, return_tensors=\"pt\")\n",
    "        ids1 = base[\"input_ids\"].clone()\n",
    "        att1 = base[\"attention_mask\"].clone()\n",
    "\n",
    "        ids2 = ids1.clone()\n",
    "        att2 = att1.clone()\n",
    "\n",
    "        # random mask tokens in view2\n",
    "        if WORD_MASK_P > 0 and self.tok.mask_token_id is not None:\n",
    "            special = set(self.tok.all_special_ids)\n",
    "            for i in range(ids2.size(1)):\n",
    "                if ids2[0, i].item() in special: \n",
    "                    continue\n",
    "                if random.random() < WORD_MASK_P:\n",
    "                    ids2[0, i] = self.tok.mask_token_id\n",
    "\n",
    "        return {\n",
    "            \"ids1\": ids1.squeeze(0), \"att1\": att1.squeeze(0),\n",
    "            \"ids2\": ids2.squeeze(0), \"att2\": att2.squeeze(0),\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row  = self.df.iloc[idx]\n",
    "        text = row[self.text_col]\n",
    "        imgp = str(row[self.img_col]) if row[self.img_col] is not None else \"\"\n",
    "\n",
    "        # CLIP text\n",
    "        clip_txt = self._tok_clip_text(text)\n",
    "\n",
    "        # CLIP image (may be missing)\n",
    "        img_missing = 0\n",
    "        im = self._load_image(imgp)\n",
    "        pixel_values = None\n",
    "        if im is None:\n",
    "            img_missing = 1\n",
    "            if self.policy == \"zero\":\n",
    "                pixel_values = self._dummy_pixel.clone()\n",
    "        else:\n",
    "            enc_img = self.proc(images=im, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Distil two views\n",
    "        distil = self._tok_distil(text)\n",
    "\n",
    "        item = {\n",
    "            # CLIP text\n",
    "            \"clip_input_ids\": clip_txt[\"input_ids\"],\n",
    "            \"clip_attention_mask\": clip_txt[\"attention_mask\"],\n",
    "            # Distil 2 views\n",
    "            \"distil_ids1\": distil[\"ids1\"],\n",
    "            \"distil_att1\": distil[\"att1\"],\n",
    "            \"distil_ids2\": distil[\"ids2\"],\n",
    "            \"distil_att2\": distil[\"att2\"],\n",
    "            # Image\n",
    "            \"img_missing\": torch.tensor(img_missing, dtype=torch.uint8),\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            item[\"pixel_values\"] = pixel_values\n",
    "\n",
    "        if self.y_log2 is not None:\n",
    "            item[\"target\"] = torch.tensor(self.y_log2[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "@dataclass\n",
    "class DualCollate:\n",
    "    clip_tokenizer: any\n",
    "    distil_pad_id: int\n",
    "    def __call__(self, batch):\n",
    "        # Pad CLIP text with its tokenizer pad util\n",
    "        clip_ids  = [b[\"clip_input_ids\"] for b in batch]\n",
    "        clip_attn = [b[\"clip_attention_mask\"] for b in batch]\n",
    "        clip_padded = self.clip_tokenizer.pad(\n",
    "            {\"input_ids\": clip_ids, \"attention_mask\": clip_attn},\n",
    "            padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Pad Distil views manually\n",
    "        def pad_stack(tensors: List[torch.Tensor], pad_val: int):\n",
    "            maxlen = max(t.size(0) for t in tensors)\n",
    "            out = []\n",
    "            for t in tensors:\n",
    "                if t.size(0) < maxlen:\n",
    "                    pad = torch.full((maxlen - t.size(0),), pad_val, dtype=t.dtype)\n",
    "                    t = torch.cat([t, pad], dim=0)\n",
    "                out.append(t.unsqueeze(0))\n",
    "            return torch.cat(out, dim=0)\n",
    "\n",
    "        ids1 = pad_stack([b[\"distil_ids1\"] for b in batch], self.distil_pad_id)\n",
    "        att1 = pad_stack([b[\"distil_att1\"] for b in batch], 0)\n",
    "        ids2 = pad_stack([b[\"distil_ids2\"] for b in batch], self.distil_pad_id)\n",
    "        att2 = pad_stack([b[\"distil_att2\"] for b in batch], 0)\n",
    "\n",
    "        # Images (optional per item)\n",
    "        pixel_values = None\n",
    "        if any(\"pixel_values\" in b for b in batch):\n",
    "            C,H,W = next(b[\"pixel_values\"].shape for b in batch if \"pixel_values\" in b)\n",
    "            pv = []\n",
    "            for b in batch:\n",
    "                pv.append(b[\"pixel_values\"] if \"pixel_values\" in b else torch.zeros((C,H,W), dtype=torch.float32))\n",
    "            pixel_values = torch.stack(pv, dim=0)\n",
    "\n",
    "        res = {\n",
    "            \"clip_input_ids\": clip_padded[\"input_ids\"],\n",
    "            \"clip_attention_mask\": clip_padded[\"attention_mask\"],\n",
    "            \"distil_ids1\": ids1,\n",
    "            \"distil_att1\": att1,\n",
    "            \"distil_ids2\": ids2,\n",
    "            \"distil_att2\": att2,\n",
    "            \"img_missing\": torch.stack([b[\"img_missing\"] for b in batch], dim=0),\n",
    "        }\n",
    "        if pixel_values is not None: res[\"pixel_values\"] = pixel_values\n",
    "        if \"target\" in batch[0]:\n",
    "            res[\"target\"] = torch.stack([b[\"target\"] for b in batch], dim=0)\n",
    "        return res\n",
    "\n",
    "# =========================== Model ===========================\n",
    "class FusionRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    - CLIP image →  proj_dim_c (uses CLIP projection_dim)\n",
    "    - CLIP text  →  proj_dim_c\n",
    "    - Distil pooled → linear projection to proj_dim_d\n",
    "    Fuse: [norm(img_c), norm(txt_c), norm(distil_proj)] → MLP → price (log2)\n",
    "    Also returns z_img, z_txt, z_distil1/2 for contrastive losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_id: str, distil_id: str, distil_proj_dim: int = 256, head_hidden_mult: float = 2.0):\n",
    "        super().__init__()\n",
    "        # CLIP\n",
    "        self.clip = CLIPModel.from_pretrained(clip_id)\n",
    "        clip_dim = self.clip.config.projection_dim  # e.g., 768 or 1024\n",
    "\n",
    "        # Distil\n",
    "        try:\n",
    "            self.distil = AutoModel.from_pretrained(distil_id)\n",
    "            self.distil_id_used = distil_id\n",
    "        except Exception:\n",
    "            # fallback if finetuned SST-2 ID not available in your env/cache\n",
    "            self.distil = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "            self.distil_id_used = \"distilbert-base-uncased\"\n",
    "\n",
    "        distil_hidden = self.distil.config.dim  # 768\n",
    "        self.distil_proj = nn.Sequential(\n",
    "            nn.Linear(distil_hidden, distil_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(distil_hidden, distil_proj_dim),\n",
    "        )\n",
    "\n",
    "        fused_dim = clip_dim + clip_dim + distil_proj_dim\n",
    "        hidden = int(fused_dim * head_hidden_mult)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(fused_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        clip_input_ids, clip_attention_mask,\n",
    "        pixel_values: Optional[torch.Tensor],\n",
    "        img_missing: torch.Tensor,\n",
    "        distil_ids1, distil_att1,\n",
    "        distil_ids2, distil_att2,\n",
    "    ):\n",
    "        # CLIP text\n",
    "        txt_feat = self.clip.get_text_features(\n",
    "            input_ids=clip_input_ids,\n",
    "            attention_mask=clip_attention_mask\n",
    "        )  # (B, clip_dim)\n",
    "\n",
    "        # CLIP image\n",
    "        if pixel_values is not None:\n",
    "            vision_dtype = next(self.clip.vision_model.parameters()).dtype\n",
    "            img_feat = self.clip.get_image_features(pixel_values=pixel_values.to(dtype=vision_dtype))\n",
    "            # zero-out missing images to prevent leakage\n",
    "            if img_missing.any():\n",
    "                img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "        else:\n",
    "            img_feat = torch.zeros_like(txt_feat)\n",
    "\n",
    "        # Distil pooled embedding (view1 & view2 for SimCSE-style)\n",
    "        out1 = self.distil(input_ids=distil_ids1, attention_mask=distil_att1)\n",
    "        out2 = self.distil(input_ids=distil_ids2, attention_mask=distil_att2)\n",
    "        # use token[0] as pooled representation (DistilBERT has no pooler)\n",
    "        cls1 = out1.last_hidden_state[:, 0, :]\n",
    "        cls2 = out2.last_hidden_state[:, 0, :]\n",
    "        z_d1 = self.distil_proj(cls1)     # (B, dproj)\n",
    "        z_d2 = self.distil_proj(cls2)\n",
    "\n",
    "        # Normalize\n",
    "        img_n = F.normalize(img_feat, dim=-1)\n",
    "        txt_n = F.normalize(txt_feat, dim=-1)\n",
    "        d1_n  = F.normalize(z_d1, dim=-1)\n",
    "        d2_n  = F.normalize(z_d2, dim=-1)\n",
    "\n",
    "        # Fuse (use d1 for regression; d2 only for contrastive)\n",
    "        fused = torch.cat([img_n, txt_n, d1_n], dim=-1)\n",
    "        pred_log2 = self.head(fused).squeeze(-1)\n",
    "        return pred_log2, img_n, txt_n, d1_n, d2_n\n",
    "\n",
    "# =========================== Losses ===========================\n",
    "def info_nce(z_a: torch.Tensor, z_b: torch.Tensor, tau: float = TAU) -> torch.Tensor:\n",
    "    z_a = F.normalize(z_a, dim=-1)\n",
    "    z_b = F.normalize(z_b, dim=-1)\n",
    "    logits = torch.matmul(z_a, z_b.t()) / tau\n",
    "    labels = torch.arange(z_a.size(0), device=z_a.device)\n",
    "    return 0.5 * (F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels))\n",
    "\n",
    "def huber_loss(pred, target, delta=HUBER_DELTA):\n",
    "    return F.huber_loss(pred, target, delta=delta)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df9ff3b-a5dd-410a-bd1a-c018d2e82cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading CSV: train_updated.csv\n",
      "📊 Split: train=74250 | valid=750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.890</td>\n",
       "      <td>jl_fs/images/train/33127.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.120</td>\n",
       "      <td>jl_fs/images/train/198967.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.970</td>\n",
       "      <td>jl_fs/images/train/261251.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.340</td>\n",
       "      <td>jl_fs/images/train/55858.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.490</td>\n",
       "      <td>jl_fs/images/train/292686.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>41424</td>\n",
       "      <td>Item Name: ICE BREAKERS Spearmint Sugar Free M...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81p9PcPsff...</td>\n",
       "      <td>10.395</td>\n",
       "      <td>jl_fs/images/train/41424.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>35537</td>\n",
       "      <td>Item Name: Davidson's Organics, Vanilla Essenc...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51DDKoa+mb...</td>\n",
       "      <td>35.920</td>\n",
       "      <td>jl_fs/images/train/35537.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>249971</td>\n",
       "      <td>Item Name: Jolly Rancher Hard Candy - Blue Ras...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/91R2XCcpUf...</td>\n",
       "      <td>50.330</td>\n",
       "      <td>jl_fs/images/train/249971.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>188322</td>\n",
       "      <td>Item Name: Nescafe Dolce Gusto Capsules - CARA...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51W40YU98+...</td>\n",
       "      <td>15.275</td>\n",
       "      <td>jl_fs/images/train/188322.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>298504</td>\n",
       "      <td>Item Name: Pimenton de la Vera - Picante (2.47...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81dFnrP6C4...</td>\n",
       "      <td>28.240</td>\n",
       "      <td>jl_fs/images/train/298504.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                                    catalog_content  \\\n",
       "0          33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1         198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2         261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3          55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4         292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "...          ...                                                ...   \n",
       "74995      41424  Item Name: ICE BREAKERS Spearmint Sugar Free M...   \n",
       "74996      35537  Item Name: Davidson's Organics, Vanilla Essenc...   \n",
       "74997     249971  Item Name: Jolly Rancher Hard Candy - Blue Ras...   \n",
       "74998     188322  Item Name: Nescafe Dolce Gusto Capsules - CARA...   \n",
       "74999     298504  Item Name: Pimenton de la Vera - Picante (2.47...   \n",
       "\n",
       "                                              image_link   price  \\\n",
       "0      https://m.media-amazon.com/images/I/51mo8htwTH...   4.890   \n",
       "1      https://m.media-amazon.com/images/I/71YtriIHAA...  13.120   \n",
       "2      https://m.media-amazon.com/images/I/51+PFEe-w-...   1.970   \n",
       "3      https://m.media-amazon.com/images/I/41mu0HAToD...  30.340   \n",
       "4      https://m.media-amazon.com/images/I/41sA037+Qv...  66.490   \n",
       "...                                                  ...     ...   \n",
       "74995  https://m.media-amazon.com/images/I/81p9PcPsff...  10.395   \n",
       "74996  https://m.media-amazon.com/images/I/51DDKoa+mb...  35.920   \n",
       "74997  https://m.media-amazon.com/images/I/91R2XCcpUf...  50.330   \n",
       "74998  https://m.media-amazon.com/images/I/51W40YU98+...  15.275   \n",
       "74999  https://m.media-amazon.com/images/I/81dFnrP6C4...  28.240   \n",
       "\n",
       "                          image_path  \n",
       "0       jl_fs/images/train/33127.jpg  \n",
       "1      jl_fs/images/train/198967.jpg  \n",
       "2      jl_fs/images/train/261251.jpg  \n",
       "3       jl_fs/images/train/55858.jpg  \n",
       "4      jl_fs/images/train/292686.jpg  \n",
       "...                              ...  \n",
       "74995   jl_fs/images/train/41424.jpg  \n",
       "74996   jl_fs/images/train/35537.jpg  \n",
       "74997  jl_fs/images/train/249971.jpg  \n",
       "74998  jl_fs/images/train/188322.jpg  \n",
       "74999  jl_fs/images/train/298504.jpg  \n",
       "\n",
       "[75000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================== Load data ===========================\n",
    "print(f\"🔧 Loading CSV: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "for col, name in [(TEXT_COL, \"TEXT_COL\"), (PRICE_COL, \"PRICE_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"{name} '{col}' not in CSV columns={df.columns.tolist()}\")\n",
    "\n",
    "# clean / guard\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "df = df.loc[pd.to_numeric(df[PRICE_COL], errors=\"coerce\").notnull()].copy()\n",
    "df[PRICE_COL] = df[PRICE_COL].astype(float)\n",
    "df = df.loc[df[PRICE_COL] >= 0.0].reset_index(drop=True)\n",
    "\n",
    "df_tr, df_va = split_train_val(df, frac_val=VAL_FRAC, seed=SEED)\n",
    "print(f\"📊 Split: train={len(df_tr)} | valid={len(df_va)}\")\n",
    "y_tr_log = log2_price(df_tr[PRICE_COL].values)\n",
    "y_va_log = log2_price(df_va[PRICE_COL].values)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c297f5c-8120-48cc-8a72-b92f6aaf49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_proc = AutoProcessor.from_pretrained(CLIP_ID)\n",
    "# Distil tokenizer (with safe fallback)\n",
    "try:\n",
    "    distil_tok = AutoTokenizer.from_pretrained(DISTIL_ID_RAW, use_fast=True)\n",
    "    used_distil_tok_id = DISTIL_ID_RAW\n",
    "except Exception:\n",
    "    distil_tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
    "    used_distil_tok_id = \"distilbert-base-uncased\"\n",
    "\n",
    "if distil_tok.mask_token is None:\n",
    "    distil_tok.add_special_tokens({\"mask_token\": \"[MASK]\"})\n",
    "\n",
    "train_ds = DualBackboneDataset(\n",
    "    df=df_tr, text_col=TEXT_COL, img_col=IMG_COL, y_log2=y_tr_log,\n",
    "    clip_processor=clip_proc, distil_tok=distil_tok,\n",
    "    max_len_clip=MAX_LEN_CLIP, max_len_distil=MAX_LEN_DISTIL,\n",
    "    policy=IMG_MISSING_POLICY, training=True\n",
    ")\n",
    "val_ds = DualBackboneDataset(\n",
    "    df=df_va, text_col=TEXT_COL, img_col=IMG_COL, y_log2=y_va_log,\n",
    "    clip_processor=clip_proc, distil_tok=distil_tok,\n",
    "    max_len_clip=MAX_LEN_CLIP, max_len_distil=MAX_LEN_DISTIL,\n",
    "    policy=IMG_MISSING_POLICY, training=False\n",
    ")\n",
    "collate = DualCollate(clip_tokenizer=clip_proc.tokenizer,\n",
    "                      distil_pad_id=distil_tok.pad_token_id if distil_tok.pad_token_id is not None else 0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaae5a98-30d7-45e5-a5c9-95059e0e3be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️ Device: cuda\n",
      "🧮 Trainable params: 501,196,546\n"
     ]
    }
   ],
   "source": [
    "# =========================== Build model ===========================\n",
    "model = FusionRegressor(CLIP_ID, used_distil_tok_id).to(device)\n",
    "\n",
    "# Optionally freeze\n",
    "if FREEZE_CLIP:\n",
    "    for p in model.clip.parameters(): p.requires_grad = False\n",
    "if FREEZE_DISTIL:\n",
    "    for p in model.distil.parameters(): p.requires_grad = False\n",
    "\n",
    "print(f\"🖥️ Device: {device}\")\n",
    "print(f\"🧮 Trainable params: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a44312-302d-43f9-814b-5d33bb9f9ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_204/2190513594.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing images counted (train/val): 0/0\n",
      "🗑️ Dropped (policy=drop) train/val: 0/0\n"
     ]
    }
   ],
   "source": [
    "# Optimizer & scheduler\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "named_params = list(model.named_parameters())\n",
    "grouped = [\n",
    "    {\"params\": [p for n,p in named_params if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": [p for n,p in named_params if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(grouped, lr=LR)\n",
    "num_training_steps = EPOCHS * max(1, math.ceil(len(train_loader) / max(1, GRAD_ACCUM)))\n",
    "num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup, num_training_steps)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "best_smape = float(\"inf\")\n",
    "best_path = os.path.join(OUTPUT_DIR, \"best_clip+distil.pt\")\n",
    "patience = 0\n",
    "\n",
    "# Warmup to count drops/missing\n",
    "if len(train_loader) > 0: _ = next(iter(train_loader))\n",
    "if len(val_loader) > 0: _ = next(iter(val_loader))\n",
    "print(f\"⚠️ Missing images counted (train/val): {train_ds.missing_img_count}/{val_ds.missing_img_count}\")\n",
    "print(f\"🗑️ Dropped (policy=drop) train/val: {getattr(train_ds,'dropped_missing',0)}/{getattr(val_ds,'dropped_missing',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42f782-5a56-4320-8f98-d9c178c5b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_204/1213864648.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 200/4641 loss=3.4633 reg=3.2916 nce_clip=0.3388 nce_txt=1.0401\n",
      "epoch 1 step 400/4641 loss=3.3093 reg=3.1858 nce_clip=0.2981 nce_txt=0.6388\n",
      "epoch 1 step 600/4641 loss=3.0932 reg=2.9787 nce_clip=0.3325 nce_txt=0.4800\n",
      "epoch 1 step 800/4641 loss=2.8328 reg=2.7117 nce_clip=0.3967 nce_txt=0.4176\n",
      "epoch 1 step 1000/4641 loss=2.5367 reg=2.4093 nce_clip=0.4465 nce_txt=0.3815\n",
      "epoch 1 step 1200/4641 loss=2.2648 reg=2.1379 nce_clip=0.4608 nce_txt=0.3477\n",
      "epoch 1 step 1400/4641 loss=2.0480 reg=1.9257 nce_clip=0.4533 nce_txt=0.3165\n",
      "epoch 1 step 1600/4641 loss=1.8710 reg=1.7548 nce_clip=0.4364 nce_txt=0.2888\n",
      "epoch 1 step 1800/4641 loss=1.7338 reg=1.6232 nce_clip=0.4197 nce_txt=0.2663\n",
      "epoch 1 step 2000/4641 loss=1.6187 reg=1.5138 nce_clip=0.4011 nce_txt=0.2469\n",
      "epoch 1 step 2200/4641 loss=1.5234 reg=1.4235 nce_clip=0.3848 nce_txt=0.2296\n",
      "epoch 1 step 2400/4641 loss=1.4439 reg=1.3484 nce_clip=0.3700 nce_txt=0.2151\n",
      "epoch 1 step 2600/4641 loss=1.3743 reg=1.2827 nce_clip=0.3565 nce_txt=0.2025\n",
      "epoch 1 step 2800/4641 loss=1.3139 reg=1.2262 nce_clip=0.3431 nce_txt=0.1912\n",
      "epoch 1 step 3000/4641 loss=1.2611 reg=1.1761 nce_clip=0.3346 nce_txt=0.1813\n",
      "epoch 1 step 3200/4641 loss=1.2139 reg=1.1316 nce_clip=0.3255 nce_txt=0.1725\n",
      "epoch 1 step 3400/4641 loss=1.1721 reg=1.0923 nce_clip=0.3164 nce_txt=0.1646\n",
      "epoch 1 step 3600/4641 loss=1.1348 reg=1.0574 nce_clip=0.3086 nce_txt=0.1575\n",
      "epoch 1 step 3800/4641 loss=1.1013 reg=1.0258 nce_clip=0.3019 nce_txt=0.1512\n",
      "epoch 1 step 4000/4641 loss=1.0713 reg=0.9974 nce_clip=0.2968 nce_txt=0.1457\n",
      "epoch 1 step 4200/4641 loss=1.0444 reg=0.9722 nce_clip=0.2908 nce_txt=0.1405\n",
      "epoch 1 step 4400/4641 loss=1.0205 reg=0.9498 nce_clip=0.2854 nce_txt=0.1357\n",
      "epoch 1 step 4600/4641 loss=0.9971 reg=0.9278 nce_clip=0.2811 nce_txt=0.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1: VAL SMAPE = 52.559%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 step 200/4641 loss=0.4287 reg=0.3972 nce_clip=0.1425 nce_txt=0.0299\n",
      "epoch 2 step 400/4641 loss=0.4225 reg=0.3914 nce_clip=0.1415 nce_txt=0.0281\n",
      "epoch 2 step 600/4641 loss=0.4198 reg=0.3893 nce_clip=0.1385 nce_txt=0.0277\n",
      "epoch 2 step 800/4641 loss=0.4196 reg=0.3892 nce_clip=0.1382 nce_txt=0.0277\n",
      "epoch 2 step 1000/4641 loss=0.4231 reg=0.3920 nce_clip=0.1418 nce_txt=0.0278\n",
      "epoch 2 step 1200/4641 loss=0.4211 reg=0.3898 nce_clip=0.1427 nce_txt=0.0279\n",
      "epoch 2 step 1400/4641 loss=0.4223 reg=0.3912 nce_clip=0.1422 nce_txt=0.0272\n",
      "epoch 2 step 1600/4641 loss=0.4216 reg=0.3906 nce_clip=0.1415 nce_txt=0.0269\n",
      "epoch 2 step 1800/4641 loss=0.4206 reg=0.3894 nce_clip=0.1424 nce_txt=0.0266\n",
      "epoch 2 step 2000/4641 loss=0.4184 reg=0.3872 nce_clip=0.1429 nce_txt=0.0259\n",
      "epoch 2 step 2200/4641 loss=0.4176 reg=0.3866 nce_clip=0.1422 nce_txt=0.0255\n",
      "epoch 2 step 2400/4641 loss=0.4172 reg=0.3862 nce_clip=0.1424 nce_txt=0.0253\n",
      "epoch 2 step 2600/4641 loss=0.4173 reg=0.3863 nce_clip=0.1424 nce_txt=0.0250\n",
      "epoch 2 step 2800/4641 loss=0.4178 reg=0.3869 nce_clip=0.1423 nce_txt=0.0249\n",
      "epoch 2 step 3000/4641 loss=0.4179 reg=0.3869 nce_clip=0.1424 nce_txt=0.0246\n",
      "epoch 2 step 3200/4641 loss=0.4175 reg=0.3866 nce_clip=0.1423 nce_txt=0.0244\n",
      "epoch 2 step 3400/4641 loss=0.4171 reg=0.3862 nce_clip=0.1426 nce_txt=0.0242\n",
      "epoch 2 step 3600/4641 loss=0.4172 reg=0.3862 nce_clip=0.1426 nce_txt=0.0240\n",
      "epoch 2 step 3800/4641 loss=0.4164 reg=0.3855 nce_clip=0.1425 nce_txt=0.0238\n",
      "epoch 2 step 4000/4641 loss=0.4164 reg=0.3856 nce_clip=0.1420 nce_txt=0.0237\n",
      "epoch 2 step 4200/4641 loss=0.4168 reg=0.3861 nce_clip=0.1415 nce_txt=0.0235\n",
      "epoch 2 step 4400/4641 loss=0.4171 reg=0.3865 nce_clip=0.1416 nce_txt=0.0234\n",
      "epoch 2 step 4600/4641 loss=0.4164 reg=0.3857 nce_clip=0.1418 nce_txt=0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2: VAL SMAPE = 47.802%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 step 200/4641 loss=0.3016 reg=0.2794 nce_clip=0.1026 nce_txt=0.0165\n",
      "epoch 3 step 400/4641 loss=0.3050 reg=0.2831 nce_clip=0.1012 nce_txt=0.0166\n",
      "epoch 3 step 600/4641 loss=0.3023 reg=0.2804 nce_clip=0.1007 nce_txt=0.0172\n",
      "epoch 3 step 800/4641 loss=0.3033 reg=0.2814 nce_clip=0.1013 nce_txt=0.0170\n",
      "epoch 3 step 1000/4641 loss=0.3028 reg=0.2808 nce_clip=0.1017 nce_txt=0.0166\n",
      "epoch 3 step 1200/4641 loss=0.3029 reg=0.2810 nce_clip=0.1014 nce_txt=0.0162\n",
      "epoch 3 step 1400/4641 loss=0.3058 reg=0.2840 nce_clip=0.1012 nce_txt=0.0163\n",
      "epoch 3 step 1600/4641 loss=0.3057 reg=0.2838 nce_clip=0.1016 nce_txt=0.0163\n",
      "epoch 3 step 1800/4641 loss=0.3065 reg=0.2845 nce_clip=0.1016 nce_txt=0.0163\n",
      "epoch 3 step 2000/4641 loss=0.3069 reg=0.2850 nce_clip=0.1015 nce_txt=0.0162\n",
      "epoch 3 step 2200/4641 loss=0.3078 reg=0.2859 nce_clip=0.1012 nce_txt=0.0162\n",
      "epoch 3 step 2400/4641 loss=0.3071 reg=0.2854 nce_clip=0.1004 nce_txt=0.0164\n",
      "epoch 3 step 2600/4641 loss=0.3079 reg=0.2861 nce_clip=0.1005 nce_txt=0.0164\n",
      "epoch 3 step 2800/4641 loss=0.3078 reg=0.2861 nce_clip=0.1001 nce_txt=0.0162\n",
      "epoch 3 step 3000/4641 loss=0.3090 reg=0.2874 nce_clip=0.0999 nce_txt=0.0161\n",
      "epoch 3 step 3200/4641 loss=0.3092 reg=0.2875 nce_clip=0.1005 nce_txt=0.0161\n",
      "epoch 3 step 3400/4641 loss=0.3103 reg=0.2886 nce_clip=0.1006 nce_txt=0.0161\n",
      "epoch 3 step 3600/4641 loss=0.3106 reg=0.2889 nce_clip=0.1005 nce_txt=0.0160\n",
      "epoch 3 step 3800/4641 loss=0.3116 reg=0.2900 nce_clip=0.1001 nce_txt=0.0160\n",
      "epoch 3 step 4000/4641 loss=0.3124 reg=0.2909 nce_clip=0.0998 nce_txt=0.0159\n",
      "epoch 3 step 4200/4641 loss=0.3129 reg=0.2914 nce_clip=0.0999 nce_txt=0.0159\n",
      "epoch 3 step 4400/4641 loss=0.3135 reg=0.2920 nce_clip=0.0998 nce_txt=0.0160\n",
      "epoch 3 step 4600/4641 loss=0.3137 reg=0.2921 nce_clip=0.0996 nce_txt=0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3: VAL SMAPE = 46.846%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 step 200/4641 loss=0.2222 reg=0.2050 nce_clip=0.0791 nce_txt=0.0143\n",
      "epoch 4 step 400/4641 loss=0.2240 reg=0.2065 nce_clip=0.0806 nce_txt=0.0139\n",
      "epoch 4 step 600/4641 loss=0.2246 reg=0.2072 nce_clip=0.0803 nce_txt=0.0135\n",
      "epoch 4 step 800/4641 loss=0.2229 reg=0.2058 nce_clip=0.0788 nce_txt=0.0134\n",
      "epoch 4 step 1000/4641 loss=0.2211 reg=0.2043 nce_clip=0.0774 nce_txt=0.0130\n",
      "epoch 4 step 1200/4641 loss=0.2222 reg=0.2057 nce_clip=0.0761 nce_txt=0.0129\n",
      "epoch 4 step 1400/4641 loss=0.2218 reg=0.2054 nce_clip=0.0757 nce_txt=0.0128\n",
      "epoch 4 step 1600/4641 loss=0.2220 reg=0.2056 nce_clip=0.0756 nce_txt=0.0128\n",
      "epoch 4 step 1800/4641 loss=0.2224 reg=0.2060 nce_clip=0.0758 nce_txt=0.0127\n",
      "epoch 4 step 2000/4641 loss=0.2219 reg=0.2053 nce_clip=0.0769 nce_txt=0.0127\n",
      "epoch 4 step 2200/4641 loss=0.2225 reg=0.2057 nce_clip=0.0778 nce_txt=0.0127\n",
      "epoch 4 step 2400/4641 loss=0.2239 reg=0.2069 nce_clip=0.0783 nce_txt=0.0128\n",
      "epoch 4 step 2600/4641 loss=0.2252 reg=0.2083 nce_clip=0.0782 nce_txt=0.0129\n",
      "epoch 4 step 2800/4641 loss=0.2263 reg=0.2093 nce_clip=0.0783 nce_txt=0.0130\n",
      "epoch 4 step 3000/4641 loss=0.2273 reg=0.2103 nce_clip=0.0788 nce_txt=0.0130\n",
      "epoch 4 step 3200/4641 loss=0.2280 reg=0.2110 nce_clip=0.0786 nce_txt=0.0130\n",
      "epoch 4 step 3400/4641 loss=0.2281 reg=0.2111 nce_clip=0.0783 nce_txt=0.0129\n",
      "epoch 4 step 3600/4641 loss=0.2287 reg=0.2119 nce_clip=0.0778 nce_txt=0.0129\n",
      "epoch 4 step 3800/4641 loss=0.2294 reg=0.2125 nce_clip=0.0779 nce_txt=0.0130\n",
      "epoch 4 step 4000/4641 loss=0.2292 reg=0.2124 nce_clip=0.0777 nce_txt=0.0130\n",
      "epoch 4 step 4200/4641 loss=0.2292 reg=0.2123 nce_clip=0.0778 nce_txt=0.0130\n",
      "epoch 4 step 4400/4641 loss=0.2295 reg=0.2127 nce_clip=0.0778 nce_txt=0.0130\n",
      "epoch 4 step 4600/4641 loss=0.2304 reg=0.2136 nce_clip=0.0778 nce_txt=0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4: VAL SMAPE = 45.268%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 step 200/4641 loss=0.1559 reg=0.1418 nce_clip=0.0635 nce_txt=0.0135\n",
      "epoch 5 step 400/4641 loss=0.1607 reg=0.1467 nce_clip=0.0635 nce_txt=0.0132\n",
      "epoch 5 step 600/4641 loss=0.1589 reg=0.1451 nce_clip=0.0629 nce_txt=0.0129\n",
      "epoch 5 step 800/4641 loss=0.1614 reg=0.1475 nce_clip=0.0634 nce_txt=0.0128\n",
      "epoch 5 step 1000/4641 loss=0.1612 reg=0.1475 nce_clip=0.0626 nce_txt=0.0125\n",
      "epoch 5 step 1200/4641 loss=0.1603 reg=0.1465 nce_clip=0.0626 nce_txt=0.0127\n",
      "epoch 5 step 1400/4641 loss=0.1607 reg=0.1470 nce_clip=0.0622 nce_txt=0.0127\n",
      "epoch 5 step 1600/4641 loss=0.1611 reg=0.1476 nce_clip=0.0616 nce_txt=0.0125\n",
      "epoch 5 step 1800/4641 loss=0.1619 reg=0.1485 nce_clip=0.0611 nce_txt=0.0125\n",
      "epoch 5 step 2000/4641 loss=0.1626 reg=0.1492 nce_clip=0.0609 nce_txt=0.0124\n",
      "epoch 5 step 2200/4641 loss=0.1629 reg=0.1494 nce_clip=0.0612 nce_txt=0.0123\n",
      "epoch 5 step 2400/4641 loss=0.1633 reg=0.1498 nce_clip=0.0614 nce_txt=0.0122\n",
      "epoch 5 step 2600/4641 loss=0.1639 reg=0.1503 nce_clip=0.0619 nce_txt=0.0122\n",
      "epoch 5 step 2800/4641 loss=0.1647 reg=0.1511 nce_clip=0.0616 nce_txt=0.0122\n",
      "epoch 5 step 3000/4641 loss=0.1656 reg=0.1521 nce_clip=0.0615 nce_txt=0.0122\n",
      "epoch 5 step 3200/4641 loss=0.1660 reg=0.1524 nce_clip=0.0620 nce_txt=0.0121\n",
      "epoch 5 step 3400/4641 loss=0.1661 reg=0.1524 nce_clip=0.0622 nce_txt=0.0121\n",
      "epoch 5 step 3600/4641 loss=0.1666 reg=0.1530 nce_clip=0.0621 nce_txt=0.0120\n",
      "epoch 5 step 3800/4641 loss=0.1669 reg=0.1533 nce_clip=0.0618 nce_txt=0.0119\n",
      "epoch 5 step 4000/4641 loss=0.1671 reg=0.1536 nce_clip=0.0615 nce_txt=0.0118\n",
      "epoch 5 step 4200/4641 loss=0.1670 reg=0.1535 nce_clip=0.0613 nce_txt=0.0119\n",
      "epoch 5 step 4400/4641 loss=0.1673 reg=0.1538 nce_clip=0.0616 nce_txt=0.0118\n",
      "epoch 5 step 4600/4641 loss=0.1677 reg=0.1543 nce_clip=0.0616 nce_txt=0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5: VAL SMAPE = 46.064%\n",
      "⏸️ No improvement. Patience 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 step 200/4641 loss=0.1210 reg=0.1086 nce_clip=0.0560 nce_txt=0.0125\n",
      "epoch 6 step 400/4641 loss=0.1203 reg=0.1085 nce_clip=0.0530 nce_txt=0.0114\n",
      "epoch 6 step 600/4641 loss=0.1208 reg=0.1093 nce_clip=0.0516 nce_txt=0.0116\n",
      "epoch 6 step 800/4641 loss=0.1224 reg=0.1112 nce_clip=0.0503 nce_txt=0.0116\n",
      "epoch 6 step 1000/4641 loss=0.1214 reg=0.1103 nce_clip=0.0498 nce_txt=0.0114\n",
      "epoch 6 step 1200/4641 loss=0.1228 reg=0.1117 nce_clip=0.0498 nce_txt=0.0113\n",
      "epoch 6 step 1400/4641 loss=0.1231 reg=0.1121 nce_clip=0.0495 nce_txt=0.0112\n",
      "epoch 6 step 1600/4641 loss=0.1232 reg=0.1122 nce_clip=0.0494 nce_txt=0.0111\n",
      "epoch 6 step 1800/4641 loss=0.1229 reg=0.1118 nce_clip=0.0498 nce_txt=0.0110\n",
      "epoch 6 step 2000/4641 loss=0.1232 reg=0.1121 nce_clip=0.0501 nce_txt=0.0110\n",
      "epoch 6 step 2200/4641 loss=0.1231 reg=0.1121 nce_clip=0.0497 nce_txt=0.0108\n",
      "epoch 6 step 2400/4641 loss=0.1235 reg=0.1125 nce_clip=0.0494 nce_txt=0.0108\n",
      "epoch 6 step 2600/4641 loss=0.1238 reg=0.1129 nce_clip=0.0494 nce_txt=0.0108\n",
      "epoch 6 step 2800/4641 loss=0.1244 reg=0.1135 nce_clip=0.0493 nce_txt=0.0107\n",
      "epoch 6 step 3000/4641 loss=0.1249 reg=0.1140 nce_clip=0.0494 nce_txt=0.0106\n",
      "epoch 6 step 3200/4641 loss=0.1249 reg=0.1139 nce_clip=0.0494 nce_txt=0.0106\n",
      "epoch 6 step 3400/4641 loss=0.1252 reg=0.1142 nce_clip=0.0492 nce_txt=0.0106\n",
      "epoch 6 step 3600/4641 loss=0.1256 reg=0.1147 nce_clip=0.0493 nce_txt=0.0106\n",
      "epoch 6 step 3800/4641 loss=0.1254 reg=0.1145 nce_clip=0.0491 nce_txt=0.0106\n",
      "epoch 6 step 4000/4641 loss=0.1256 reg=0.1147 nce_clip=0.0492 nce_txt=0.0106\n",
      "epoch 6 step 4200/4641 loss=0.1255 reg=0.1147 nce_clip=0.0492 nce_txt=0.0105\n",
      "epoch 6 step 4400/4641 loss=0.1257 reg=0.1149 nce_clip=0.0492 nce_txt=0.0105\n",
      "epoch 6 step 4600/4641 loss=0.1259 reg=0.1150 nce_clip=0.0490 nce_txt=0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6: VAL SMAPE = 45.094%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 step 200/4641 loss=0.0968 reg=0.0878 nce_clip=0.0402 nce_txt=0.0100\n",
      "epoch 7 step 400/4641 loss=0.0981 reg=0.0889 nce_clip=0.0406 nce_txt=0.0103\n",
      "epoch 7 step 600/4641 loss=0.0974 reg=0.0883 nce_clip=0.0400 nce_txt=0.0104\n",
      "epoch 7 step 800/4641 loss=0.0975 reg=0.0885 nce_clip=0.0397 nce_txt=0.0104\n",
      "epoch 7 step 1000/4641 loss=0.0980 reg=0.0889 nce_clip=0.0402 nce_txt=0.0103\n",
      "epoch 7 step 1200/4641 loss=0.0979 reg=0.0887 nce_clip=0.0405 nce_txt=0.0103\n",
      "epoch 7 step 1400/4641 loss=0.0971 reg=0.0881 nce_clip=0.0402 nce_txt=0.0101\n",
      "epoch 7 step 1600/4641 loss=0.0975 reg=0.0884 nce_clip=0.0403 nce_txt=0.0100\n",
      "epoch 7 step 1800/4641 loss=0.0977 reg=0.0887 nce_clip=0.0404 nce_txt=0.0099\n",
      "epoch 7 step 2000/4641 loss=0.0969 reg=0.0878 nce_clip=0.0406 nce_txt=0.0099\n",
      "epoch 7 step 2200/4641 loss=0.0966 reg=0.0876 nce_clip=0.0403 nce_txt=0.0098\n",
      "epoch 7 step 2400/4641 loss=0.0966 reg=0.0876 nce_clip=0.0404 nce_txt=0.0098\n",
      "epoch 7 step 2600/4641 loss=0.0966 reg=0.0876 nce_clip=0.0402 nce_txt=0.0098\n",
      "epoch 7 step 2800/4641 loss=0.0963 reg=0.0873 nce_clip=0.0405 nce_txt=0.0097\n",
      "epoch 7 step 3000/4641 loss=0.0963 reg=0.0873 nce_clip=0.0403 nce_txt=0.0096\n",
      "epoch 7 step 3200/4641 loss=0.0962 reg=0.0872 nce_clip=0.0401 nce_txt=0.0096\n",
      "epoch 7 step 3600/4641 loss=0.0959 reg=0.0870 nce_clip=0.0399 nce_txt=0.0095\n",
      "epoch 7 step 3800/4641 loss=0.0959 reg=0.0870 nce_clip=0.0397 nce_txt=0.0095\n",
      "epoch 7 step 4000/4641 loss=0.0957 reg=0.0868 nce_clip=0.0397 nce_txt=0.0094\n",
      "epoch 7 step 4200/4641 loss=0.0957 reg=0.0868 nce_clip=0.0397 nce_txt=0.0093\n",
      "epoch 7 step 4400/4641 loss=0.0956 reg=0.0868 nce_clip=0.0395 nce_txt=0.0093\n",
      "epoch 7 step 4600/4641 loss=0.0956 reg=0.0868 nce_clip=0.0394 nce_txt=0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7: VAL SMAPE = 44.585%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 step 200/4641 loss=0.0800 reg=0.0727 nce_clip=0.0323 nce_txt=0.0084\n",
      "epoch 8 step 400/4641 loss=0.0798 reg=0.0724 nce_clip=0.0324 nce_txt=0.0085\n",
      "epoch 8 step 600/4641 loss=0.0802 reg=0.0728 nce_clip=0.0330 nce_txt=0.0082\n",
      "epoch 8 step 800/4641 loss=0.0787 reg=0.0712 nce_clip=0.0331 nce_txt=0.0081\n",
      "epoch 8 step 1000/4641 loss=0.0775 reg=0.0702 nce_clip=0.0325 nce_txt=0.0081\n",
      "epoch 8 step 1200/4641 loss=0.0775 reg=0.0701 nce_clip=0.0330 nce_txt=0.0083\n",
      "epoch 8 step 1400/4641 loss=0.0771 reg=0.0696 nce_clip=0.0333 nce_txt=0.0084\n",
      "epoch 8 step 1600/4641 loss=0.0768 reg=0.0693 nce_clip=0.0332 nce_txt=0.0083\n",
      "epoch 8 step 1800/4641 loss=0.0762 reg=0.0688 nce_clip=0.0330 nce_txt=0.0083\n",
      "epoch 8 step 2000/4641 loss=0.0758 reg=0.0684 nce_clip=0.0328 nce_txt=0.0082\n",
      "epoch 8 step 2200/4641 loss=0.0752 reg=0.0679 nce_clip=0.0328 nce_txt=0.0081\n",
      "epoch 8 step 2400/4641 loss=0.0750 reg=0.0677 nce_clip=0.0325 nce_txt=0.0080\n",
      "epoch 8 step 2600/4641 loss=0.0749 reg=0.0676 nce_clip=0.0323 nce_txt=0.0079\n",
      "epoch 8 step 2800/4641 loss=0.0748 reg=0.0675 nce_clip=0.0322 nce_txt=0.0079\n",
      "epoch 8 step 3000/4641 loss=0.0748 reg=0.0676 nce_clip=0.0322 nce_txt=0.0079\n",
      "epoch 8 step 3200/4641 loss=0.0750 reg=0.0678 nce_clip=0.0323 nce_txt=0.0078\n",
      "epoch 8 step 3400/4641 loss=0.0748 reg=0.0676 nce_clip=0.0321 nce_txt=0.0078\n",
      "epoch 8 step 3600/4641 loss=0.0746 reg=0.0674 nce_clip=0.0321 nce_txt=0.0078\n",
      "epoch 8 step 3800/4641 loss=0.0744 reg=0.0673 nce_clip=0.0319 nce_txt=0.0077\n",
      "epoch 8 step 4000/4641 loss=0.0744 reg=0.0672 nce_clip=0.0319 nce_txt=0.0077\n",
      "epoch 8 step 4200/4641 loss=0.0742 reg=0.0671 nce_clip=0.0318 nce_txt=0.0077\n",
      "epoch 8 step 4400/4641 loss=0.0742 reg=0.0671 nce_clip=0.0318 nce_txt=0.0076\n",
      "epoch 8 step 4600/4641 loss=0.0740 reg=0.0669 nce_clip=0.0318 nce_txt=0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8: VAL SMAPE = 44.108%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 step 200/4641 loss=0.0638 reg=0.0576 nce_clip=0.0273 nce_txt=0.0078\n",
      "epoch 9 step 400/4641 loss=0.0595 reg=0.0535 nce_clip=0.0263 nce_txt=0.0074\n",
      "epoch 9 step 600/4641 loss=0.0598 reg=0.0537 nce_clip=0.0267 nce_txt=0.0074\n",
      "epoch 9 step 800/4641 loss=0.0596 reg=0.0536 nce_clip=0.0262 nce_txt=0.0073\n",
      "epoch 9 step 1000/4641 loss=0.0593 reg=0.0534 nce_clip=0.0260 nce_txt=0.0070\n",
      "epoch 9 step 1200/4641 loss=0.0589 reg=0.0529 nce_clip=0.0267 nce_txt=0.0069\n",
      "epoch 9 step 1400/4641 loss=0.0592 reg=0.0532 nce_clip=0.0266 nce_txt=0.0071\n",
      "epoch 9 step 1600/4641 loss=0.0588 reg=0.0528 nce_clip=0.0263 nce_txt=0.0071\n",
      "epoch 9 step 1800/4641 loss=0.0588 reg=0.0529 nce_clip=0.0262 nce_txt=0.0071\n",
      "epoch 9 step 2000/4641 loss=0.0588 reg=0.0528 nce_clip=0.0265 nce_txt=0.0071\n",
      "epoch 9 step 2200/4641 loss=0.0588 reg=0.0528 nce_clip=0.0264 nce_txt=0.0071\n",
      "epoch 9 step 2400/4641 loss=0.0587 reg=0.0527 nce_clip=0.0263 nce_txt=0.0070\n",
      "epoch 9 step 2600/4641 loss=0.0585 reg=0.0526 nce_clip=0.0261 nce_txt=0.0070\n",
      "epoch 9 step 2800/4641 loss=0.0585 reg=0.0526 nce_clip=0.0261 nce_txt=0.0070\n",
      "epoch 9 step 3000/4641 loss=0.0583 reg=0.0524 nce_clip=0.0259 nce_txt=0.0069\n",
      "epoch 9 step 3200/4641 loss=0.0582 reg=0.0524 nce_clip=0.0258 nce_txt=0.0069\n",
      "epoch 9 step 3400/4641 loss=0.0580 reg=0.0522 nce_clip=0.0258 nce_txt=0.0069\n",
      "epoch 9 step 3800/4641 loss=0.0578 reg=0.0520 nce_clip=0.0256 nce_txt=0.0069\n",
      "epoch 9 step 4000/4641 loss=0.0577 reg=0.0519 nce_clip=0.0255 nce_txt=0.0068\n",
      "epoch 9 step 4200/4641 loss=0.0576 reg=0.0518 nce_clip=0.0254 nce_txt=0.0068\n",
      "epoch 9 step 4400/4641 loss=0.0575 reg=0.0517 nce_clip=0.0254 nce_txt=0.0068\n",
      "epoch 9 step 4600/4641 loss=0.0574 reg=0.0517 nce_clip=0.0253 nce_txt=0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9: VAL SMAPE = 43.070%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 step 200/4641 loss=0.0469 reg=0.0419 nce_clip=0.0221 nce_txt=0.0060\n",
      "epoch 10 step 400/4641 loss=0.0475 reg=0.0425 nce_clip=0.0223 nce_txt=0.0062\n",
      "epoch 10 step 600/4641 loss=0.0470 reg=0.0420 nce_clip=0.0220 nce_txt=0.0060\n",
      "epoch 10 step 800/4641 loss=0.0471 reg=0.0420 nce_clip=0.0221 nce_txt=0.0064\n",
      "epoch 10 step 1000/4641 loss=0.0472 reg=0.0421 nce_clip=0.0221 nce_txt=0.0064\n",
      "epoch 10 step 1200/4641 loss=0.0470 reg=0.0421 nce_clip=0.0216 nce_txt=0.0062\n",
      "epoch 10 step 1400/4641 loss=0.0469 reg=0.0420 nce_clip=0.0215 nce_txt=0.0062\n",
      "epoch 10 step 1600/4641 loss=0.0465 reg=0.0416 nce_clip=0.0213 nce_txt=0.0062\n",
      "epoch 10 step 1800/4641 loss=0.0464 reg=0.0415 nce_clip=0.0212 nce_txt=0.0062\n",
      "epoch 10 step 2000/4641 loss=0.0460 reg=0.0412 nce_clip=0.0212 nce_txt=0.0061\n",
      "epoch 10 step 2200/4641 loss=0.0459 reg=0.0411 nce_clip=0.0210 nce_txt=0.0061\n",
      "epoch 10 step 2400/4641 loss=0.0458 reg=0.0409 nce_clip=0.0211 nce_txt=0.0061\n",
      "epoch 10 step 2600/4641 loss=0.0456 reg=0.0407 nce_clip=0.0213 nce_txt=0.0061\n",
      "epoch 10 step 2800/4641 loss=0.0455 reg=0.0406 nce_clip=0.0213 nce_txt=0.0060\n",
      "epoch 10 step 3000/4641 loss=0.0454 reg=0.0406 nce_clip=0.0210 nce_txt=0.0060\n",
      "epoch 10 step 3200/4641 loss=0.0451 reg=0.0403 nce_clip=0.0209 nce_txt=0.0060\n",
      "epoch 10 step 3400/4641 loss=0.0450 reg=0.0402 nce_clip=0.0210 nce_txt=0.0061\n",
      "epoch 10 step 3600/4641 loss=0.0449 reg=0.0401 nce_clip=0.0209 nce_txt=0.0061\n",
      "epoch 10 step 3800/4641 loss=0.0449 reg=0.0400 nce_clip=0.0210 nce_txt=0.0061\n",
      "epoch 10 step 4000/4641 loss=0.0447 reg=0.0399 nce_clip=0.0209 nce_txt=0.0061\n",
      "epoch 10 step 4200/4641 loss=0.0447 reg=0.0399 nce_clip=0.0208 nce_txt=0.0060\n",
      "epoch 10 step 4400/4641 loss=0.0446 reg=0.0398 nce_clip=0.0208 nce_txt=0.0060\n",
      "epoch 10 step 4600/4641 loss=0.0446 reg=0.0398 nce_clip=0.0208 nce_txt=0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10: VAL SMAPE = 43.538%\n",
      "⏸️ No improvement. Patience 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 step 200/4641 loss=0.0374 reg=0.0335 nce_clip=0.0168 nce_txt=0.0050\n",
      "epoch 11 step 400/4641 loss=0.0382 reg=0.0343 nce_clip=0.0171 nce_txt=0.0053\n",
      "epoch 11 step 600/4641 loss=0.0376 reg=0.0337 nce_clip=0.0171 nce_txt=0.0051\n",
      "epoch 11 step 800/4641 loss=0.0374 reg=0.0334 nce_clip=0.0176 nce_txt=0.0051\n",
      "epoch 11 step 1000/4641 loss=0.0368 reg=0.0328 nce_clip=0.0176 nce_txt=0.0052\n",
      "epoch 11 step 1200/4641 loss=0.0370 reg=0.0329 nce_clip=0.0178 nce_txt=0.0052\n",
      "epoch 11 step 1400/4641 loss=0.0369 reg=0.0329 nce_clip=0.0177 nce_txt=0.0052\n",
      "epoch 11 step 1600/4641 loss=0.0368 reg=0.0328 nce_clip=0.0177 nce_txt=0.0053\n",
      "epoch 11 step 1800/4641 loss=0.0364 reg=0.0324 nce_clip=0.0174 nce_txt=0.0052\n",
      "epoch 11 step 2000/4641 loss=0.0362 reg=0.0322 nce_clip=0.0175 nce_txt=0.0053\n",
      "epoch 11 step 2200/4641 loss=0.0359 reg=0.0319 nce_clip=0.0175 nce_txt=0.0053\n",
      "epoch 11 step 2400/4641 loss=0.0357 reg=0.0317 nce_clip=0.0174 nce_txt=0.0052\n",
      "epoch 11 step 2600/4641 loss=0.0356 reg=0.0316 nce_clip=0.0173 nce_txt=0.0052\n",
      "epoch 11 step 2800/4641 loss=0.0354 reg=0.0315 nce_clip=0.0173 nce_txt=0.0052\n",
      "epoch 11 step 3000/4641 loss=0.0354 reg=0.0314 nce_clip=0.0172 nce_txt=0.0052\n",
      "epoch 11 step 3200/4641 loss=0.0351 reg=0.0312 nce_clip=0.0171 nce_txt=0.0052\n",
      "epoch 11 step 3400/4641 loss=0.0350 reg=0.0311 nce_clip=0.0170 nce_txt=0.0051\n",
      "epoch 11 step 3600/4641 loss=0.0348 reg=0.0309 nce_clip=0.0170 nce_txt=0.0051\n",
      "epoch 11 step 3800/4641 loss=0.0347 reg=0.0308 nce_clip=0.0169 nce_txt=0.0051\n",
      "epoch 11 step 4000/4641 loss=0.0347 reg=0.0308 nce_clip=0.0169 nce_txt=0.0051\n",
      "epoch 11 step 4200/4641 loss=0.0344 reg=0.0305 nce_clip=0.0170 nce_txt=0.0051\n",
      "epoch 11 step 4400/4641 loss=0.0343 reg=0.0304 nce_clip=0.0169 nce_txt=0.0051\n",
      "epoch 11 step 4600/4641 loss=0.0341 reg=0.0303 nce_clip=0.0168 nce_txt=0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 11: VAL SMAPE = 43.387%\n",
      "⏸️ No improvement. Patience 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 step 200/4641 loss=0.0274 reg=0.0232 nce_clip=0.0180 nce_txt=0.0053\n",
      "epoch 12 step 400/4641 loss=0.0277 reg=0.0239 nce_clip=0.0166 nce_txt=0.0052\n",
      "epoch 12 step 600/4641 loss=0.0275 reg=0.0238 nce_clip=0.0157 nce_txt=0.0052\n",
      "epoch 12 step 800/4641 loss=0.0271 reg=0.0234 nce_clip=0.0156 nce_txt=0.0052\n",
      "epoch 12 step 1000/4641 loss=0.0270 reg=0.0234 nce_clip=0.0154 nce_txt=0.0051\n",
      "epoch 12 step 1200/4641 loss=0.0268 reg=0.0232 nce_clip=0.0153 nce_txt=0.0051\n",
      "epoch 12 step 1400/4641 loss=0.0266 reg=0.0231 nce_clip=0.0151 nce_txt=0.0051\n",
      "epoch 12 step 1600/4641 loss=0.0267 reg=0.0232 nce_clip=0.0149 nce_txt=0.0050\n",
      "epoch 12 step 1800/4641 loss=0.0267 reg=0.0232 nce_clip=0.0149 nce_txt=0.0049\n",
      "epoch 12 step 2000/4641 loss=0.0266 reg=0.0231 nce_clip=0.0150 nce_txt=0.0049\n",
      "epoch 12 step 2200/4641 loss=0.0267 reg=0.0232 nce_clip=0.0148 nce_txt=0.0048\n",
      "epoch 12 step 2400/4641 loss=0.0265 reg=0.0231 nce_clip=0.0147 nce_txt=0.0049\n",
      "epoch 12 step 2600/4641 loss=0.0264 reg=0.0230 nce_clip=0.0146 nce_txt=0.0049\n",
      "epoch 12 step 2800/4641 loss=0.0264 reg=0.0230 nce_clip=0.0145 nce_txt=0.0048\n",
      "epoch 12 step 3000/4641 loss=0.0263 reg=0.0229 nce_clip=0.0145 nce_txt=0.0048\n",
      "epoch 12 step 3200/4641 loss=0.0261 reg=0.0228 nce_clip=0.0144 nce_txt=0.0048\n",
      "epoch 12 step 3400/4641 loss=0.0261 reg=0.0227 nce_clip=0.0143 nce_txt=0.0048\n",
      "epoch 12 step 3600/4641 loss=0.0260 reg=0.0226 nce_clip=0.0142 nce_txt=0.0048\n",
      "epoch 12 step 3800/4641 loss=0.0258 reg=0.0225 nce_clip=0.0142 nce_txt=0.0048\n",
      "epoch 12 step 4000/4641 loss=0.0258 reg=0.0225 nce_clip=0.0141 nce_txt=0.0047\n",
      "epoch 12 step 4200/4641 loss=0.0258 reg=0.0225 nce_clip=0.0141 nce_txt=0.0047\n",
      "epoch 12 step 4400/4641 loss=0.0257 reg=0.0224 nce_clip=0.0141 nce_txt=0.0047\n",
      "epoch 12 step 4600/4641 loss=0.0256 reg=0.0223 nce_clip=0.0141 nce_txt=0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 12: VAL SMAPE = 42.732%\n",
      "💾 Saved new best to price_clip+distil_fusionv2/best_clip+distil.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 step 200/4641 loss=0.0199 reg=0.0170 nce_clip=0.0125 nce_txt=0.0043\n"
     ]
    }
   ],
   "source": [
    "# =========================== Train Loop ===========================\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    run_loss = run_reg = run_nce_clip = run_nce_txt = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        # Move to device\n",
    "        batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k,v in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            pred_log2, z_img, z_txt, z_d1, z_d2 = model(\n",
    "                clip_input_ids=batch[\"clip_input_ids\"],\n",
    "                clip_attention_mask=batch[\"clip_attention_mask\"],\n",
    "                pixel_values=batch.get(\"pixel_values\", None),\n",
    "                img_missing=batch[\"img_missing\"],\n",
    "                distil_ids1=batch[\"distil_ids1\"],\n",
    "                distil_att1=batch[\"distil_att1\"],\n",
    "                distil_ids2=batch[\"distil_ids2\"],\n",
    "                distil_att2=batch[\"distil_att2\"],\n",
    "            )\n",
    "\n",
    "            loss_reg  = huber_loss(pred_log2, batch[\"target\"], delta=HUBER_DELTA)\n",
    "            # Only compute CLIP NCE if we actually used images and have at least 2 valid\n",
    "            loss_nce_clip = torch.tensor(0.0, device=device)\n",
    "            valid_idx = (batch[\"img_missing\"] == 0).nonzero(as_tuple=False).squeeze(-1)\n",
    "            if (\"pixel_values\" in batch) and (IMG_MISSING_POLICY != \"text_only\") and valid_idx.numel() > 1:\n",
    "                loss_nce_clip = info_nce(z_img[valid_idx], z_txt[valid_idx], tau=TAU)\n",
    "\n",
    "            # Distil SimCSE-style (two views)\n",
    "            loss_nce_txt = info_nce(z_d1, z_d2, tau=TAU)\n",
    "\n",
    "            loss = loss_reg + ALPHA_CLIP_NCE*loss_nce_clip + ALPHA_TXT_NCE*loss_nce_txt\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % GRAD_ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        run_loss     += float(loss.item())\n",
    "        run_reg      += float(loss_reg.item())\n",
    "        run_nce_clip += float(loss_nce_clip.item())\n",
    "        run_nce_txt  += float(loss_nce_txt.item())\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            denom = step if step > 0 else 1\n",
    "            print(f\"epoch {epoch} step {step}/{len(train_loader)} \"\n",
    "                  f\"loss={run_loss/denom:.4f} reg={run_reg/denom:.4f} \"\n",
    "                  f\"nce_clip={run_nce_clip/denom:.4f} nce_txt={run_nce_txt/denom:.4f}\")\n",
    "\n",
    "    # ------------------ Validation ------------------\n",
    "    model.eval()\n",
    "    preds_log = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k,v in batch.items()}\n",
    "            pred_log2, *_ = model(\n",
    "                clip_input_ids=batch[\"clip_input_ids\"],\n",
    "                clip_attention_mask=batch[\"clip_attention_mask\"],\n",
    "                pixel_values=batch.get(\"pixel_values\", None),\n",
    "                img_missing=batch[\"img_missing\"],\n",
    "                distil_ids1=batch[\"distil_ids1\"],\n",
    "                distil_att1=batch[\"distil_att1\"],\n",
    "                distil_ids2=batch[\"distil_ids2\"],\n",
    "                distil_att2=batch[\"distil_att2\"],\n",
    "            )\n",
    "            preds_log.append(pred_log2.detach().float().cpu().numpy())\n",
    "\n",
    "    preds_log = np.concatenate(preds_log, axis=0) if len(preds_log) else np.array([])\n",
    "    if len(preds_log):\n",
    "        va_preds = delog2(preds_log)\n",
    "        va_true  = delog2(y_va_log)\n",
    "        smape = smape_np(va_true, va_preds)\n",
    "    else:\n",
    "        smape = float(\"inf\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch}: VAL SMAPE = {smape:.3f}%\")\n",
    "\n",
    "    # Save best\n",
    "    if smape < best_smape - 1e-6:\n",
    "        best_smape = smape\n",
    "        patience = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"fusion_state\": model.state_dict(),\n",
    "                \"clip_id\": CLIP_ID,\n",
    "                \"distil_id_used\": model.distil_id_used,\n",
    "                \"config\": {\n",
    "                    \"ALPHA_CLIP_NCE\": ALPHA_CLIP_NCE,\n",
    "                    \"ALPHA_TXT_NCE\": ALPHA_TXT_NCE,\n",
    "                    \"TAU\": TAU,\n",
    "                    \"HUBER_DELTA\": HUBER_DELTA,\n",
    "                    \"MAX_LEN_CLIP\": MAX_LEN_CLIP,\n",
    "                    \"MAX_LEN_DISTIL\": MAX_LEN_DISTIL,\n",
    "                    \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "                    \"clip_projection_dim\": model.clip.config.projection_dim,\n",
    "                },\n",
    "                \"columns\": {\"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "                \"val_frac\": VAL_FRAC,\n",
    "            },\n",
    "            best_path\n",
    "        )\n",
    "        print(f\"💾 Saved new best to {best_path}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"⏸️ No improvement. Patience {patience}/{EARLY_STOP_ROUNDS}\")\n",
    "        if patience >= EARLY_STOP_ROUNDS:\n",
    "            print(\"🛑 Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f\"🏁 Best VAL SMAPE: {best_smape:.3f}% | Checkpoint: {best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90bb00e-adb1-4128-9874-d4bc5f0344e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save run metadata\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_fusion.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"best_val_smape\": float(best_smape),\n",
    "        \"train_missing_images\": int(train_ds.missing_img_count),\n",
    "        \"valid_missing_images\": int(val_ds.missing_img_count),\n",
    "        \"dropped_train\": int(getattr(train_ds, \"dropped_missing\", 0)),\n",
    "        \"dropped_valid\": int(getattr(val_ds, \"dropped_missing\", 0)),\n",
    "        \"missing_policy\": IMG_MISSING_POLICY,\n",
    "        \"val_frac\": VAL_FRAC,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"clip_id\": CLIP_ID,\n",
    "        \"distil_id_requested\": DISTIL_ID_RAW,\n",
    "        \"distil_id_used\": model.distil_id_used,\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e587a1-4031-4089-b75b-9edf2dd28190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(42.077175012917564)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d605d8d2-972d-4e81-986b-c85c7ad10fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e41469b63fb421b9a5a82d4b49ac4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions: price_clip+distil_fusionv2/predictions_amritha.csv\n",
      "🖼️ Missing/failed image loads: 0\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl, total=len(dl)):\n",
    "        batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in batch.items()}\n",
    "        out = model(\n",
    "            clip_input_ids=batch[\"clip_input_ids\"],\n",
    "            clip_attention_mask=batch[\"clip_attention_mask\"],\n",
    "            pixel_values=batch.get(\"pixel_values\", None),\n",
    "            img_missing=batch[\"img_missing\"],\n",
    "            distil_ids1=batch[\"distil_ids1\"], distil_att1=batch[\"distil_att1\"],\n",
    "            distil_ids2=batch[\"distil_ids2\"], distil_att2=batch[\"distil_att2\"],\n",
    "        )\n",
    "\n",
    "        # ✅ Handle tuple/list outputs defensively\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            # pick the first tensor-like item\n",
    "            pred_log2 = next((t for t in out if torch.is_tensor(t)), out[0])\n",
    "        else:\n",
    "            pred_log2 = out\n",
    "\n",
    "        pred_log2 = pred_log2.squeeze(-1)  # (B,) if it was (B,1)\n",
    "        all_preds.append(pred_log2.detach().float().cpu().numpy())\n",
    "\n",
    "# stack + de-log2\n",
    "preds_log = np.concatenate(all_preds, axis=0) if len(all_preds) else np.zeros((len(dft),), dtype=np.float32)\n",
    "price_pred = delog2(preds_log).astype(np.float64)\n",
    "\n",
    "# optional: guard against infs/NAs\n",
    "price_pred = np.nan_to_num(price_pred, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "pd.DataFrame({ID_COL: dft[ID_COL].values, \"price_pred\": price_pred}).to_csv(PRED_OUT, index=False)\n",
    "print(f\"✅ Saved predictions: {PRED_OUT}\")\n",
    "print(f\"🖼️ Missing/failed image loads: {infer_ds.missing_img_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038fa5d-1f70-4735-91f1-d3b22443fb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
