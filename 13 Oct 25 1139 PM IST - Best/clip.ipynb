{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2bff2f-6f59-4a73-8978-025e2e189620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open-clip-torch in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: torch>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from open-clip-torch) (2.7.0)\n",
      "Requirement already satisfied: torchvision in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from open-clip-torch) (0.22.0)\n",
      "Requirement already satisfied: regex in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from open-clip-torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from open-clip-torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from open-clip-torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from open-clip-torch) (0.31.2)\n",
      "Requirement already satisfied: safetensors in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from open-clip-torch) (0.5.3)\n",
      "Requirement already satisfied: timm>=1.0.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from open-clip-torch) (1.0.20)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from timm>=1.0.17->open-clip-torch) (6.0.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=2.0->open-clip-torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0->open-clip-torch) (75.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0->open-clip-torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from ftfy->open-clip-torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub->open-clip-torch) (25.0)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2->torch>=2.0->open-clip-torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->huggingface-hub->open-clip-torch) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->huggingface-hub->open-clip-torch) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->huggingface-hub->open-clip-torch) (2025.1.31)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torchvision->open-clip-torch) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torchvision->open-clip-torch) (11.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038d9a75-bef1-4f36-9ee5-b14a93275c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "\n",
    "df = pd.read_csv(\"jl_fs/train.csv\")\n",
    "df[\"image_path\"] = df[\"sample_id\"].apply(lambda x : f\"jl_fs/images/train/{x}.jpg\")\n",
    "df.to_csv(\"train_updated.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e410691d-1274-468f-b696-9a2b940af71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "      <td>jl_fs/images/train/33127.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "      <td>jl_fs/images/train/198967.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "      <td>jl_fs/images/train/261251.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "      <td>jl_fs/images/train/55858.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "      <td>jl_fs/images/train/292686.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  \\\n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89   \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12   \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97   \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34   \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49   \n",
       "\n",
       "                      image_path  \n",
       "0   jl_fs/images/train/33127.jpg  \n",
       "1  jl_fs/images/train/198967.jpg  \n",
       "2  jl_fs/images/train/261251.jpg  \n",
       "3   jl_fs/images/train/55858.jpg  \n",
       "4  jl_fs/images/train/292686.jpg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f28a7-b4b2-4a66-b931-af8f61366260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading CSV: train_updated.csv\n",
      "📊 Split: train=74250 | valid=750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️ Device: cuda\n",
      "🧮 Trainable params clip=427,944,193 | head=2,362,369\n",
      "🔎 Warmup batch to tally missing images…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1036/838372350.py:279: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing images counted (train/val): 0/0\n",
      "🗑️ Dropped due to policy=drop (train/val): 0/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_1036/838372350.py:308: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 200/4641 loss=2.4999 reg=3.2356 con=0.2928\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    CLIPModel,\n",
    "    AutoProcessor,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "CSV_PATH        = os.environ.get(\"TRAIN_CSV\", \"train_updated.csv\")   # must contain text + price + image path\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "PRICE_COL       = os.environ.get(\"PRICE_COL\", \"price\")\n",
    "IMG_COL         = os.environ.get(\"IMG_COL\",  \"image_path\")         # <-- new: local jpg path column\n",
    "\n",
    "MODEL_ID        = os.environ.get(\"MODEL_ID\", \"openai/clip-vit-large-patch14-336\")\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_clip_vit_99_1_split\")\n",
    "\n",
    "SEED            = int(os.environ.get(\"SEED\", \"42\"))\n",
    "VAL_FRAC        = float(os.environ.get(\"VAL_FRAC\", \"0.01\"))         # set 0.5 for 50/50\n",
    "MAX_LEN         = int(os.environ.get(\"MAX_LEN\", \"64\"))             # CLIP text context is shorter\n",
    "BATCH_SIZE      = int(os.environ.get(\"BATCH_SIZE\", \"16\"))\n",
    "LR              = float(os.environ.get(\"LR\", \"2e-5\"))\n",
    "WEIGHT_DECAY    = float(os.environ.get(\"WEIGHT_DECAY\", \"0.01\"))\n",
    "EPOCHS          = int(os.environ.get(\"EPOCHS\", \"10\"))\n",
    "WARMUP_RATIO    = float(os.environ.get(\"WARMUP_RATIO\", \"0.06\"))\n",
    "GRAD_ACCUM      = int(os.environ.get(\"GRAD_ACCUM\", \"2\"))\n",
    "MAX_GRAD_NORM   = float(os.environ.get(\"MAX_GRAD_NORM\", \"1.0\"))\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "ALPHA_CONTRAST  = float(os.environ.get(\"ALPHA_CONTRAST\", \"0.25\"))  # weight for contrastive loss\n",
    "TAU             = float(os.environ.get(\"TAU\", \"0.07\"))             # temperature\n",
    "HUBER_DELTA     = float(os.environ.get(\"HUBER_DELTA\", \"1.0\"))\n",
    "\n",
    "EARLY_STOP_ROUNDS = int(os.environ.get(\"EARLY_STOP_ROUNDS\", \"3\"))\n",
    "MIN_PRICE       = float(os.environ.get(\"MIN_PRICE\", \"1e-6\"))\n",
    "\n",
    "IMG_MISSING_POLICY = os.environ.get(\"IMG_MISSING_POLICY\", \"zero\").lower()  # zero | text_only | drop\n",
    "assert IMG_MISSING_POLICY in {\"zero\", \"text_only\", \"drop\"}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------- Utils ---------------------------\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def log2_price(p: np.ndarray) -> np.ndarray:\n",
    "    return np.log2(np.clip(p, MIN_PRICE, None))\n",
    "\n",
    "def delog2(x: np.ndarray) -> np.ndarray:\n",
    "    return np.power(2.0, x)\n",
    "\n",
    "def split_train_val(df: pd.DataFrame, frac_val: float = VAL_FRAC, seed: int = SEED):\n",
    "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    n_val = int(len(df) * frac_val)\n",
    "    df_val = df.iloc[:n_val].reset_index(drop=True)\n",
    "    df_tr  = df.iloc[n_val:].reset_index(drop=True)\n",
    "    return df_tr, df_val\n",
    "\n",
    "# --------------------------- Dataset & Collate ---------------------------\n",
    "class ClipPriceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Policies:\n",
    "      - 'zero':      returns dummy pixel_values for missing images; vision forward done; features zeroed later.\n",
    "      - 'text_only': returns pixel_values=None for missing images; vision forward skipped; text features only.\n",
    "      - 'drop':      rows with missing images removed at dataset build time.\n",
    "    Yields dict with: input_ids, attention_mask, (pixel_values), img_missing, (target)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, text_col: str, img_col: str, prices_log2: Optional[np.ndarray],\n",
    "                 processor: AutoProcessor, max_len: int, policy: str):\n",
    "        self.processor = processor\n",
    "        self.max_len = max_len\n",
    "        self.policy = policy\n",
    "\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "        if policy == \"drop\":\n",
    "            before = len(df)\n",
    "            df = df[df[img_col].apply(lambda p: isinstance(p, str) and len(p) > 0 and os.path.exists(p))]\n",
    "            self.dropped_missing = before - len(df)\n",
    "        else:\n",
    "            self.dropped_missing = 0\n",
    "\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.img_paths = df[img_col].fillna(\"\").astype(str).tolist()\n",
    "        self.prices_log2 = prices_log2\n",
    "        self.missing_img_count = 0\n",
    "\n",
    "        # Dummy pixel for consistent shapes (use processor to derive size)\n",
    "        dummy = self.processor(images=Image.new(\"RGB\", (224, 224)), return_tensors=\"pt\")\n",
    "        self._dummy_pixel = dummy[\"pixel_values\"].squeeze(0)  # (3,H,W)\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def _load_image(self, path: str):\n",
    "        if isinstance(path, str) and path and os.path.exists(path):\n",
    "            try:\n",
    "                return Image.open(path).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.missing_img_count += 1\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        img  = self._load_image(self.img_paths[idx])\n",
    "\n",
    "        enc_text = self.processor(text=[text], padding=False, truncation=True,\n",
    "                                  max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        img_missing = 0\n",
    "        pixel_values = None\n",
    "\n",
    "        if img is None:\n",
    "            img_missing = 1\n",
    "            if self.policy == \"zero\":\n",
    "                pixel_values = self._dummy_pixel.clone()\n",
    "            elif self.policy == \"text_only\":\n",
    "                pixel_values = None\n",
    "        else:\n",
    "            enc_img = self.processor(images=img, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": enc_text[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc_text[\"attention_mask\"].squeeze(0),\n",
    "            \"img_missing\": torch.tensor(img_missing, dtype=torch.uint8),\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            item[\"pixel_values\"] = pixel_values\n",
    "        if self.prices_log2 is not None:\n",
    "            item[\"target\"] = torch.tensor(self.prices_log2[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "@dataclass\n",
    "class CollateClip:\n",
    "    processor: AutoProcessor\n",
    "    def __call__(self, batch):\n",
    "        # pad text\n",
    "        input_ids = [b[\"input_ids\"] for b in batch]\n",
    "        attention = [b[\"attention_mask\"] for b in batch]\n",
    "        text_padded = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention},\n",
    "            padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        # images: some may be absent (text_only policy)\n",
    "        has_pix = [(\"pixel_values\" in b) for b in batch]\n",
    "        pixel_values = None\n",
    "        if any(has_pix):\n",
    "            shapes = [b[\"pixel_values\"].shape for b in batch if \"pixel_values\" in b]\n",
    "            C,H,W = shapes[0]\n",
    "            stacked = []\n",
    "            for b in batch:\n",
    "                if \"pixel_values\" in b:\n",
    "                    stacked.append(b[\"pixel_values\"])\n",
    "                else:\n",
    "                    stacked.append(torch.zeros((C,H,W), dtype=torch.float32))\n",
    "            pixel_values = torch.stack(stacked, dim=0)\n",
    "\n",
    "        res = {\n",
    "            \"input_ids\": text_padded[\"input_ids\"],\n",
    "            \"attention_mask\": text_padded[\"attention_mask\"],\n",
    "            \"img_missing\": torch.stack([b[\"img_missing\"] for b in batch], dim=0),\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            res[\"pixel_values\"] = pixel_values\n",
    "        if \"target\" in batch[0]:\n",
    "            res[\"target\"] = torch.stack([b[\"target\"] for b in batch], dim=0)\n",
    "        return res\n",
    "\n",
    "# --------------------------- Model & Loss ---------------------------\n",
    "class ClipRegressionHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        in_dim = 2 * embed_dim  # concat image+text\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_dim, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "def info_nce(z_img: torch.Tensor, z_txt: torch.Tensor, tau: float = TAU) -> torch.Tensor:\n",
    "    z_img = F.normalize(z_img, dim=-1)\n",
    "    z_txt = F.normalize(z_txt, dim=-1)\n",
    "    logits = torch.matmul(z_img, z_txt.t()) / tau  # (B,B)\n",
    "    labels = torch.arange(z_img.size(0), device=z_img.device)\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.t(), labels)\n",
    "    return 0.5 * (loss_i + loss_t)\n",
    "\n",
    "def huber_loss(pred, target, delta=HUBER_DELTA):\n",
    "    return F.huber_loss(pred, target, delta=delta)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# --------------------------- Load data ---------------------------\n",
    "print(f\"🔧 Loading CSV: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "for col, name in [(TEXT_COL, \"TEXT_COL\"), (PRICE_COL, \"PRICE_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"{name} '{col}' not in CSV columns={df.columns.tolist()}\")\n",
    "\n",
    "# clean / guard\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "df = df.loc[pd.to_numeric(df[PRICE_COL], errors=\"coerce\").notnull()].copy()\n",
    "df[PRICE_COL] = df[PRICE_COL].astype(float)\n",
    "df = df.loc[df[PRICE_COL] >= 0.0].reset_index(drop=True)\n",
    "\n",
    "# split\n",
    "df_tr, df_va = split_train_val(df, frac_val=VAL_FRAC, seed=SEED)\n",
    "print(f\"📊 Split: train={len(df_tr)} | valid={len(df_va)}\")\n",
    "\n",
    "y_tr_log = log2_price(df_tr[PRICE_COL].values)\n",
    "y_va_log = log2_price(df_va[PRICE_COL].values)\n",
    "\n",
    "# --------------------------- CLIP backbone ---------------------------\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "\n",
    "train_ds = ClipPriceDataset(df_tr, TEXT_COL, IMG_COL, y_tr_log, processor, MAX_LEN, IMG_MISSING_POLICY)\n",
    "val_ds   = ClipPriceDataset(df_va, TEXT_COL, IMG_COL, y_va_log, processor, MAX_LEN, IMG_MISSING_POLICY)\n",
    "collate  = CollateClip(processor)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device)\n",
    "price_head = ClipRegressionHead(embed_dim=clip_model.config.projection_dim, dropout=0.1).to(device)\n",
    "\n",
    "print(f\"🖥️ Device: {device}\")\n",
    "print(f\"🧮 Trainable params clip={count_parameters(clip_model):,} | head={count_parameters(price_head):,}\")\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = True  # end-to-end; set False to freeze\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "# Optimizer & scheduler\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "params = list(clip_model.named_parameters()) + [(f\"head.{n}\", p) for n, p in price_head.named_parameters()]\n",
    "grouped = [\n",
    "    {\"params\": [p for n, p in params if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": [p for n, p in params if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(grouped, lr=LR)\n",
    "num_training_steps = EPOCHS * max(1, math.ceil(len(train_loader) / max(1, GRAD_ACCUM)))\n",
    "num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "# --------------------------- Train ---------------------------\n",
    "best_smape = float(\"inf\")\n",
    "best_path = os.path.join(OUTPUT_DIR, \"best_clip.pt\")\n",
    "patience = 0\n",
    "\n",
    "print(\"🔎 Warmup batch to tally missing images…\")\n",
    "if len(train_loader) > 0:\n",
    "    _ = next(iter(train_loader))\n",
    "if len(val_loader) > 0:\n",
    "    _ = next(iter(val_loader))\n",
    "print(f\"⚠️ Missing images counted (train/val): {train_ds.missing_img_count}/{val_ds.missing_img_count}\")\n",
    "print(f\"🗑️ Dropped due to policy=drop (train/val): {getattr(train_ds,'dropped_missing',0)}/{getattr(val_ds,'dropped_missing',0)}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    clip_model.train(); price_head.train()\n",
    "    train_loss_running = 0.0\n",
    "    reg_loss_running = 0.0\n",
    "    con_loss_running = 0.0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        targets        = batch[\"target\"].to(device, non_blocking=True).float()\n",
    "        img_missing    = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            # Text features\n",
    "            txt_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Image features (guarded by policy/batch content)\n",
    "            do_vision = (\"pixel_values\" in batch) and (IMG_MISSING_POLICY != \"text_only\")\n",
    "            if do_vision:\n",
    "                pixel_values = batch[\"pixel_values\"].to(\n",
    "                    device,\n",
    "                    dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "                    non_blocking=True\n",
    "                )\n",
    "                img_feat = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "                if img_missing.any():\n",
    "                    img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "            else:\n",
    "                img_feat = torch.zeros_like(txt_feat)\n",
    "\n",
    "            # Normalize and fuse\n",
    "            txt_n = F.normalize(txt_feat, dim=-1)\n",
    "            img_n = F.normalize(img_feat, dim=-1)\n",
    "            fused = torch.cat([img_n, txt_n], dim=-1)\n",
    "\n",
    "            # Losses\n",
    "            pred_log = price_head(fused)\n",
    "            reg_loss = huber_loss(pred_log, targets, delta=HUBER_DELTA)\n",
    "\n",
    "            con_loss = torch.tensor(0.0, device=device, dtype=txt_n.dtype)\n",
    "            valid_idx = (img_missing == 0).nonzero(as_tuple=False).squeeze(-1)\n",
    "            if do_vision and valid_idx.numel() > 1:\n",
    "                con_loss = info_nce(img_n[valid_idx], txt_n[valid_idx], tau=TAU)\n",
    "\n",
    "            loss = (1.0 - ALPHA_CONTRAST) * reg_loss + ALPHA_CONTRAST * con_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % GRAD_ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(list(clip_model.parameters()) + list(price_head.parameters()), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss_running += float(loss.item())\n",
    "        reg_loss_running   += float(reg_loss.item())\n",
    "        con_loss_running   += float(con_loss.item()) if isinstance(con_loss, torch.Tensor) else float(con_loss)\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"epoch {epoch} step {step}/{len(train_loader)} \"\n",
    "                  f\"loss={train_loss_running/step:.4f} reg={reg_loss_running/step:.4f} con={con_loss_running/step:.4f}\")\n",
    "\n",
    "    # ------------------ Validation ------------------\n",
    "    clip_model.eval(); price_head.eval()\n",
    "    preds_log = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            img_missing    = batch[\"img_missing\"].to(device)\n",
    "\n",
    "            txt_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            do_vision = (\"pixel_values\" in batch) and (IMG_MISSING_POLICY != \"text_only\")\n",
    "            if do_vision:\n",
    "                pixel_values = batch[\"pixel_values\"].to(\n",
    "                    device,\n",
    "                    dtype=next(clip_model.vision_model.parameters()).dtype,\n",
    "                    non_blocking=True\n",
    "                )\n",
    "                img_feat = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "                if img_missing.any():\n",
    "                    img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "            else:\n",
    "                img_feat = torch.zeros_like(txt_feat)\n",
    "\n",
    "            txt_n = F.normalize(txt_feat, dim=-1)\n",
    "            img_n = F.normalize(img_feat, dim=-1)\n",
    "            fused = torch.cat([img_n, txt_n], dim=-1)\n",
    "            pred  = price_head(fused)\n",
    "            preds_log.append(pred.detach().float().cpu().numpy())\n",
    "\n",
    "    preds_log = np.concatenate(preds_log, axis=0) if len(preds_log) else np.array([])\n",
    "    if len(preds_log):\n",
    "        va_preds  = delog2(preds_log)\n",
    "        va_true   = delog2(y_va_log)\n",
    "        smape = smape_np(va_true, va_preds)\n",
    "    else:\n",
    "        smape = float(\"inf\")\n",
    "    print(f\"✅ Epoch {epoch}: VAL SMAPE = {smape:.3f}% | missing_imgs (train/val) = {train_ds.missing_img_count}/{val_ds.missing_img_count}\")\n",
    "\n",
    "    # Save best\n",
    "    if smape < best_smape - 1e-6:\n",
    "        best_smape = smape\n",
    "        patience = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"clip_state\": clip_model.state_dict(),\n",
    "                \"head_state\": price_head.state_dict(),\n",
    "                \"model_id\": MODEL_ID,\n",
    "                \"config\": {\n",
    "                    \"ALPHA_CONTRAST\": ALPHA_CONTRAST,\n",
    "                    \"TAU\": TAU,\n",
    "                    \"MAX_LEN\": MAX_LEN,\n",
    "                    \"projection_dim\": clip_model.config.projection_dim,\n",
    "                    \"IMG_MISSING_POLICY\": IMG_MISSING_POLICY,\n",
    "                },\n",
    "                \"columns\": {\"text\": TEXT_COL, \"image\": IMG_COL, \"price\": PRICE_COL},\n",
    "                \"val_frac\": VAL_FRAC,\n",
    "            },\n",
    "            best_path\n",
    "        )\n",
    "        print(f\"💾 Saved new best to {best_path}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"⏸️ No improvement. Patience {patience}/{EARLY_STOP_ROUNDS}\")\n",
    "        if patience >= EARLY_STOP_ROUNDS:\n",
    "            print(\"🛑 Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f\"🏁 Best VAL SMAPE: {best_smape:.3f}% | Checkpoint: {best_path}\")\n",
    "\n",
    "# Save final artifacts\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_clip.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"best_val_smape\": float(best_smape),\n",
    "        \"train_missing_images\": int(train_ds.missing_img_count),\n",
    "        \"valid_missing_images\": int(val_ds.missing_img_count),\n",
    "        \"dropped_train\": int(getattr(train_ds, \"dropped_missing\", 0)),\n",
    "        \"dropped_valid\": int(getattr(val_ds, \"dropped_missing\", 0)),\n",
    "        \"missing_policy\": IMG_MISSING_POLICY,\n",
    "        \"val_frac\": VAL_FRAC,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY\n",
    "    }, f, indent=2)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4e286-ff22-4ebd-b2fd-579fbd5f7300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9acfb9-d5ca-448a-824e-698f4bab6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('gelllo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1eb755-a097-47c0-9d37-b28cbcb0afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('melllo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be428d2-bb29-4bbd-be97-c774e5fc9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('melllo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d78be0-f152-48aa-ae5c-1e63662cf563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jello\n"
     ]
    }
   ],
   "source": [
    "print('jello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8338972d-8fa2-4852-9797-bd38f99c9dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(42.69014756236535)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b9fc540-f0a9-4b73-94ad-b973ee28b034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'price_clip_vit_99_1_split/best_clip.pt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17feae1b-f94e-45be-8642-bada5f28dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "\n",
    "df = pd.read_csv(\"jl_fs/test.csv\")\n",
    "df[\"image_path\"] = df[\"sample_id\"].apply(lambda x : f\"jl_fs/images/train/{x}.jpg\")\n",
    "df.to_csv(\"test_updated.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784b5cad-3123-4cca-b910-3e28f0a98b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loaded checkpoint from: price_clip_vit_99_1_split/best_clip.pt\n",
      "🔤 MODEL_ID=openai/clip-vit-large-patch14-336 | projection_dim=768 | IMG_MISSING_POLICY=zero | MAX_LEN=64\n",
      "🖥 Device: cuda\n",
      "🧪 Test rows: 75000 | Missing images encountered (during getitem): 0\n",
      "🗑 Dropped due to policy=drop: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a555f7717f7b42329e94e225f9c45bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_1036/2017285176.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# --- Inference: load best checkpoint and predict on TEST_CSV ---\n",
    "\n",
    "# %%\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "\n",
    "# ---- Config / paths ----\n",
    "TEST_CSV        = os.environ.get(\"TEST_CSV\", \"test_updated.csv\")   # must contain ID + text + image path\n",
    "ID_COL          = os.environ.get(\"ID_COL\", \"sample_id\")\n",
    "TEXT_COL        = os.environ.get(\"TEXT_COL\", \"catalog_content\")\n",
    "IMG_COL         = os.environ.get(\"IMG_COL\",  \"image_path\")\n",
    "\n",
    "OUTPUT_DIR      = os.environ.get(\"OUTPUT_DIR\", \"price_clip_vit_99_1_split\")\n",
    "CKPT_PATH       = os.environ.get(\"CKPT_PATH\", os.path.join(OUTPUT_DIR, \"best_clip.pt\"))\n",
    "\n",
    "BATCH_SIZE      = int(os.environ.get(\"INF_BATCH_SIZE\", \"64\"))\n",
    "MAX_LEN_ENV     = os.environ.get(\"MAX_LEN\", None)  # if you want to override tokenizer max len\n",
    "FP16            = os.environ.get(\"FP16\", \"true\").lower() == \"true\"\n",
    "\n",
    "assert os.path.exists(CKPT_PATH), f\"Checkpoint not found at {CKPT_PATH}\"\n",
    "assert os.path.exists(TEST_CSV),  f\"Test CSV not found at {TEST_CSV}\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- Load checkpoint ----\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "model_id = ckpt.get(\"model_id\", \"openai/clip-vit-large-patch14-336\")\n",
    "cfg = ckpt.get(\"config\", {})\n",
    "projection_dim = cfg.get(\"projection_dim\")\n",
    "img_missing_policy = cfg.get(\"IMG_MISSING_POLICY\", \"zero\")\n",
    "max_len = int(cfg.get(\"MAX_LEN\", 64)) if MAX_LEN_ENV is None else int(MAX_LEN_ENV)\n",
    "\n",
    "print(f\"📦 Loaded checkpoint from: {CKPT_PATH}\")\n",
    "print(f\"🔤 MODEL_ID={model_id} | projection_dim={projection_dim} | IMG_MISSING_POLICY={img_missing_policy} | MAX_LEN={max_len}\")\n",
    "\n",
    "# ---- Recreate processor & models ----\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "clip_model = CLIPModel.from_pretrained(model_id)\n",
    "clip_model.load_state_dict(ckpt[\"clip_state\"], strict=True)\n",
    "clip_model.to(device).eval()\n",
    "\n",
    "# Recreate and load regression head (same class as training cell)\n",
    "price_head = ClipRegressionHead(embed_dim=projection_dim, dropout=0.0)\n",
    "price_head.load_state_dict(ckpt[\"head_state\"], strict=True)\n",
    "price_head.to(device).eval()\n",
    "\n",
    "# ---- Load test data ----\n",
    "dft = pd.read_csv(TEST_CSV)\n",
    "dft[\"image_path\"] = dft[\"sample_id\"].apply(lambda x : f\"jl_fs/images/test/{x}.jpg\")\n",
    "for col, name in [(ID_COL, \"ID_COL\"), (TEXT_COL, \"TEXT_COL\"), (IMG_COL, \"IMG_COL\")]:\n",
    "    if col not in dft.columns:\n",
    "        raise ValueError(f\"{name} '{col}' missing from test CSV. Columns={dft.columns.tolist()}\")\n",
    "\n",
    "# Basic clean\n",
    "dft[TEXT_COL] = dft[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "dft[IMG_COL]  = dft[IMG_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# Build dataset/dataloader with no targets\n",
    "test_ds = ClipPriceDataset(\n",
    "    df=dft[[ID_COL, TEXT_COL, IMG_COL]].copy(),\n",
    "    text_col=TEXT_COL,\n",
    "    img_col=IMG_COL,\n",
    "    prices_log2=None,\n",
    "    processor=processor,\n",
    "    max_len=max_len,\n",
    "    policy=img_missing_policy\n",
    ")\n",
    "collate = CollateClip(processor)\n",
    "\n",
    "dl_te = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=2, pin_memory=True, collate_fn=collate\n",
    ")\n",
    "\n",
    "print(f\"🖥 Device: {device}\")\n",
    "print(f\"🧪 Test rows: {len(test_ds)} | Missing images encountered (during getitem): {test_ds.missing_img_count}\")\n",
    "print(f\"🗑 Dropped due to policy=drop: {getattr(test_ds, 'dropped_missing', 0)}\")\n",
    "\n",
    "# ---- Inference loop ----\n",
    "clip_model_dtype = next(clip_model.vision_model.parameters()).dtype\n",
    "preds_log2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl_te, total = len(dl_te)):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        img_missing    = batch[\"img_missing\"].to(device)\n",
    "\n",
    "        # Text features\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            txt_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Image features depending on policy\n",
    "            do_vision = (\"pixel_values\" in batch) and (img_missing_policy != \"text_only\")\n",
    "            if do_vision:\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=clip_model_dtype, non_blocking=True)\n",
    "                img_feat = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "                if img_missing.any():\n",
    "                    img_feat = img_feat * (1.0 - img_missing.unsqueeze(1).float())\n",
    "            else:\n",
    "                img_feat = torch.zeros_like(txt_feat)\n",
    "\n",
    "            # Normalize + fuse\n",
    "            txt_n = F.normalize(txt_feat, dim=-1)\n",
    "            img_n = F.normalize(img_feat, dim=-1)\n",
    "            fused = torch.cat([img_n, txt_n], dim=-1)\n",
    "\n",
    "            # Predict log2(price)\n",
    "            pred_log = price_head(fused)\n",
    "            preds_log2.append(pred_log.detach().float().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1f523-7ec4-4e8f-b839-8cbc384fd26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
